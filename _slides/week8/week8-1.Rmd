---
title: "$2^2$ Factorial Designs"
author: "Kris Sankaran | UW Madison | 23 September 2021"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false  
---

```{r echo=FALSE}
library(knitr)
library(ggplot2)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = FALSE, dpi = 200, fig.width = 6, fig.height = 2.8, dev = 'svg', dev.args = list(bg = "transparent"))
theme424 <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16),
    legend.position = "bottom"
  )
theme_set(theme424)
```

# $2^2$ Factorial Designs in Depth

```{r, out.width = 300}
include_graphics("https://uwmadison.box.com/shared/static/32pelq6o9jz7nnnavkdx1kjuxfjkvzrc.png")
```

### Statistical Experimental Design

.large[Kris Sankaran | UW Madison | 26 October 2021]

---

### Discussion Follow-up (I)

* In the examples, why were we only using 2 levels per factor? 
  - This is mainly for visual convenience (and to give a head start for the next sections)
  - We will see that there is much more that can be said about the 2-level special case
* Which method should we use when?
  - The _motivation_ slide in past lectures should help
  - We'll try including slides about this in future lectures
* Where are we going, overall?
  - $2^K$ designs, $2^K$ designs with very large $K$ (fractional factorials),
  and advanced response surfaces
  - We want richer machinery for working with many factors

---

### Discussion Follow-up (II)

There was a fair bit of confusion about interaction terms, the cube
visualization of them, and their intuitive interpretation.

Intuition: Interactions exist when there is a nonuniformity of effects
  - Ice cream / French Fries vs. Chocolate / Ketchup
  - Education level, political affiliation, and belief in climate change
  - Drug given to patients with different stroke severity

---

### Cube Plot

.pull-left[
* The cube plot was needed because we can't visualize a surface in four
dimensions.
* We can understand it through analogy with the 2D case
]

.pull-right[
```{r}
include_graphics("https://uwmadison.box.com/shared/static/gnoa5i4w5qp6ehbweh30n3a2v91qpi89.png")
```
]

---

### Cube Plot

* The cube plot was needed because we can't visualize a surface in four
dimensions.
* We can understand it through analogy with the 2D case

```{r, out.width = 900}
include_graphics("sketches/no-interactions.png")
```

---

### Cube Plot

* The cube plot was needed because we can't visualize a surface in four
dimensions.
* We can understand it through analogy with the 2D case

```{r, out.width = 760}
include_graphics("sketches/interactions-present.png")
```

---

### Higher-Order Interactions

* For second-order interactions (e.g., ABC), we compute "correction factors" for
the model that could use first-order interactions (e.g., AB, BC, and AC).
* Example: Suppose there were an interaction between drug efficacy (A) and
disease severity (B). The drug leads to more improvement among sicker people.
  - If the interaction effect looks identical across two age groups (C), then
  there is only a first-order interaction
  - If the shape of the interactions changes across C, then there is an ABC
  interaction
* In the future, we *will* consider models with all high-order interactions, but
we will be more skeptical about their existence

---

---

### Today

* Book Sections: 6.1 - 6.2
* Online Notes: Week 8 [1] and [2]

---

### Pre-Post Writing Exercise

Take 2 minutes to write down everything you can think of related to $2^2$
factorial designs.
* If you started the reading, what do you understand?
* If not, what would your best guess be for an example $2^2$ experiment?

We will take 2 minutes before the code example to summarize your progress.

---

### Motivation

A $2^{K}$ design is a factorial design where each of the $K$ factors is tested
at only two values (high and low).

.pull-left[
* These designs are useful for screening experiments
* If the response does not change even when the factor is at its extremes, that factor likely doesn't matter
* We can study the relationship important factors and the response in a follow-up experiment
]

.pull-right[
General two factor vs. $2^{2}$ design.
```{r, out.width = 220}
include_graphics("https://uwmadison.box.com/shared/static/e6sprneaeamlav35csswq68t62p0xney.png")
include_graphics("https://uwmadison.box.com/shared/static/jst0ib619vjw386gnl2hs8pee5rkkbx1.png")
```
]

---

### Motivation

* It may be possible to vary many parameters in a manufacturing process. Which
avenues are worth pursuing in more depth?
* The transportation department would like to encourage alternatives to driving
to campus. Which particular levers are likely to influence commuters?
* When training a machine learning model, many hyperparameters might impact
performance. Which are the most important?

---

### Representations

* A $2^{2}$ design samples data at corners of a square
* We can represent corners using either `+ / -` or alphabetical notation
* We often abuse notation and write a letter to represent the *sum* of the
responses at that corner, not just its index

| A | B | label |
|---|---|---------|
| - | - | (1)     |
| + | - | a       |
| - | + | b       |
| + | + | ab      |

---

### Estimation: Main Effects

Simple formulas summarize the main and interaction effects in a $2^{2}$ design.

.pull-left[
> "What is the average change in the response when A is activated?"

\begin{align}
A &= \frac{1}{2n}\left[\left(ab +a\right) - \left(b + (1)\right)\right]
\end{align}
]

.pull-right[
```{r}
include_graphics("https://uwmadison.box.com/shared/static/32pelq6o9jz7nnnavkdx1kjuxfjkvzrc.png")
```
]

---

### Estimation: Interaction Effects

Simple formulas summarize the main and interaction effects in a $2^{2}$ design.

.pull-left[
> "Does the effect of A change depending on whether or not B is active?"

\begin{align}
AB &= \frac{1}{2n}\left[\left(ab - b\right) - \left(a - \left(1\right)\right)\right]
\end{align}
]

.pull-right[
```{r}
include_graphics("https://uwmadison.box.com/shared/static/hvedijshgaw7so91kqcl1l8eunml4m9v.png")
```
]

---

### Notational Trick

If we can remember the tabular representation, we don't need to memorize any
formulas.

|label | effect A | effect B | effect AB |
| --- |---|---|---------|
| (1) | - | - |  + |
| a | + | - | - |
| b | - | + | - |
| ab | + | + | + |

---

### Hypothesis Testing

How can we gauge if an estimated effect is significant?

* Call the numerators in the effect estimates above "contrasts"
* E.g., the contrast for A is $ab + a - b - \left(1\right)$
* The associated sum of squares has a simple form,

\begin{align*}
SS_{\text{Factor}} &= \frac{1}{2^2 n}\left(\text{Contrast(Factor)}\right)^2
\end{align*}

Q: What is the form of $SS_{A}$?

---

### ANOVA-like Identity

In the two factor case, we have the identity,

\begin{align*}
SS_{\text{Total}} = SS_A + SS_B + SS_{AB} + SS_E
\end{align*}

* The df’s for the main and interaction terms are all 1 (there are two levels for each)
* The df of $SS_T$ is $n 2^2 - 1$ (number of samples minus one)
* The df of $SS_{E}$ is $\left(n 2^2 - 1\right) - 3 = 4\left(n - 1\right)$
* $F$-statistics are formed by taking the ratios between $SS$ and $MS_E$ terms

---

### Regression Perspective

All the formulas we've shown are consistent with the effect estimates of a
regression model,

\begin{align*}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_{1}x_{2} + \epsilon
\end{align*}

where the $x_k$’s take on one of two values, $-1$, or $1$.

It will turn out that $2 \hat{\beta}_1$ is the effect estimate A, and similarly
for B and AB.

---

### Coding

.pull-left[
* To use the regression approach, we need to _code_ the variables
* Coding converts low and high levels to ${-1, 1}$.
* This makes scales comparable and allows predictions at intermediate values
]

.pull-right[
```{r}
include_graphics("https://uwmadison.box.com/shared/static/giacopxkfj5jhuucepards9fv0cqvbmc.png")
```
]

---

### When to use this?

* Whenever there are two factors with two levels in each
  - Especially useful when we care about interactions
* Ensure no blocking is present
  - Neither factor should be viewed as a nuisance variable
  - There should be no systematic differences between groups of samples
* It may be necessary to recognize that "low" and "high" values for a continuous
factor can be coded into two levels

---

### Pre-Post Writing Exercise

Take 2 minutes to write down everything you can think of related to $2^2$
factorial designs.
* What new definitions or notation did you learn?
* What associations do you want to strengthen for this phrase?

---

# Code Implementation

```{r}
opts_chunk$set(echo = TRUE)
```


---

### Yield Dataset

We will illustrate these ideas on a yield dataset. There are 12 samples total,
three replicates at each corner of the square.

```{r}
library(readr)
yield <- read_table2("https://uwmadison.box.com/shared/static/bfwd6us8xsii4uelzftg1azu2f7z77mk.txt")
head(yield, 8)
```

---

### Visualization

We can use the `facet_wrap` command to plot the effect of A when B is or is not
active. The slope changes slightly across panels, but it could be due to chance.

```{r}
ggplot(yield) +
  geom_point(aes(A, Yield)) +
  facet_wrap(~B)
```

---

### Interaction Plot

Another useful command is `interaction.plot`, which shows the same kind of
information, but in one line of code.

```{r, fig.width = 6, fig.height = 3.8}
interaction.plot(yield$A, yield$B, yield$Yield)
```

---

### Coding

* For testing, we can still use the `lm + aov` approach from before
* First though, we code the low and high values as -1 and 1

```{r}
library(dplyr)
code <- function(x) ifelse(x == '-', -1, 1)
yield <- yield %>%
  mutate_at(vars(A, B), code)
fit <- lm(Yield ~ A * B, data = yield)
summary(aov(fit))
```

---

### Effect Estimates

We could implement the formulas by hand, e.g., for the main effect of A,

```{r}
mean(yield$Yield[yield$A == 1]) - mean(yield$Yield[yield$A == -1])
```
Alternatively, we could use the coefficients of the fitted model.

```{r}
2 * coef(fit) # works for main and interaction effects
```

The fact that the effect estimates in $2^K$ designs are always `2 * coef(fit)`
will be proved next week.

---

### Top-Down Visualization

For a $2^2$ design, we can visualize the fitted regression surface from the top
down, using the `image` command from the `rsm` package.

```{r, fig.height = 3.5, fig.width = 3.5}
library(rsm)
image(fit, ~ A + B)
```


---

### Exercise

This walks through Exercise 6.14.

An article describes the application of two-level factorial designs to circuit
manufacturing. An experiment was run using two factors: arsenic flow rate (A)
and deposition time (B). Four replicates were run, and the epitaxial layer
thickness was measured. The data are at
[https://tinyurl.com/562du4db](https://tinyurl.com/562du4db).

* Estimate the factor effects
* Conduct an analysis of variance. Which factors are important?
* Write down a regression equation that could be used to predict epitaxial layer
thickness. The codings are at
[https://tinyurl.com/3tje3d7z](https://tinyurl.com/3tje3d7z).

---

(1) Read the data in from the link provided.

(2) Using `mutate_at`, code the `+` and `-` levels for each factor into -1, 1.

(3) Fit the response `yield` onto both factors, A and B.

(4) Estimate the factor effects by using `2 * coef(name_of_your_model)`

(5) Interpret the $p$-values given by `aov` to determine the important factors.

(6) Challenge: Use the estimated factor effects and the given codings to write
an equation to predict the response at new flow rates and deposition times.

---

### Solution (1) and (2)

```{r}
library(readr)
library(dplyr)
code <- function(x) ifelse(x == '-', -1, 1)

experiment <- read_csv("https://tinyurl.com/562du4db") %>%
  mutate_at(vars(A, B), code)
```

---

### Solution (3) and (4)

```{r}
fit <- lm(yield ~ A * B, experiment)
2 * coef(fit)
```

---

### Solution (5)

```{r}
summary(aov(fit))
```

None of the factors are significant at the $\alpha = 0.05$ level, though B is
significant at $\alpha = 0.1$.

---

### Solution (6)

First code the flow and time values,

\begin{align*}
\widetilde{\text{flow}} &= 2\left[\frac{\text{flow} - 55}{59 - 55}\right] - 1 \\
\widetilde{\text{time}} &= 2\left[\frac{\text{time} - 10}{15 - 5}\right] - 1
\end{align*}

(to convince yourself that the coding works, check that 55 maps to -1 and 59
maps to 1 in the first coding, for example).

---

### Solution (6)

Then substitute the fitted coefficients,

\begin{align*}
\hat{y} &= 29.0405 -1.1975\times \widetilde{\text{flow}} + 0.609 \times \widetilde{\text{time}} + 1.126 \times \widetilde{\text{flow}} \times \widetilde{\text{time}}.
\end{align*}

Therefore, to make a prediction the yield at any new flow and time value, we can
code them using the transformations on the previous page and substitute into
this formula.