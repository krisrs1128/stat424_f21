---
title: "Probability Review"
author: "Kris Sankaran | UW Madison | 14 September 2021"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false  
---
```{r echo = FALSE}
library(knitr)
library(xaringanthemer)
opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, dpi = 500, cache = FALSE)
style_mono_light(base_color = "#5f558e")
```

```{r, echo = FALSE}
library(purrr)
library(tidyr)
library(ggplot2)
theme424 <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16),
    legend.position = "bottom"
  )
theme_set(theme424)
```

# Probability Review

<img src="https://uwmadison.box.com/shared/static/2nue43dk0rw0v25swmbd552qk1bz1jze.png" width="350px"/>

### Statistical Experimental Design

.large[Kris Sankaran | UW Madison | 14 September 2021]

---

### Announcements

* For today's exercises, we have groups assigned on canvas
  - Sorted according to whether project group is found
  - Arrangement around room
* If you do not find a group, we will randomly assign you (there is no penalty)
* Let us know if you have a midterm conflict by Sunday 9/19
* There is a week 2 feedback form on Canvas

---

### Sample $\rightarrow$ Population Properties

* The distribution $\mathbf{P}$ summarizes our model of the world
* We will be happy if we can make precise statements about it
  - Where is the center?
  - How spread out is it?
  - What is its shape?
  - How many peaks does it have? 
  - ...

---

### Statistical Estimators

* We only have access to a sample $x_1, \dots, x_n$ from $\mathbf{P}$.
  - Assume they are all independent replicates
* We can define functions of the sample in order to estimate properties of
$\mathbf{P}$
  - $\bar{x} \approx \mu\left(\mathbf{P}\right)$
  - $\frac{1}{n - 1}\sum_{i = 1}^{n} \left(x_i - \bar{x}\right)^2 \approx \sigma^2\left(\mathbf{P}\right)$

---

### How to evaluate estimators?

* Suppose we have 100 samples from a random normal distribution with unknown
mean $\mu$
* Which is a better estimator of the mean? Why?

$$\text{Option 1: } \bar{x} = \frac{1}{10}\sum_{i = 1}^{10} x_i \\ \text{Option 2: } \bar{x} = \frac{1}{100}\sum_{i = 1}^{100} x_i$$

---

### Low Bias and Variance

The code below generates 5000 datasets with population means 2.5, then computes
the two estimators.

```{r}
n_sim <- 5e3
datasets <- matrix(rnorm(n_sim * 100, 2.5), n_sim, 100)
means <- data.frame(
  id = 1:n_sim,
  partial = rowMeans(datasets[, 1:10]),
  full = rowMeans(datasets)
)
```

---

### Low Bias and Variance

The code below generates 5000 datasets with population means 2.5, then computes
the two estimators.

```{r}
head(means)
```

---

```{r, echo = FALSE}
p <- means %>%
  pivot_longer(-id, names_to = "estimator")
```

### Low Bias and Variance

.pull-left[
* Unbiased: The statistic is centered around the truth
* Low Variance: The spread of the statistic is low
* Using all the data gives an estimate with lower variance than using only a
fraction
]

.pull-right[
```{r, include = TRUE, echo = FALSE}
ggplot(p) +
  geom_histogram(
    aes(value, fill = estimator),
    position = "identity",
    bins = 100,
    alpha = 0.6
  )
```
]

---

# Central Limit Theorem

---

### Theorem Statement

If $y_i$ are drawn i.i.d. from some distribution with mean $\mu$ and variance
$\sigma^2$, then

\begin{align*}
\frac{\sqrt{n}\left(\bar{y} - \mu\right)}{\sigma} \to \mathcal{N}\left(0, 1\right).
\end{align*}

---

### Theorem Importance

.pull-left[This theorem reduces calculations across arbitrary distributions into
calculations with normal distributions.]

.pull-right[
<img src="https://uwmadison.box.com/shared/static/2nue43dk0rw0v25swmbd552qk1bz1jze.png" width="500px"/>
]

---

### Exercise Warm-Up

What will be the shape of the histogram in the block below? Why?

```{r}
#hist(runif(1000))
```

---

### Exercise Warm-Up

What will be the shape of the histogram in the block below? Why?

```{r, fig.width = 7, fig.height = 2.4}
hist(runif(1000))
```

---

### Exercise Warm-Up

What will be the shape of the histogram in the block below? Why?

```{r}
datasets <- matrix(runif(n_sim * 1000), nrow = n_sim, ncol = 1000)
#hist(rowMeans(datasets), breaks = 100)
```

---

### Exercise Warm-Up

What will be the shape of the histogram in the block below? Why?

```{r, fig.width = 7, fig.height = 2.5}
datasets <- matrix(runif(n_sim * 1000), nrow = n_sim, ncol = 1000)
hist(rowMeans(datasets), breaks = 100)
```

---

### Exercise

What will be the differences between the three histograms below? Why?

```{r}
plot_hist <- function(sample_size) {
  matrix(runif(n_sim * sample_size), n_sim, sample_size) %>%
    rowMeans() %>%
    hist(breaks = 50)
}

#plot_hist(1)
#plot_hist(2)
#plot_hist(1000)
```

---

### Exercise

What will be the differences between the three histograms below? Why?

```{r, fig.width = 7.5, fig.height = 2.8}
plot_hist(1)
```

---

### Exercise

What will be the differences between the three histograms below? Why?

```{r, fig.width = 7.5, fig.height = 2.8}
plot_hist(2)
```

---

### Exercise

What will be the differences between the three histograms below? Why?

```{r, fig.width = 7.5, fig.height = 2.8}
plot_hist(1000)
```

---

# Useful Distributions

---

### R Syntax

* `r{name of density}(n)` will sample $n$ points
* `d{name of density}(x)` will compute the probability density at at $x$
* `p{name of density}(x)` will integrate the density up to $x$
* `q{name of density}(p)` will find the $x$ value of the density at the $p$
quantile

---

### $t$ Distribution

* We can use the `dt` function to compute the density of the $t$ distribution.
* Evaluate over a grid of `x` values to make a plot

```{r, fig.height = 2.5, fig.width = 8}
x <- seq(-3, 3, length.out = 100)
data.frame(x, density = dt(x, df=2)) %>%
  ggplot() +
  geom_line(aes(x, density))
```

---

### $t$ Distribution

It has a hyperparameter, called the "degrees-of-freedom" (df). Smaller df means
heavier tails.

```{r, fig.height = 1.6, fig.width = 8}
densities <- map_dfr(
  seq(1, 10, .5), 
  ~ data.frame(x, density = dt(x, .), df = .)
)

ggplot(densities) +
  geom_line(aes(x, density, col = df, group = df)) +
  theme(legend.position = "right")
```

---

### $t$ Distribution

Here are examples generating samples and computing quantiles.

```{r, fig.height = 2.6, fig.width = 7}
hist(rt(1000, df = 4), breaks = 100)
qt(0.975, df = 4)
```

---

### chi-square Distribution

The chi-square distribution is nonnegative with one parameter and can be
referenced using `(prefix)chisq`.

```{r, echo = FALSE, fig.height = 2.2, fig.width = 7}
x <- seq(1e-1, 15, length.out = 100)
densities <- map_dfr(seq(3, 10, .5), ~ data.frame(x, density = dchisq(x, .), df = .))

ggplot(densities) +
  geom_line(aes(x, density, col = df, group = df)) +
  scale_color_viridis_c()
```

---

### $F$ Distribution

The F distribution is also nonnegative, but has two parameters.

```{r, fig.height = 2.3, fig.width = 7}
hist(rf(1000, 1, 100), breaks = 100)
```

---

### $F$ Distribution

```{r, fig.height = 2.3, fig.width = 7}
hist(rf(1000, 10, 10), breaks = 100)
```

---

# Where these distributions arise

---

### Main Idea

* We will often want the distribution of a particular statistic
* We may know the distribution of individual terms within the statistic
* Learning how one distribution arises as a function of another is key

---

### Chi-square Distribution

.pull-left[
This distribution arises as the sum-of-squares of standard normals. If 
$z_k \sim \mathcal{N}\left(0, 1\right)$, then $\sum_{k = 1}^{K} z_k^2 \sim \chi^2_{K}$.
]

.pull-right[
```{r, fig.height = 2.8, fig.width = 3.5}
rchisq(n_sim, 5) %>%
  hist(breaks = 50, freq=F, ylim = c(0, .18))
```

]

---

### Chi-square Distribution

.pull-left[
This distribution arises as the sum-of-squares of standard normals. If 
$z_k \sim \mathcal{N}\left(0, 1\right)$, then $\sum_{k = 1}^{K} z_k^2 \sim \chi^2_{K}$.
]

.pull-right[
```{r, fig.height = 2.8, fig.width = 3.5}
matrix(rnorm(n_sim * 5)^2, n_sim, 5) %>%
  rowSums() %>%
  hist(breaks = 50, freq=F, ylim = c(0, .18))
```

]

---

### Chi-square Distribution

A related (but nontrivial) fact is that if $y_i \sim \mathcal{N}\left(\mu, \sigma^2\right)$,

\begin{align}
\frac{1}{\sigma^2}\sum_{i = 1}^{n}\left(y_i - \bar{y}\right)^2 \sim \chi^2_{n - 1}
\end{align}

---

### Chi-square Distribution

```{r, fig.width = 7, fig.height = 2.8}
hist(rchisq(n_sim, 9), breaks = 50, col = rgb(0, 0, 1, .6))
datasets <- rerun(n_sim, rnorm(10, 2.5, 1))
ss <- map_dbl(datasets, ~ sum((. - mean(.)) ^ 2))
hist(ss, breaks = 50, col = rgb(0, 1, 0, 0.6), add = TRUE)
```

---

### t Distribution

The $t$ distribution can be formed as the ratio, 

$$
\frac{\mathcal{N}\left(0, 1\right)}{\sqrt{\frac{\chi^2_{K}}{K}}}
$$

This ratio often occurs when we standardize using an estimate of the standard
deviation,

$$
\frac{\sqrt{n}\left(\bar{y} - \mu\right)}{S}
$$

---

### $F$ Distribution

This distribution arises as the ratio,

$$
F_{u, v} = \frac{\frac{1}{u}\chi^2_u}{\frac{1}{v}\chi^2_v}.
$$

Since chi-squares come up whenever we compute sums-of-squares of normals, this
statistic will arise whenever we want to compare two different sums-of-squares.

---

### Summary

<img src="https://uwmadison.box.com/shared/static/dv5tvok0m9vkqqmkd3c0woam5is7gzse.png"/>

---

### Exercise

Write a small simulation to generate $t$-distributed variables (with whatever
d.f. you want), without using `rt`. 
  - Bonus: Make a histogram
  - Bonus: Use only `rnorm()`.

---

### Solution 1
```{r, echo = FALSE}
par(mar=c(0, 0, 0, 0))
```

```{r, fig.height = 2.5, fig.width = 6.8}
df <- 3
(rnorm(n_sim) / sqrt(rchisq(n_sim, df) / df)) %>%
  hist(breaks = 100, freq = F, xlab = NULL, main = NULL)
```


---

### Solution 2

```{r, fig.height = 2.5, fig.width = 6.8}
normalized_stat <- function(n) {
  y <- rnorm(n)
  sqrt(n) * mean(y) / sd(y)
}

unlist(rerun(n_sim, normalized_stat(df + 1))) %>%
  hist(breaks = 100, freq = F, xlab = NULL, main = NULL)
```
