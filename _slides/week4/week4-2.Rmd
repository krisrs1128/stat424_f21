---
title: "ANOVA Extensions"
author: "Kris Sankaran | UW Madison | 23 September 2021"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css", "cols.css"]
    lib_dir: libs
    nature:
      beforeInit: "cols_macro.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false  
---

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, dpi = 200, fig.width = 2, fig.height = 1, dev = 'svg', dev.args = list(bg = "transparent"))
theme424 <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16),
    legend.position = "bottom"
  )
theme_set(theme424)
```

# ANOVA Extensions

```{r, out.width = 200}
include_graphics("https://uwmadison.box.com/shared/static/a8jqduhcmjzj9re22a81236k3enbtzzn.png")
```
### Statistical Experimental Design

.large[Kris Sankaran | UW Madison | 23 September 2021]

---

### Today

* Book Sections: 3.9 & 3.11
* Online Notes: Week 4 [2] and [3]

---

### Motivation

* Nonparametric ANOVA
  - We may not believe the assumed model for ANOVA
  - Transformations may not help
* Estimation of Random Effects
  - Different algorithms provide different outputs
  - Conceptual understanding can guide the choice of software
  
.pull-right[
```{r}
include_graphics("https://uwmadison.box.com/shared/static/a2evjeib97ay362iue2vhkxk18gbh7l4.png")
```
]

---

### Nonparametric ANOVA

* We can compare groups based on the ranks of the observations they contain
* Ranks are not sensitive to outliers or distributional assumptions

```{r}
include_graphics("https://uwmadison.box.com/shared/static/kdv2ayi2heooodf0a0j15swxld4ofll7.png")
include_graphics("https://uwmadison.box.com/shared/static/nn8v4t3ntvcsowvuy91b5lvoajju3ekf.png")
```

---

### Nonparametric ANOVA

* Transform the data to their ranks. The smallest of the $y_{ij}$ becomes 1, the
next smallest becomes 2, etc. Denote these ranks by $R_{ij}$. 
* Compute the test statistic
\begin{align*}
  \frac{\sum_{i} n_{i}\left(\bar{R}_{i} - \bar{R}\right)^2}{\frac{1}{N - 1}\sum_{i, j} \left(R_{ij} - \bar{R}\right)^2}
\end{align}
  where $\bar{R}_{i}$ is the average rank in group $i$ and $\bar{R}$ is the
  average rank overall
  
---

### Hypothesis Test

* If this statistic is above a chi-square cutoff, then reject the null
hypothesis of equality in group means
* The test is valid even if the errors are highly non-normal
  - However, this procedure may have lower power

---

## Fitting Random Effects

\begin{align*}
y_{ij} &= \mu + \tau_i + \epsilon_{ij} \\
\tau_i &\sim \mathcal{N}\left(0, \sigma^2_{\tau}\right) \\
\epsilon_{ij} &\sim \mathcal{N}\left(0, \sigma^2\right) \\
\end{align*}

There are three (nonrandom) parameters to estimate: $\mu, \sigma^2_{\tau}, \sigma^2$. Two main approaches are,
* Method of Moments
* (Restricted) Maximum Likelihood

---

## Method of Moments

Main idea: Write an equation relating unknown, population means with observed
sample means. Then solve for unknowns.

For example,
\begin{align*}
\mu = \mathbf{E}\left[y\right] \approx \bar{y}
\end{align*}
which leads to the estimator $\hat{\mu} := \bar{y}$ 

---

## Method of Moments

It is not obvious, but the expected value of the mean squared errors in random
effects models can be found in closed form,

\begin{align*}
\mathbf{E}\left[MS_{\text{treatment}}\right] &= \sigma^2 + n \sigma_{\tau}^2 \\
\mathbf{E}\left[MS_{E}\right] &= \sigma^2
\end{align*}

---

### Method of Moments

If we approximate the expected values by the value we observed, we are left with
two equations for our two unknowns ($\sigma^2_{\tau}$ and $\sigma^2$).

\begin{align*}
MS_{\text{treatment}} &\approx \mathbf{E}\left[MS_{\text{treatment}}\right] = \sigma^2 + n \sigma_{\tau}^2 \\
\MS_{E} &\approx \mathbf{E}\left[MS_{E}\right] = \sigma^2
\end{align*}

Solving for the unknowns gives estimators $\hat{\sigma}^2$ and
$\hat{\sigma}_{\tau}^2$.

---

### Confidence Intervals

* In addition to estimates of the parameters, it would be nice to have a sense
of our uncertainty
* There are formulas that give valid confidence intervals for $\hat{\mu},
\hat{\sigma}^2$, but not for $\hat{\sigma}_{\tau}^2$
* We will use the formulas in the code example below

---

### Pros & Cons: Method of Moments

* Pros: Very fast, since just solving system of equations
* Can give negative variance estimates (and intervals)
* No confidence intervals for $\hat{\sigma}_{\tau}^2$

---

### Maximum Likelihood

* Define $L(\mu, \sigma^2, \sigma^2_{\tau})$ to be the probability of the
dataset $y_{11}, y_{12}, \dots y_{an}$ given the choice of parameters $\mu, \sigma^2$, and $\sigma^2_{\tau}$
* A good choice for the parameters is one that maximizes $L\left(\mu, \sigma^2, \sigma^2_{\tau}\right)$.

```{r}
include_graphics("https://uwmadison.box.com/shared/static/0au3iwj17u49ueqor5djqg58rensvmqo.png")
```

---

### Maximum Likliehood

From the previous lecture, we know,
* Given the parameters, the data are multivariate normally distributed
* The mean is $\mu$, and covariance is block diagonal, depending on $\sigma^2, \sigma_{\tau}$

```{r}
include_graphics("https://uwmadison.box.com/shared/static/ujmby14i61prsex43i6drsgpgsu9lvjq.png")
```

---

### Confidence Intervals

Confidence intervals can be constructed based on the curvature of the likelihood function.

```{r}
include_graphics("https://uwmadison.box.com/shared/static/kqgsua08wh8n2h6zhssbmxf4byldcq2o.png")
```

---

### Maximum Likelihood: Pros and Cons

* Pros
  - Confidence intervals for all parameters
  - Never gives negative variance estimates
* Cons
  - Optimization can be slow

---

# Code Implementation

---

### Loom Dataset

We will use the same dataset as last lecture, taken from Example 3.10 of the
textbook.

```{r, echo = TRUE}
library(readr)
loom <- read_csv("https://uwmadison.box.com/shared/static/ezp3i2pflhi96si7u6rfn3dg3lb5cl3z.csv")
loom$loom <- as.factor(loom$loom)
loom
```

`lmer` actually uses maximum likelihood estimation, so we will focus on the
method of moments and nonparametric ANOVA.

---

### Method of Moments Setup

Before we can solve the equations for the method of moments, we need to obtain
the values that appear in the equations.

```{r}
library(dplyr)
library(broom)
N <- nrow(loom)
a <- nlevels(loom$loom)
n <- N / a

aov_table <- aov(lm(strength ~ loom, data = loom)) %>%
  tidy()
aov_table
```

### Method of Moments Equations

Solving for unknowns in the method of moments gives these estimates for
$\sigma^2_{\tau}$ and $\sigma^2$.

```{r}
sigma_sqs <- vector(length = 2)
sigma_sqs[1] <- aov_table$meansq[2] # estimate for sigma^2
sigma_sqs[2] <- (aov_table$meansq[1] - aov_table$meansq[2]) / n # estimate for sigma^2_tau
sigma_sqs
```

The results are very similar as those from LMER in the last lecture.

---

### Method of Moments: Confidence Intervals

The formulas below compute 95\% confidence intervals for $\hat{\mu}$ and
$\hat{\sigma}^2$.

```{r}
int_bounds <- c(0.975, 0.025)
(N - a) * sigma_sqs[1] / qchisq(int_bounds, N - a) # CI for sigma^2
```


---

### Maximum Likelihood: Confidence Intervals

To get a confidence interval from the maximum likelihood approach, we can use
the `confint` function.

```{r}
library(lme4)
fit <- lmer(strength ~ (1 | loom), data = loom)
confint(fit)^2
```

---

### Nonparametric ANOVA

To conduct a hypothesis test using nonparametric ANOVA, we can use the
`kruskal.test` function,

```{r}
kruskal.test(strength ~ loom, data = loom)
```

---

### Exercise

This walks through Problem [] in the textbook.

---