---
title: "ANOVA Extensions"
author: "Kris Sankaran | UW Madison | 23 September 2021"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false  
---

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = FALSE, dpi = 200, fig.width = 2, fig.height = 1, dev = 'svg', dev.args = list(bg = "transparent"))
theme424 <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16),
    legend.position = "bottom"
  )
theme_set(theme424)
```

# ANOVA Extensions

```{r, out.width = 200}
include_graphics("https://uwmadison.box.com/shared/static/nn8v4t3ntvcsowvuy91b5lvoajju3ekf.png")
```
### Statistical Experimental Design

.large[Kris Sankaran | UW Madison | 23 September 2021]

---

### Today

* Book Sections: 3.9 & 3.11
* Online Notes: Week 4 [2] and [3]

---

### Motivation

.pull-left[
* Nonparametric ANOVA
  - We may not believe the assumed model for ANOVA
  - Transformations may not help
* Estimation of Random Effects
  - Different algorithms provide different outputs
  - Conceptual understanding can guide the choice of software
]
  
.pull-right[
```{r, out.width = "80%"}
include_graphics("https://uwmadison.box.com/shared/static/a2evjeib97ay362iue2vhkxk18gbh7l4.png")
```
]

---

### Nonparametric ANOVA

.pull-left[
* We can compare groups based on the ranks of the observations they contain
* Ranks are not sensitive to outliers or distributional assumptions
]

.pull-right[
```{r, fig.show = "hold", out.width = "50%"}
include_graphics("https://uwmadison.box.com/shared/static/kdv2ayi2heooodf0a0j15swxld4ofll7.png")
include_graphics("https://uwmadison.box.com/shared/static/nn8v4t3ntvcsowvuy91b5lvoajju3ekf.png")
```
]

---

### Nonparametric ANOVA

* Transform the data to their ranks. The smallest of the $y_{ij}$ becomes 1, the
next smallest becomes 2, etc. Denote these ranks by $R_{ij}$. 
* Compute the test statistic

\begin{align*}
  \frac{\sum_{i} n_{i}\left(\bar{R}_{i} - \bar{R}\right)^2}{\frac{1}{N - 1}\sum_{i, j} \left(R_{ij} - \bar{R}\right)^2}
\end{align*}

  where $\bar{R}_{i}$ is the average rank in group $i$ and $\bar{R}$ is the
  average rank overall
  
---

### Hypothesis Test

* If this statistic is above a chi-square cutoff, then reject the null
hypothesis of equality in group means
* The test is valid even if the errors are highly non-normal
  - However, this procedure may have lower power

---

## Fitting Random Effects

\begin{align*}
y_{ij} &= \mu + \tau_i + \epsilon_{ij} \\
\tau_i &\sim \mathcal{N}\left(0, \sigma^2_{\tau}\right) \\
\epsilon_{ij} &\sim \mathcal{N}\left(0, \sigma^2\right) \\
\end{align*}

There are three (nonrandom) parameters to estimate: $\mu, \sigma^2_{\tau}, \sigma^2$. Two main approaches are,
* Method of Moments
* (Restricted) Maximum Likelihood

---

## Method of Moments

Main idea: Write an equation relating unknown, population means with observed
sample means. Then solve for unknowns.

For example,
\begin{align*}
\mu = \mathbf{E}\left[y\right] \approx \bar{y}
\end{align*}
which leads to the estimator $\hat{\mu} := \bar{y}$ 

---

## Method of Moments

It is not obvious, but the expected value of the mean squared errors in random
effects models can be found in closed form,

\begin{align*}
\mathbf{E}\left[MS_{\text{treatment}}\right] &= \sigma^2 + n \sigma_{\tau}^2 \\
\mathbf{E}\left[MS_{E}\right] &= \sigma^2
\end{align*}

---

### Method of Moments

If we approximate the expected values by the value we observed, we get two
equations with two unknowns,
$\sigma_{\tau}^2$ and $\sigma^2$.

\begin{align*}
MS_{\text{treatment}} &\approx \mathbf{E}\left[MS_{\text{treatment}}\right] = \sigma^2 + n \sigma_{\tau}^2 \\
MS_{E} &\approx \mathbf{E}\left[MS_{E}\right] = \sigma^2
\end{align*}

Solving for the unknowns gives estimators $\hat{\sigma}^2_{\tau}$ and
$\hat{\sigma}^2$.

---

### Confidence Intervals

* In addition to estimates of the parameters, it would be nice to have a sense
of our uncertainty
* There are formulas that give valid confidence intervals for $\hat{\mu}, \hat{\sigma}^2$, and the Intraclass Correlation (ICC),
$\widehat{\frac{\sigma^2_{\tau}}{\sigma_{\tau}^2 + \sigma^2}}$, but not
for $\hat{\sigma}_{\tau}^2$
* We demonstrate use of the formulas in a code example below

---

### Pros & Cons: Method of Moments

* Pros: Very fast, since just solving system of equations
* Can give negative variance estimates (and intervals)
* No confidence intervals for $\hat{\sigma}_{\tau}^2$

---

### Maximum Likelihood

* Define $L(\mu, \sigma^2, \sigma^2_{\tau})$ to be the probability of the
dataset $y_{11}, y_{12}, \dots y_{an}$ given the choice of parameters $\mu, \sigma^2$, and $\sigma^2_{\tau}$.
* A good choice for the parameters is one that maximizes $L\left(\mu, \sigma^2, \sigma^2_{\tau}\right)$.
  - This is usually found through iterative optimization

```{r, fig.align = "center", out.width = 500}
include_graphics("https://uwmadison.box.com/shared/static/0au3iwj17u49ueqor5djqg58rensvmqo.png")
```

---

### Maximum Likliehood

From the previous lecture, we know,
* Given the parameters, the data are multivariate normally distributed
* The mean is $\mu$, and covariance is block diagonal, depending on $\sigma^2, \sigma_{\tau}^2$

```{r, out.width = 500, fig.align = "center"}
include_graphics("https://uwmadison.box.com/shared/static/ujmby14i61prsex43i6drsgpgsu9lvjq.png")
```

---

### Confidence Intervals

Confidence intervals can be constructed based on the curvature of the likelihood function.

```{r, out.width = 650, fig.align = "center"}
include_graphics("https://uwmadison.box.com/shared/static/kqgsua08wh8n2h6zhssbmxf4byldcq2o.png")
```

---

### Maximum Likelihood: Pros and Cons

* Pros
  - Confidence intervals for all parameters
  - Never gives negative variance estimates
* Cons
  - Optimization can be slow

---

# Code Implementation

```{r}
opts_chunk$set(echo = TRUE)
```

---

### Loom Dataset

We will use the same dataset as last lecture, taken from Example 3.10. `lmer`
uses maximum likelihood estimation, so we will focus on the method of moments
and nonparametric ANOVA.


```{r, echo = TRUE}
library(readr)
loom <- read_csv("https://uwmadison.box.com/shared/static/ezp3i2pflhi96si7u6rfn3dg3lb5cl3z.csv")
loom$loom <- as.factor(loom$loom)
head(loom, 4)
```

---

### Nonparametric ANOVA

To conduct a hypothesis test using nonparametric ANOVA, we can use the
`kruskal.test` function,

```{r}
kruskal.test(strength ~ loom, data = loom)
```

---

### Method of Moments Setup

Before we can solve the equations for the method of moments, we need to obtain
the values that appear in the equations.

```{r}
library(dplyr)
library(broom)
N <- nrow(loom)
a <- nlevels(loom$loom)
n <- N / a

fit <- lm(strength ~ loom, data = loom)
aov_table <- tidy(aov(fit))
aov_table
```

---

### Method of Moments Equations

Solving for unknowns in the method of moments gives these estimates for
$\sigma^2_{\tau}$ and $\sigma^2$.

```{r}
sigma_sqs <- vector(length = 2)
sigma_sqs[1] <- aov_table$meansq[2] # estimate for sigma^2
sigma_sqs[2] <- (aov_table$meansq[1] - aov_table$meansq[2]) / n # estimate for sigma^2_tau
sigma_sqs
```

The results are very similar as those from LMER in the last lecture.

---

### Method of Moments: Confidence Intervals

The code below provides a confidence interval for $\hat{\mu}$, according to
Equation 3.58 in the textbook,

\begin{align*}
\left[\bar{y} - t_{1 - \frac{\alpha}{2}, a\left(n - 1\right)}\sqrt{\frac{MS_{E}}{an}}, \bar{y} + t_{1 - \frac{\alpha}{2}, a\left(n - 1\right)}\sqrt{\frac{MS_{E}}{an}}\right]
\end{align*}

```{r}
width <- sqrt(aov_table$meansq[2] / (n * a))
mu_hat <- summary(fit)$coefficients[1]
cutoff <- qt(0.975, a * (n - 1))
c(mu_hat - cutoff * width, mu_hat + cutoff * width) # CI for mu_hat (Equation 3.58)
```

---

### Method of Moments: Confidence Intervals

The code below confidence intervals for $\hat{\sigma}^2$ and the ICC, according
to Equations 3.53 and 3.57 in the textbook, respectively.

```{r}
bounds <- c(0.975, 0.025)
(N - a) * sigma_sqs[1] / qchisq(bounds, N - a) # CI for sigma^2

ratio_bounds <- 1 / n * (aov_table$statistic[1] / qf(bounds, a - 1, N - a) - 1)
ratio_bounds / (1 + ratio_bounds) # CI for ICC
```

---

### Maximum Likelihood: Confidence Intervals

To get a confidence interval from the maximum likelihood approach, we can use
the `confint` function.

```{r}
library(lme4)
fit <- lmer(strength ~ (1 | loom), data = loom)
confint(fit)^2
```

---

### Exercise - Option A

This walks through Problem 3.35 in the textbook.

> A single factor completely randomized design has four levels of the factor.
There are three replicates and the total sum of squares is 330.56. The treatment
sum of squares is 250.65. Find a 95% confidence interval on the Intraclass
Correlation Coefficient.

---

### Exercise - Option A

(1) Based on the problem description, find the values of $n$ and $a$ from our notation above.
(2) Based on the problem data, compute $MS_{E}$.
(3) Using the formula from the code example, compute a confidence interval for the ICC.

---

### Exercise - Option B

This walks through Problem 3.52 in the textbook.

.pull-left[
The effective life of insulating fluids at an accelerated load of 35kV is being
studied. Test data have been obtained for four types of fluids. The results from
a completely randomized experiment are on the right. Use the Kruskal-Wallis test
to gauge whether the fluids differ.
]

.pull-right[
```{r}
fluids <- data.frame(
  type = rep(c("1", "2", "3", "4"), each = 6),
  life = c(17.6, 18.9, 16.3, 17.4, 20.1, 21.6,
           16.9, 15.3, 18.6, 17.1, 19.5, 20.3,
           21.4, 23.6, 19.4, 18.5, 20.5, 22.3,
           19.3, 21.1, 16.9, 17.5, 18.3, 19.8)
)
```
]

---

### Exercise - Option B

(1) What is the null hypothesis that nonparametric ANOVA (i.e., the
Kruskal-Wallis Test) tests? Interpret this null hypothesis in this application's
context.
(2) Use the `kruskal.test` function to gauge whether the fluids differ.
(3) Interpret your results from (2) in context.

---

### Option (B) - (1)

```{r}
kruskal.test(fluids$life, fluids$type)
```

---

### Option (A) - 

```{r}
bounds <- c(0.025, 0.975)
n <- 1
a <- 1
  
ratio_bounds <- 1 / n * (MSE / qf(bounds, a - 1, N - a) - 1)
ratio_bounds / (1 + ratio_bounds)
```

---