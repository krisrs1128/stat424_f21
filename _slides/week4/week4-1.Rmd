---
title: "Random Effects"
author: "Kris Sankaran | UW Madison | 28 September 2021"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false  
---

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = FALSE, dpi = 200, fig.width = 6, fig.height = 4, dev = 'svg', dev.args = list(bg = "transparent"))
theme424 <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16),
    legend.position = "bottom"
  )
theme_set(theme424)
```


# Random Effects

```{r, out.width = 800}
include_graphics("https://uwmadison.box.com/shared/static/f2e3b39odm4ejkyqinvpii24s9tegnqb.png")
```

### Statistical Experimental Design

.large[Kris Sankaran | UW Madison | 28 September 2021]

---

## Announcements

---

### Today

* Book Sections: 3.9
* Online Notes: Week 4 [1]

---

### Motivation

.pull-left[
* Suppose there are a large number of levels for a factor
  - We may not be able to get measurements for them all
  - It could be hard to interpret each $\hat{\tau}_i$
* New Q: Does a factor has an effect on average?
* There are random effects versions of many models, not just ANOVA
]

.pull-right[
```{r, out.width = 700}
include_graphics("https://uwmadison.box.com/shared/static/ig50uxftq5mz19t15w74kryh9ko0jm6e.png")
```
]

---

### Some examples

- Is there a loom effect on fiber strength?
- Is there a microbiome effect on preterm births?
- Is there a county effect on election outcomes?
- Is there a middle school effect on high school graduation?

---

### Model

.pull-left[
Random Effects ANOVA is a version of ANOVA where *the treatment effect is
random*,

\begin{align}
y_{ij} &= \mu + \tau_i + \epsilon_{ij} \\ \\
\tau_i &\sim \mathcal{N}\left(0, \sigma^2_{\tau}\right) \\
\epsilon_{ij} &\sim \mathcal{N}\left(0, \sigma^2\right)
\end{align}

As before, $i$ indexes groups and $j$ indexes observations within groups.

]

.pull-right[
```{r, out.width = 2000}
include_graphics("https://uwmadison.box.com/shared/static/f2e3b39odm4ejkyqinvpii24s9tegnqb.png")
```
]

---

### Within-Group Covariance

In ordinary ("fixed-effects") ANOVA, all the observations are uncorrelated,
because the $\epsilon_{ij}$'s are independent,

\begin{align}
\text{Cov}\left(y_{ij}, y_{i^\prime j^\prime}\right) = \begin{cases} \sigma^2 &\text{ if } i = i^{\prime}, j = j^{\prime} \\0 &\text{otherwise}. \end{cases}
\end{align}

---

### Within-Group Covariance

In random effects ANOVA, observations within the same group are correlated.

\begin{align}
\text{Cov}\left(y_{ij}, y_{i^\prime j^\prime}\right) = \begin{cases} \sigma^2 + \sigma_{\tau}^2 &\text{ if } i = i^{\prime}, j = j^{\prime} \\  
\sigma_{\tau}^2 &\text{ if } i = i^{\prime}, j \neq j^{\prime} \\
0 &\text{otherwise}. \end{cases}
\end{align}

---

### Within-Group Covariance

The associated covariance has a block diagonal structure,

```{r, out.width = 620, fig.align = "center"}
include_graphics("https://uwmadison.box.com/shared/static/ujmby14i61prsex43i6drsgpgsu9lvjq.png")
```

In fact, the vector $\left(y_{11}, \dots, y_{an}\right)$ is normally distributed
with mean $\mu$ and this covariance. This will be useful in our next lecture.

---

### Terminology

Some terms are frequently used in reports that use random effects models,

* The parameters $\sigma^2_{\tau}$ and $\sigma^2$ are called "variance components"
* The proportion $\frac{\sigma^2_{\tau}}{\sigma^2_{\tau} + \sigma^2}$ is
sometimes called the "Intraclass Correlation Coefficient" (ICC)

---

### Hypothesis Testing

Given this model, we may interested in the test,

\begin{align*}
H_0:&\space \sigma_\tau^2 = 0 \\
H_1:&\space \sigma^2_{\tau} > 0
\end{align*}

```{r, out.width = 900}
include_graphics("https://uwmadison.box.com/shared/static/l9nclru13n0s5e52oiu1aasz66vbwr6m.png")
```

---

### Hypothesis Testing

* The statistic $\frac{MS_{\text{treatment}}}{MS_{E}}$ from fixed-effects ANOVA
can still be used here
* The reference distribution is unchanged (an $F\left(a - 1, N - a\right)$)
* Everything you know for fixed-effects ANOVA can be used for testing in random
effects ANOVA

---

### Code Implementation

---

###  Loom Dataset

We will illustrate this on a dataset of fabric strength across looms in a
textile manufacturing plant. It is from Example 3.10 of the textbook.

```{r, echo = TRUE}
library(readr)
loom <- read_csv("https://uwmadison.box.com/shared/static/ezp3i2pflhi96si7u6rfn3dg3lb5cl3z.csv")
loom$loom <- as.factor(loom$loom)
loom
```

---

### Plot the data

There seems to be an effect. Can we quantify it?

```{r, echo = TRUE, fig.height = 3, fig.align = "center"}
ggplot(loom) +
  geom_point(aes(loom, strength))
```

---

### `lmer`

.pull-left[
* To fit the model, we use `lmer` from the `lme4` package
* The syntax `(1 | variable)` means that variable should be treated as a random
]

.pull-right[
```{r, echo = TRUE}
library(lme4)
fit <- lmer(strength ~ (1 | loom), data = loom)
fit
```
]

---

### Interpretation

.pull-left[
* The most important parts of the output is the column called `Std. Dev.`. These
are $\sigma_{\tau}$ and $\sigma$.
* Though the variation in strength for any given loom is small (s.d. $\approx 1.3$) the loom-to-loom variation is large (s.d. $\approx 2.6$).
]

.pull-right[
```{r, echo = TRUE}
fit
```
]

---

### Hypothesis Test

* We can perform a hypothesis test just like in the earlier notes.
* The results are consistent with our interpretation of fitted parameters

```{r, echo = TRUE}
aov_table <- aov(lm(strength ~ loom, data = loom))
summary(aov_table)
```

---

### Exercise

This walks through problem 3.30 in the book.

.pull-left[
Several ovens in a metal working shop are used to heat metal specimens. All the
ovens are supposed to operate at the same temperature, although it is suspected
that this may not be true. Three ovens are selected at random, and their
temperatures on successive heats are noted. The data are collected as follows,
]

.pull-right[
```{r, echo = TRUE}
experiment <- data.frame(
  "oven" = c(rep("1", 5), rep("2", 4), rep("3", 6)),
  "temperature" = c(491.5, 498.3, 498.1, 493.5, 493.6,
                    488.5, 484.65, 479.9, 477.35, 
                    490.1, 484.4, 488.25, 473, 471.85, 478.65)
)

head(experiment)
```
]

---

(1) Frame this problem as a random effects ANOVA problem. What is the
interpretation of $\tau_i$ and $\sigma_{\tau}^2$ in this context?

(2) Use `lmer` with a random effect for oven, i.e. `(1 | oven)`, to fit a random
effects ANOVA model to this data. What are the variance components?

(3) Interpret the results. Is there any concerning oven-to-oven variation?

---

### Solution (1)

Since the ovens are sampled from a larger population of ovens, it's natural to
think of the oven effects as being random.

We model $\tau_i \sim \mathcal{N}\left(0, \sigma^2_{\tau}\right)$, where
$\tau_i$ represents the deviation of oven $i$'s temperature from the global mean
and $\sigma^2_{\tau}$ controls the variation in mean temperatures from one oven
to the next.

---

### Solution (2)

The variance components are $\sigma^2_{\tau} \approx 7.2^2 \approx 52$ and
$\sigma^2 \approx 5.9 ^ 2 \approx 34$.

```{r, echo = TRUE}
library(lme4)
fit <- lmer(temperature ~ (1 | oven), data = experiment)
fit
```

---

### Solution (3)

There appears to be a noticeable oven-to-oven effect, on the same order of
magnitude as the within-oven variation.

---

### Bonus

A brief look at the raw data raises suspicions.

  * The first oven seems quite different from the other two. Is it an outlier
  oven? It might be helpful to gather data from more ovens.
  * The variance in temperature within ovens do not seem comparable.

```{r, echo = TRUE, fig.height = 2.3}
ggplot(experiment) +
  geom_point(aes(oven, temperature))
```

