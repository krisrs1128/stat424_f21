---
title: "Contrasts and Multiple Comparisons"
author: "Kris Sankaran | UW Madison | 23 September 2021"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css", "cols.css"]
    lib_dir: libs
    nature:
      beforeInit: "cols_macro.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false  
---

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, dpi = 200, fig.width = 10, fig.height = 5, dev = 'svg', dev.args = list(bg = "transparent"))
theme424 <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16),
    legend.position = "bottom"
  )
theme_set(theme424)
```


# Contrasts and Multiple Comparisons

<img src="https://uwmadison.box.com/shared/static/ti2ukjamosvu0quvhgauvre1idm3mjfa.png" width=350/>


### Statistical Experimental Design

.large[Kris Sankaran | UW Madison | 23 September 2021]

---

### Today

* Book Sections: 3.5
* Online Notes: Week 3 [3] and [4]

---

### Limitation of ANOVA $F$-test

Recall the ANOVA model,

\begin{align}
y_{ij} = \mu + \tau_i + \epsilon_{ij} \\
\epsilon_{ij} \sim \mathcal{N}\left(0, \sigma^2\right)
\end{align}

and the associated hypothesis test,

\begin{align}
&H_0: \tau_1 = \dots = \tau_{a} = 0 \\
&H_1: \tau_{i} \neq 0 \text{ for at least one }i.
\end{align}

Note that it *does not* allow us to conclude which group(s) are responsible for
a rejection.

---

### Follow-up Tests

We may instead define a follow-up hypothesis test.

.pull-left[Imagine we had 4 groups,
* Are the first two means different from the last two? $H_0: \tau_1 + \tau_2 = \tau_3 + \tau_4$
* Are the first two means equal to each other? $H_0 : \tau_1 = \tau_2$
* ...
]

.pull-right[
<img src="https://uwmadison.box.com/shared/static/ti2ukjamosvu0quvhgauvre1idm3mjfa.png" width=400/>
]

---

### Contrasts

A single framework covers all these cases.

* Let $\mu_i = \mu + \tau_i$ be the mean of group $i$.
* A contrast is a linear combination of the means,
\begin{align}
    \Gamma(c)=\sum_{i=1}^{a} c_{i} \mu_{i}
\end{align}
* The previous examples reduce to,

\begin{aligned}
&H_{0}: \Gamma(c)=0 \\
&H_{1}: \Gamma(c) \neq 0
\end{aligned}
  
  for $c = (1, 1, -1, -1)$ and $c = \left(1, -1, 0, 0\right)$, respectively.
  
---

### Plug-In Approximations

Since the sample means $\bar{y}_i \approx \mu_i$, we can approximate,

\begin{align*}
\hat{\Gamma}\left(c\right) := \sum_{i}c_i \bar{y}_i \approx \sum_{i}c_i \mu_i = \Gamma\left(c\right)
\end{align*}

Moreover, since $\hat{\sigma}^2 \approx \sigma^2$,

\begin{align*}
\widehat{\operatorname{Var}}\left(\hat{\Gamma}(c)\right) := \left[\sum_{i}c_i^2\right] \frac{\hat{\sigma}^2}{n} \approx \left[\sum_i c_i^2\right]\frac{\sigma^2}{n} = \text{Var}\left(\hat{\Gamma}(c)\right)
\end{align*}

---

### Reference Distribution

If the null hypothesis is true, the statistic is close to 0, because,

\begin{align*}
\hat{\Gamma}\left(c\right) \approx \Gamma\left(c\right) = 0
\end{align*}

In fact, under the null, it's possible to derive that
\begin{align*}
\frac{\hat{\Gamma}\left(c\right)}{\sqrt{\widehat{\operatorname{Var}}\left(\hat{\Gamma}(c)\right)}}
\end{align*}
is $t$-distributed with $N - a$ degrees of freedom (the proof is unimportant in
this class). This gives the basis for a $t$-test for any specific contrast.

---

### Confidence Interval

Given this reference distribution, it's also possible to derive a confidence
interval,
\begin{align*}
    \left[\hat{\Gamma}(c)-t_{\mathrm{left}} \sqrt{\widehat{\operatorname{Var}}(\hat{\Gamma}(c))}, \hat{\Gamma}(c)+t_{\mathrm{left}} \sqrt{\widehat{\operatorname{Var}}(\hat{\Gamma}(c))}\right]
\end{align*}

* This quantifies the uncertainty in our estimate of $\hat{\Gamma}\left(c\right)$.
* In practice, we would never compute these statistics by hand.

---

### Multiple Comparisons

.pull-left[
* If we knew the interesting contrasts in advance, we will be fine
* But what if we go in search for significant results using different contrasts?
]

.pull-right[
<img src="https://imgs.xkcd.com/comics/significant.png" width=180/>
]

---

### Multiple Comparisons

* We need to adapt our methodology to account for the search over contrasts
* The quantity of interest is the experimentwise error rate, the probability
that we get at least one false positive across the entire experiment
* Two methods, each with different properties,
  - Scheffe's method
  - Tukey's Honest Significant Difference

---

### Scheffe's Method

* Suppose we are interested in $m$ contrasts $c_1, \dots, c_m$
* We can widen the confidence intervals for each to control the experimentwise
error
* It's not obvious, but the appropriate scaling factor is
\begin{align}
\sqrt{\left(a - 1\right)F_{\frac{\alpha}{2}, a - 1, N - a}}
\end{align}

```{r, out.width = 500, fig.align = "center"}
include_graphics("https://uwmadison.box.com/shared/static/tmez3gdyre3lth822zm2wimmes3nujyy.png")
```

---

### Tukey's Honest Significant Difference

.pull-left[
* A common special case is when we're interest in all pairwise comparisons,

\begin{align}
\Gamma\left(c\right) = \mu_i - \mu_{i'}
\end{align}

* If we want to make confidence intervals, we should center them around,
\begin{align*}
\hat{\Gamma}\left(c\right) &= \bar{y}_i - \bar{y}_{i'}
\end{align*}

  but how wide should they be?
]

.pull-right[
<img src="https://uwmadison.box.com/shared/static/lc34pprk90p7evabiycjiw5c22xsm666.png" width=350/>
]

---

### Tukey's Honest Significant Difference

* Let $\bar{y}_{\max}$ and $\bar{y}_{\min}$ be the largest and smallest of the
group means
* Notice that
\begin{align}
\left|\bar{y}_i - \bar{y}_{i^\prime}\right| \leq \bar{y}_{\max} - \bar{y}_{\min}
\end{align}
* Therefore, we can rescale our confidence intervals based on the reference
distribution for the difference $\bar{y}_{\max} - \bar{y}_{\min}$

```{r, out.width = 500, fig.align = "center"}
include_graphics("https://uwmadison.box.com/shared/static/alo5fgwmkwrniwejs32ow05carx9uwna.png")
```

---

### Fisher's Least Significant Difference

* A final method, closely related to Tukey's HSD is Fisher's LSD
* It also tests for all pairwise differences, but does not control experimentwise error rate
* Notice that the variance of the differences between two group's means is,

\begin{aligned}
\operatorname{Var}\left(\bar{y}_{i}-\bar{y}_{i^\prime}\right) &=\operatorname{Var}\left(\bar{y}_{i}\right)+\operatorname{Var}\left(\bar{y}_{i^\prime}\right) \\
&=\frac{\sigma^{2}}{n_{i}}+\frac{\sigma^{2}}{n_{i'}} \\
& \approx \hat{\sigma}^{2}\left(\frac{1}{n_{i}}+\frac{1}{n_{i'}}\right)
\end{aligned}

---

### Fisher's LSD

Fisherâ€™s LSD compares each difference $\left|\bar{y}_i - \bar{y}_i'\right|$ to the cutoff,
\begin{align}
t_{\text {right }} \sqrt{\hat{\sigma}^{2}\left(\frac{1}{n_{i}}+\frac{1}{n_{i'}}\right)}
\end{align}
and rejects the null that the pairs have equal means if the difference is larger.

Unlike the two-sample $t$-test, it estimates variances using _all_ the groups,
not just the current pair under comparison.

---

# Code Implementation

---

### ANOVA Estimates

Before we construct any contrasts, we need to have an underlying ANOVA fit.

```{r, echo = TRUE}
library(readr)
etch_rate <- read_csv("https://uwmadison.box.com/shared/static/vw3ldbgvgn7rupt4tz3ditl1mpupw44h.csv")
etch_rate$power <- as.factor(etch_rate$power)
fit <- lm(rate ~ power, data = etch_rate)
aov_fit <- aov(fit)
summary(aov_fit)
```

---

### Defining and Testing a Contrast

* To fit a contrast, we can use the `fit.contrast` function from the `gmodels`
package.
* Which power levels are we comparing with this contrast?

```{r, echo = TRUE}
library(gmodels)
contrast <- c(1, -1, 0, 0)
fit.contrast(aov_fit, "power", contrast)
```

---

### Confidence Intervals

We can get a confidence interval for the contrast using the `confi.int`
parameter.

```{r, echo = TRUE}
fit.contrast(aov_fit, "power", contrast, conf.int = 0.95)
```

---

### Defining Many Contrasts

In a multiple testing setting, we can specify each contrast as a separate row in
a matrix.

```{r, echo = FALSE}
options(width = 300)
```

```{r, echo = TRUE}
contrasts <- matrix(
    c(1, -1, 0, 0,
      1, 1, -1, -1,
      0, 0, 1, -1),
    nrow = 3, byrow = TRUE
  )

fit.contrast(aov_fit, "power", contrasts, conf.int = 0.95)
```


---

### Scheffe's Method

* The Scheffe adjusted confidence intervals can be found using `PostHocTest`
from the `DescTools` package
* Make sure to set `method = "scheffe"`

```{r, echo = TRUE}
library(DescTools)
PostHocTest(aov_fit, method = "scheffe", contrast = t(contrasts))
```

---

### All Pairwise Tests

If we want all pairwise comparisons without any correction, we can use  `pairwise.t.test`

```{r, echo = TRUE}
pairwise.t.test(etch_rate$rate, etch_rate$power, p.adjust = "none", pooled.sd = FALSE)
```

---

### Tukey's Method

* All the intervals for Tukey's method can be found using the `TukeyHSD`
* Each row gives an interval for a $\mu_i - \mu_j$.

```{r, echo = TRUE}
TukeyHSD(aov_fit)
```

---

### Fisher's Method

* The `PostHocTest` function can also be used for Fisher's method
* Notice that the intervals are all narrower: we are not controlling the
experimentwise error

```{r, echo = TRUE}
PostHocTest(aov_fit, method = "lsd")
```

---

### Exercise

This walks through parts of problem 3.9.

.pull-left[
The tensile strength of Portland cement is being studied. Four different mixing
techniques can be used economically. An experiment was conducted and the
following data were collected.
]

.pull-right[
```{r, echo = TRUE}
cement <- data.frame(
  method = c("1", "2", "3", "4"),
  rep1 = c(3129, 3200, 2800, 2600),
  rep2 = c(3000, 3300, 2900, 2700),
  rep3 = c(2865, 2975, 2985, 2600),
  rep4 = c(2890, 3150, 3050, 2765)
)
cement
```
]

---

(1) Use `pivot_longer` to reshape the data so that the outcome is in a single
column.

(2) Use `lm` and `aov` to make an ANOVA table. Does the method detect any
difference in the means across the four groups?

(3) Use `PostHocTest` to compare between all pairs of means using Fisher's LSD.

(4) Make  QQ plot of the residuals. What can you conclude about the validity of
the ANOVA assumptions?

---

### Solution

(1) The `pivot_longer` method reshapes the data, creating a new column for the
replicate indicator.

```{r, echo = TRUE}
library(tidyr)
cement <- pivot_longer(cement, -method, names_to = "replicate")
head(cement)
```

---

### Solution

(2) Yes, since the $F$-statistic has a very small $p$-value, we can conclude that
there is a difference between the cement types.

```{r, echo = TRUE}
aov_table <- aov(lm(value ~ method, data = cement))
summary(aov_table)
```
---

### Solution

(3) We find 5 significant pairwise differences using Fisher's method, but these
should be interpreted with a grain of salt, since the method does not control
for multiple comparisons.

```{r, echo = TRUE}
PostHocTest(aov_table, method = "lsd")
```

---

### Solution

(4) There is an unusual jump in the residuals, possibly due to a difference in
the variance of the residuals across groups. This plot should be followed-up by
a direct plot of the residuals to see whether any transformations might help.

```{r, echo = TRUE, fig.width = 14, fig.height = 3.5}
qqnorm(resid(fit))
qqline(resid(fit), col = "red")
```