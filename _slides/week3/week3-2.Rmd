---
title: "Contrasts and Multiple Comparisons"
author: "Kris Sankaran | UW Madison | 23 September 2021"
output:
  xaringan::moon_reader:
    css: ["default", "css/xaringan-themer.css", "cols.css"]
    lib_dir: libs
    nature:
      beforeInit: "cols_macro.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false  
---

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE, dpi = 200, fig.width = 10, fig.height = 5, dev = 'svg', dev.args = list(bg = "transparent"))
theme424 <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16),
    legend.position = "bottom"
  )
theme_set(theme424)
```


# Contrasts and Multiple Comparisons

### Statistical Experimental Design

.large[Kris Sankaran | UW Madison | 16 September 2021]

---

### Today

* Book Sections: 3.5
* Online Notes: Week 3 [3] and [4]

---

### Limitation of ANOVA $F$-test

Recall the ANOVA model,

\begin{align}
y_{ij} = \mu + \tau_i + \epsilon_{ij} \\
\epsilon_{ij} \sim \mathcal{N}\left(0, \sigma^2\right)
\end{align}

and the associated hypothesis test,

\begin{align}
&H_0: \tau_1 = \dots = \tau_{a} = 0 \\
&H_1: \tau_{i} \neq 0 \text{ for at least one }i.
\end{align}

Note that it *does not* allow us to conclude which group(s) are responsible for
a rejection.

---

### Follow-up Tests

We may instead define a follow-up hypothesis test.

For example, imagine we had 4 groups,
* Are the first two means different from the last two? $H_0: \tau_1 + \tau_2 = \tau_3 + \tau_4$
* Are the first two means equal to each other? $H_0 : \tau_1 = \tau_2$
* ...

<img src="https://uwmadison.box.com/shared/static/ti2ukjamosvu0quvhgauvre1idm3mjfa.png"/>

---

### Contrasts

* Fortunately, the same framework covers all these cases.
* Let $\mu_i = \mu + \tau_i$ be the mean of group $i$.
* A contrast is a linear combination of the means,
\begin{align}
    \Gamma(c)=\sum_{i=1}^{a} c_{i} \mu_{i}
\end{align}
* The previous examples reduce to,

\begin{aligned}
&H_{0}: \Gamma(c)=0 \\
&H_{1}: \Gamma(c) \neq 0
\end{aligned}

for $c = (1, 1, -1, -1)$ and $c = \left(-1, -1, 0, 0\right)$, respectively.

---

### Plug-In Approximations

Since the sample means $\bar{y}_i \approx \mu_i$, we can approximate,

\begin{align*}
\hat{\Gamma}\left(c\right) := \sum_{i}c_i \bar{y}_i \approx \sum_{i}c_i \mu_i = \Gamma\left(c\right)
\end{align*}

Moreover, since $\hat{\sigma}^2 \approx \sigma^2$,

\begin{align*}
\widehat{\operatorname{Var}}\left(\hat{\Gamma}(c)\right) := \left[\sum_{i}c_i^2\right] \frac{\hat{\sigma}^2}{n} \approx \left[\sum_i c_i^2\right]\frac{\sigma^2}{n} = \text{Var}\left(\hat{\Gamma}(c)\right)
\end{align*}

---

### Reference Distribution

If the null hypothesis is true, the statistic is close to 0, because,

\begin{align*}
\hat{\Gamma}\left(c\right) \approx \Gamma\left(c\right) = 0
\end{align*}

In fact, under the null, it's possible to derive that
\begin{align*}
\frac{\hat{\Gamma}\left(c\right)}{\sqrt{\widehat{\operatorname{Var}}\left(\hat{\Gamma}(c)\right)}}
\end{align*}
is $t$-distributed with $N - a$ degrees of freedom (the proof is unimportant in
this class).

---

### Confidence Interval

Given this reference distribution, it's also possible to derive a confidence
interval,
\begin{align*}
    \left[\hat{\Gamma}(c)-t_{\mathrm{left}} \sqrt{\widehat{\operatorname{Var}}(\hat{\Gamma}(c))}, \hat{\Gamma}(c)+t_{\mathrm{left}} \sqrt{\widehat{\operatorname{Var}}(\hat{\Gamma}(c))}\right]
\end{align*}

* This quantifies the uncertainty in our estimate of $\hat{\Gamma}\left(c\right)$.
* In practice, we would never compute these statistics by hand.

---

### Multiple Comparisons

.pull-left[
* If we knew the interest contrasts in advance, we will be fine
* But what if we go in search for significant results using different contrasts?
]

.pull-right[
<img src="https://imgs.xkcd.com/comics/significant.png" width=200/>
]

---

### Multiple Comparisons

* We need to adapt our methodology to account for the search over contrasts
* The quantity of interest is the experimentwise error rate, the probability
that we get at least one false positive across the entire experiment
* Three methods, each with different properties,
  - Scheffe's method
  - Tukey's Honest Significant Difference
  - Fisher's Least Significant Difference

---

### Scheffe's Method

* Suppose we are interested in $m$ contrasts $c_1, \dots, c_m$
* We can widen the confidence intervals for each to control the experimentwise
error
* It's not obvious, but the appropriate scaling factor is
\begin{align}
\sqrt{\left(a - 1\right)F_{\frac{\alpha}{2}, a - 1, N - a}}
\end{align}

<img src="https://uwmadison.box.com/shared/static/tmez3gdyre3lth822zm2wimmes3nujyy.png"/>

---

### Tukey's Honest Significant Difference

* A common special case is when we're interest in all pairwise comparisons,

$$
\Gamma\left(c\right) = \mu_i - \mu_j
$$

* If we want to make confidence intervals, we should center them around,

\begin{align*}
\hat{\Gamma}\left(c\right) &= \bar{y}_i - \bar{y}_j.
\end{align*}

but how wide should they be?

<img src="https://uwmadison.box.com/shared/static/lc34pprk90p7evabiycjiw5c22xsm666.png"/>

---

### Tukey's Honest Significant Difference

* Let $\bar{y}_{\max}$ and $\bar{y}_{\min}$ be the largest and smallest of the
group means
* Notice that
\begin{align}
\left|\bar{y}_i - \bar{y}_j\right| \leq \bar{y}_{\max} - \bar{y}_{\min}
\end{align}
* Therefore, we can rescale our confidence intervals based on the reference
distribution for the difference $\bar{y}_{\max} - \bar{y}_{\min}$

<img src="https://uwmadison.box.com/shared/static/alo5fgwmkwrniwejs32ow05carx9uwna.png"/>

---

### Fisher's Least Significant Difference

Notice that the variance of the differences between two group's means is,

\begin{aligned}
\operatorname{Var}\left(\bar{y}_{i}-\bar{y}_{j}\right) &=\operatorname{Var}\left(\bar{y}_{i}\right)+\operatorname{Var}\left(\bar{y}_{j}\right) \\
&=\frac{\sigma^{2}}{n_{i}}+\frac{\sigma^{2}}{n_{j}} \\
& \approx \hat{\sigma}^{2}\left(\frac{1}{n_{i}}+\frac{1}{n_{j}}\right)
\end{aligned}

---

### Fisher's LSD

Fisherâ€™s LSD compares each difference $\left|y_i - y_j\right|$ to the cutoff,
\begin{align}
t_{\text {right }} \sqrt{\hat{\sigma}^{2}\left(\frac{1}{n_{i}}+\frac{1}{n_{j}}\right)}
\end{align}
and rejects the null that the pairs have equal means if the difference is larger.

Though, like Tukey's method, it considers all pairs of means, it *does not*
control the experimentwise error.

---

# Code Implementation

---

### ANOVA Estimates

Before we construct any contrasts, we need to have an underlying ANOVA fit.

```{r, echo = TRUE}
library(readr)
etch_rate <- read_csv("https://uwmadison.box.com/shared/static/vw3ldbgvgn7rupt4tz3ditl1mpupw44h.csv")
etch_rate$power <- as.factor(etch_rate$power)
fit <- lm(rate ~ power, data = etch_rate)
aov_fit <- aov(fit)
```

---

### Defining and Testing a Contrast

* To fit a contrast, we can use the `fit.contrast` function from the `gmodels`
package.
* Which power levels are we comparing with this contrast?

```{r, echo = TRUE}
library(gmodels)
contrast <- c(1, -1, 0, 0)
fit.contrast(aov_fit, "power", contrast)
```
---

### Confidence Intervals

We can get a confidence interval for the contrast using the `confi.int`
parameter.

```{r, echo = TRUE}
fit.contrast(aov_fit, "power", contrast, conf.int = 0.95)
```

---

### Defining Many Contrasts

In a multiple testing setting, we can specify each contrast as a separate row in
a matrix.

```{r, echo = FALSE}
options(width = 300)
```

```{r, echo = TRUE}
contrasts <- matrix(
    c(1, -1, 0, 0,
      1, 1, -1, -1,
      0, 0, 1, -1),
    nrow = 3, byrow = TRUE
  )

fit.contrast(aov_fit, "power", contrasts, conf.int = 0.95)
```

---

### Scheffe's Method

* The Scheffe adjusted confidence intervals can be found using `PostHocTest`
from the `DescTools` package
* Make sure to set `method = "scheffe"`

```{r, echo = TRUE}
library(DescTools)
PostHocTest(aov_fit, method = "scheffe", contrast = t(contrasts))
```

---

### Tukey's Method

* All the intervals for Tukey's method can be found using the `TukeyHSD` function.
* Each row gives an interval for a $\mu_i - \mu_j$.

```{r, echo = TRUE}
TukeyHSD(aov_fit)
```

---

### Fisher's Method

* The `PostHocTest` function can also be used for Fisher's method
* Notice that the intervals are all narrower: we are not controlling the
experimentwise error

```{r, echo = TRUE}
PostHocTest(aov_fit, method = "lsd")
```

---

### Exercise

This walks through parts of problem 3.9.

The tensile strength of Portland cement is being studied. Four different mixing
techniques can be used economically. An experiment was conducted and the
following data were collected.

```{r}
cement <- data.frame(
  method = c("1", "2", "3", "4"),
  rep1 = c(3129, 3200, 2800, 2600)
)
```


---