[
  {
    "path": "posts/2021-08-06-week5-3/",
    "title": "RCBD with Random Block Effects",
    "description": "The random effects analog of RCBD designs",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-07",
    "categories": [],
    "contents": "\nReadings 4.1, Rmarkdown\n\n\ninclude_graphics(\"https://uwmadison.box.com/s/gu58jurmhyraebsdyh4txlpyxegxn9xf\")\n\n\n\n\nAs in standard random effects, sometimes the blocks are from a larger population. For example, in the medical device example, we care about a ``resin effect,’’ but don’t care about each individual batch.\nThe model is setup as before,\n\\[\ny_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij}\n\\] except now both \\(\\beta_j \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\) and \\(\\epsilon_{ij} \\sim \\mathcal{N}\\left(0, 2\\right)\\), all independently.\nObservations are correlated within each block are correlated. The calculation to demonstrate this is similar to the one used to show that observations \\(i\\) within levels of an ordinary random effects model are correlated.\nHypothesis Testing\nWe’re interested in whether any of the treatments have an effect,\n\\[\\begin{align*}\nH_0 &: \\tau_1 = \\dots = \\tau_a = 0 \\\\\nH_{1} &: \\tau_{i} \\neq 0 \\text{ for at least one } i\n\\end{align*}\\]\nWe won’t show it, but it turns that \\[\\begin{align*}\n\\mathbf{E}\\left[MS_{\\text{Treatment}}\\right] &= \\sigma^2 + \\frac{b \\sum_{i = 1}^{a} \\tau_i^2}{a - 1} \\\\\n\\mathbf{E}\\left[MS_{\\text{Block}}\\right] &= \\sigma^2 + a \\sigma^2_{\\beta} \\\\\n\\mathbf{E}\\left[MS_{E}\\right] &= \\sigma^2\n\\end{align*}\\] so we should reject the null when \\(MS_{\\text{Treatment}}\\) is much larger than \\(MS_E\\).\nIn fact, as in the fixed block case, \\[\n\\frac{MS_{\\text{Treatment}}}{MS_{E}} \\sim F\\left(a - 1, \\left(a - 1\\right)\\left(b - 1\\right)\\right)\n\\] so we can use the same F-distribution cutoff when testing whether any treatment effects are nonzero.\nEstimation\nAs in random effects for the completely randomized design, we can use either the method of moments or maximum likelihood. The method of moments estimators are \\[\\begin{align*}\n\\hat{\\sigma}    &= MS_{E} \\\\\n\\hat{\\sigma}^2_{\\beta} &= \\frac{1}{a}\\left[MS_{\\text{Block}} - MS_{E}\\right]\n\\end{align*}\\]\nFinding confidence intervals continues to be a challenge for the method of moments approach. In this case, maximum likelihood is preferred. This method is shown in the computer example accompanying these notes.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:01:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-06-week5-4/",
    "title": "Latin Squares, part 1",
    "description": "An alternative to RCBDs that works with two nuisance factors",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-07",
    "categories": [],
    "contents": "\nReadings 4.2, 4.3, Rmarkdown\nRCBD is useful when we have one nuisance factor. But what if we have two?\nWe’re testing a manufacturing procedure, but raw materials come in batches and different operators have different skills.\nWe’re testing different diets on cows over a series of days, but there will both cow and day effects.\nAssume two nuisance factors, each with \\(p\\) levels. Furthermore, assume that we care about \\(p\\) different treatments.\nThe intuition for Latin Squares is similar to the intuition for RCBD,\n\nMake sure to run each treatment on each nuisance factor. For example, in the manufacturing example, each operator should see each treatment once, and each batch of materials should be used for each treatment\n\nSetup\nLatin Squares are \\(p\\times p\\) arrays, filled with \\(p\\) letters, so that each letter appears exactly once in each row and each column. Here are three tables when \\(p = 3\\).\n| A | B | C | | B | C | A | | C | A | B |\n| B | A | C | | A | C | B | | C | B | A |\n| C | B | A | | B | A | C | | A | C | B |\nThis is basically sudoku for statisticians\nWhy do we care? It tells us how we can implement the design idea above. First, randomly choose a \\(p\\times p\\) latin square\nThe rows are levels of the first blocking factor.\nThe columns are levels of the second blocking factor.\nThe letters are the treatment levels Then, the experiment consists of \\(p^2\\) runs, one for each of the pairs of blocking levels, with treatment specified by the cell’s label\nModel Setting\nInstead of just one set of block effects, we’ll have two sets, \\(\\alpha_i\\) and \\(\\beta_k\\). This results in, \\[\ny_{ijk} &= \\mu + \\alpha_i + \\tau_j + \\beta_k + \\epsilon_{ijk}\n\\] where \\(\\epsilon_{ijk} \\mathcal{N}\\left(0,\\sigma^2\\right)\\) independently. Note that each of the indices \\(i, j\\) and \\(k\\) range from \\(1, \\dots, p\\).\nTo make the model identifiable, we assume \\[\n\\sum_{i = 1}^{p} \\alpha_i = \\sum_{j = 1}^{p} \\tau_j = \\sum_{k = 1}^{p} \\beta_{k} = 0\n\\]\nHypothesis Testing\nOur test hasn’t changed at all, \\[\\begin{align*}\nH_0 &: \\tau_1 = \\dots = \\tau_a = 0 \\\\\nH_{1} &: \\tau_{i} \\neq 0 \\text{ for at least one } i\n\\end{align*}\\]\nBut now we need to account for block-to-block variation across both nuisance factors. The formula isn’t pretty, but it’s exactly analogous to the decompositions we’ve seen before,\n\\[\n\\begin{aligned}\n\\sum_{i=1}^{p} \\sum_{j=1}^{p} \\sum_{k=1}^{p}\\left(y_{i j k}-\\bar{y} . . .\\right)^{2}=& p \\sum_{i=1}^{p}\\left(\\bar{y}_{i . .}-\\bar{y} . . .\\right)^{2}+\\\\\n& p \\sum_{j=1}^{p}\\left(\\bar{y}_{\\cdot j} \\cdot \\bar{y}_{\\cdots} .\\right)^{2}+\\\\\n& p \\sum_{k=1}^{p}\\left(\\bar{y}_{\\cdot \\cdot k}-\\bar{y} . .\\right)^{2}+\\\\\n& \\sum_{i=1}^{p} \\sum_{j=1}^{p} \\sum_{k=1}^{p}\\left(y_{i j k}-\\bar{y}_{i . .}-\\bar{y}_{. j .}+\\bar{y} . .\\right)^{2}\n\\end{aligned}\n\\]\nThis is abbreviated as,\n\\[\\begin{align*}\nSS_{\\text{Total}} = &SS_{\\text{Rows}} + \\\\\n&SS_{\\text{Columns}} +\\\\\n&SS_{\\text{Treatments}} + \\\\\n&SS_{E}\n\\end{align*}\\] and we define * \\(MS_{\\text{Rows}}=\\frac{1}{p - 1}SS_{\\text{Rows}}\\) * \\(MS_{\\text{Treatments}}=\\frac{1}{p - 1}SS_{\\text{Treatments}}\\) * \\(MS_{\\text{Columns}}=\\frac{1}{p - 1}SS_{\\text{Columns}}\\) * \\(MS_{E}=\\frac{1}{(p - 1)(p - 2)}SS_{E}\\)\nIt turns out that \\[\n\\frac{MS_{\\text{Treatment}}}{MS_{E}} \\sim F\\left(p - 1, \\left(p - 1\\right)\\left(p - 2\\right)\\right)\n\\] which forms the basis for the test: reject the null if the ratio lies above the \\(1 - \\alpha\\) quantile of this \\(F\\)-distribution.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:01:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-06-week5-5/",
    "title": "Latin Squares, part 2",
    "description": "Extensions of Latin Squares",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-07",
    "categories": [],
    "contents": "\nReadings 4.2, 4.3, Rmarkdown\nWhen p is small, the test from the previous notes can have low power. If we can collect more than \\(p^2\\) samples, we should. But how exactly should the samples be collected, and how is the replicated design analyzed?\nThe design decision is context dependent,\n\nEquivalently, can keep rows, but different columns\n\nNote that in each case, we use a different latin square in each replicate.\n\nFortunately, the analysis is conceptually unified. We continue to have row, column, and treatment mean squares. Then, we have to add in replicate mean squares, to track variation from replicate-to-replicate. The code in each setting is the same,\nlm(Y ~ Replicate + W1 + W2 + X)\nGraceo-Latin Squares\nWe motivated Latin squares by asking how we can block 2 factors simultaneously. What if we have 3? We could go on forever…. That said, the transition from 2 to 3 is not hard\nIntroduce \\(p\\) greek letters to represent the third blocking factor. When \\(p=5\\), we would have \\(\\alpha, \\beta, \\gamma, \\delta, \\epsilon\\), for example.\nA Graeco-Latin square is like two Latin Squares overlaid on one another,\nwith one additional requirement: each latin and each greek letter must only appear together once. This last requirement is called orthogonality.\nA hypothesis test can be defined, by using the decomposition, \\[\nSS_{T} = SS_{\\text{Rows}} + SS_{\\text{Columns}} + SS_{\\text{Trewatments}} + SS_{\\text{Greek}} + \nSS_{E}\n\\] and noting that,\n\\[\n\\frac{MS_{\\text{Treatment}}}{MS_{E}} &= \\frac{\\frac{1}{p - 1}SS_{\\text{Treatment}}}{\\frac{1}{\\left(p - 1\\right)\\left(p - 3\\right)}SS_{E}} \\sim F\\left(p - 1, \\left(p - 1\\right)\\left(p - 3\\right)\\right)\n\\]\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:01:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week5-1/",
    "title": "Randomized Complete Block Design",
    "description": "Dealing with batch effects using a generalization of pairing.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-05",
    "categories": [],
    "contents": "\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/pq0v9wj2z63szngnxqm0mjfiq323q28k.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/35mmpml4az9ztv2wd5zp45mxiun11gja.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/rp2oe5pq19cxxvgps7hie04c3tni14r2.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/fgtpgaimnwwlm5s63bmj9f8ahe1nq4qp.png\")\n\n\n\n\nReadings 4.1, Rmarkdown\nHow can we generalize the idea of pairing to the case that we have more than two treatments to compare? Suppose that the nuisance factor is known and controllable. For simplicity, also assume that the blocks have the same number \\(a\\) of samples as we have treatments (\\(a=2 \\to\\) pairing)\nExample [page 120]. A medical device manufacturer creates vascular grafts, and wants to see the effect of manufacturing pressure on defect rate. But the resin used to make the graft arrives in batches, and may itself contribute to defects.\n\nIdea: Randomly assign each of the a treatments to the units within each block. This is called the RCBD.\n\nThis is a restriction on randomization. Only random treatment assignments where each block has each treatment are allowed. In the medical example, we would test all the manufacturing pressures on each of the resin batches, rather than having batches that receive only subsets of pressures.\nModel Setup\nFor hypothesis testing, we need a probability model. We suppose \\[\ny_{ij} = \\mu + \\tau_i + \\beta_{j} + \\epsilon_{ij}\n\\] where \\(\\epsilon_{ij} \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\) independently and \\(\\sum_{i}\\tau_i=\\sum_{j} \\beta_j=0\\) is required to ensure identifiability.\nAs before,\n\\(\\mu\\) is a global mean\n\\(\\tau_i\\) is the treatment effect for the \\(i^{th}\\) treatment level\n\\(\\epsilon_{ij}\\) is the error term\nBut now we also have * \\(\\beta_{j}\\), the effect of the \\(j^{th}\\) block\nNotice that in each block, we must have exactly \\(a\\) samples, one from each treatment.\nHypothesis Testing\nWe’re interested in whether any treatments have an effect, \\[\\begin{align*}\nH_0 &: \\tau_1 = \\dots = \\tau_a = 0 \\\\\nH_{1} &: \\tau_{i} \\neq 0 \\text{ for at least one } i\n\\end{align*}\\]\nThere’s a useful decomposition of the total variance that accounts for block-to-block variation, \\[\\begin{align*}\n\\sum_{i = 1}^{a}\\sum_{j = 1}^{b}\\left(y_{ij} - \\bar{y}_{\\cdot\\cdot}\\right)^2 = &b\\sum_{i = 1}^{a} \\left(\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot}\\right)^2 + \\\\\n& a \\sum_{j = 1}^{b}\\left(\\bar{y}_{\\cdot j} - \\bar{y}_{\\cdot\\cdot}\\right)^2 + \\\\\n&\\sum_{i = 1}^{a}\\sum_{j = 1}^{b} \\left(y_{ij} - \\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot j} + \\bar{y}_{\\cdot\\cdot}\\right)^2\n\\end{align*}\\] We’ll abbreviate this as \\(SS_{\\text{Total}} = SS_{\\text{Treatment}} + SS_{\\text{Block}} + SS_{E}\\).\nThe corresponding mean squares are,\n\\(MS_{\\text{Total}}= \\frac{1}{N - 1}SS_T\\), which is the sample variance of all \\(y_{ij}\\)\n\\(MS_{\\text{Treatment}}=\\frac{1}{a - 1}SS_{\\text{Treatment}}\\)\n\\(MS_{\\text{Block}}=\\frac{1}{b - 1}SS_{\\text{Block}}\\)\n\\(MSE=\\frac{1}{(a - 1)(b - 1)}SS_{E}\\), which a good estimator for \\(\\sigma^2\\)\nAs in usual ANOVA, if \\(MS_{\\text{Treatment}}\\) is much larger than MSE, then we have evidence against the null hypothesis. In fact, it turns out that, under the null,\n\\[\n\\frac{MS_{\\text{Treatment}}}{MS_{E}} \\sim F\\left(a - 1, \\left(a - 1\\right)\\left(b - 1\\right)\\right)\n\\] so we can reject the null hypothesis if this statistic is larger than the \\(1 - \\alpha\\) quantile of an \\(F\\) distribution with \\((a - 1, (a - 1)(b - 1))\\) d.f.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:00:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-06-week5-2/",
    "title": "RCBD Diagnostics",
    "description": "Multiple comparison and model checks for RCBDs",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-05",
    "categories": [],
    "contents": "\nReadings 4.1, Rmarkdown\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/qezu75a0hv1qhqyeoz8wb7sbb294vrsm.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/dmob9sjh8my7l8hez390o7gnn3vu1oe6.png\")\n\n\n\n\nMultiple Comparisons\nAs before, we may want to use contrasts to decide on which treatment effects are different We can continue to use the same multiple comparisons procedures as before, but have to account for a few differences,\nThe number of samples per treatment \\(n\\) is replaced by the number of blocks \\(b\\).\nThe d.f. for \\(MS_{E}\\) is now \\((a - 1)(b - 1)\\), not \\(N - a\\).\nFor example, the cutoff in Fisher’s LSD becomes \\[\nt_{\\frac{\\alpha}{2}, \\left(a - 1\\right)\\left(b - 1\\right)}\\sqrt{\\frac{2 MS_{E}}{b}}\n\\]\nModel Diagnostics\nThere are two key assumptions when we use RCBDs,\n\\(\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\) are independently distributed. Note that this also implies homoskedasticity (the variances are not changing from sample to sample).\nAdditivity (i.e. “no interaction”). The treatment effects \\(\\tau_i\\) need to be the same across all blocks.\nWe’ve seen the first assumption before, and can continue to use normal probability plots and residual analysis to check it.\nOne way to check additivity is to look at residuals, and see whether they are consistently lower / higher in some blocks.\nWhat can we do if we find an interaction effect?\nSometimes the interaction effects will disappear after transforming the response (e.g., using \\(\\log\\) or \\(\\sqrt\\))\nOtherwise, another design may be necessary. Factorial designs (to be discussed soon) allow for inference even in the presence of interactions.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:00:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week4-2/",
    "title": "Fitting Random Effects",
    "description": "Using the method of moments or maximum likelihood to estimate parameters",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-30",
    "categories": [],
    "contents": "\nReadings 3.9, Rmarkdown\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/f2e3b39odm4ejkyqinvpii24s9tegnqb.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/ujmby14i61prsex43i6drsgpgsu9lvjq.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/0au3iwj17u49ueqor5djqg58rensvmqo.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/kqgsua08wh8n2h6zhssbmxf4byldcq2o.png\")\n\n\n\n\nIn random effects models, there are three key parameters: \\(\\mu, \\sigma^2_{\\tau}, \\sigma^2\\). Two ways to fit them are (1) the method of moments and (2) maximum likelihood.\nMethod of Moments\nFor estimating \\(\\mu\\), this method uses the overall mean \\(\\bar{y}\\).\nWhat about the \\(\\sigma^2\\)’s? The key identity is, \\[\\begin{align*}\n\\mathbf{E}\\left[MS_{\\text{treatments}}\\right] &= \\sigma^2 + n \\sigma_{\\tau}^2 \\\\\n\\mathbf{E}\\left[MS_{E}\\right] &= \\sigma^2\n\\end{align*}\\]\nWe can approximate the expected values through, \\[\\begin{align*}\nMS_{\\text{treatments} \\approx \\sigma^2 + n \\sigma_{\\tau}^2 \\\\\nMS_{E} \\approx \\sigma^2\n\\end{align*}\\]\nIf we pretended these approximations were exact equalities, then we have two equations with two unknowns. The method of moments defines parameter estimates as the solutions to that system of equations. \\[\\begin{align}\n\\hat{\\sigma}^2 &= MS_{E} \\\\\n\\hat{\\sigma}^2_{\\tau} &= \\frac{1}{n}\\left[MS_{\\text{treatments}} - MS_{E}\\right]\n\\end{align}\\]\nHow can we get confidence intervals for these estimates?\nFor \\(\\hat{\\mu} = \\bar{y}\\), we can use \\(\\text{Var}\\left(y\\right) &= \\frac{1}{N} \\text{Var}\\left(y_{ij}\\right)=\\frac{1}{N}\\left(\\sigma^2 + n\\sigma^2_{\\tau}\\right)\\)\nFor \\(\\hat{\\sigma}^2\\), we can use the fact that \\(\\frac{N - a}{\\sigma^2}MS_{E} \\sim \\chi^2_{N - a}\\).\nFor $^2_{}, we’re out of luck, though some papers give approximate confidence intervals.\n\nMaximum Likelihood Estimation\nAn alternative approach is to use maximum likelihood. The first step is to stack all the \\(y_{ij}\\)’s into one long length \\(N\\) vector, and observe that the data are jointly normally distributed, \\[\n\\left(\\begin{array}{c}\ny_{11} \\\\\ny_{12} \\\\\n\\vdots \\\\\ny_{a(n-1)} \\\\\ny_{a n}\n\\end{array}\\right) \\sim \\mathcal{N}\\left(\\mu 1_{N},\\left(\\begin{array}{cccc}\n\\sigma^{2} I_{n}+\\sigma_{\\tau}^{2} 1_{n} 1_{n}^{T} & \\mathbf{0} & \\ldots & \\mathbf{0} \\\\\n\\mathbf{0} & \\sigma^{2} I_{n}+\\sigma_{\\tau}^{2} 1_{n} 1_{n}^{T} & \\ldots & \\mathbf{0} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n\\mathbf{0} & \\mathbf{0} & \\ldots & \\sigma^{2} I_{n}+\\sigma_{\\tau}^{2} 1_{n} 1_{n}^{T}\n\\end{array}\\right)\\right)\n\\]\nThe specific form of the covariance isn’t important. What is important is that we can exactly evaluate the probability of \\(y_{11}, y_{12},\\dots, y_{an}\\) under any choice of the parameters \\(\\mu, \\sigma^2, \\sigma^2_{\\tau}\\).\nDefine \\(L(\\mu, \\sigma^2, \\sigma^2_{\\tau})\\) to be the probability of the dataset \\(y_{11}, y_{12}, \\dots y_{an}\\) viewed as a function of the normal distribution’s parameters. A good estimate for these parameters comes from finding the configuration that maximizes \\(L\\left(\\mu, \\sigma^2, \\sigma^2_{\\tau}\\right)\\). The maximizers can’t be found analytically, but algorithms exist to find the maximizers.\nSoftware also gives a confidence interval for the estimates. It works by studying the curvature of the likelihood function at the maximizer, though we don’t need to worry about the details.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:00:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week4-3/",
    "title": "Nonparametric ANOVA",
    "description": "A model-free alternative to ANOVA.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-30",
    "categories": [],
    "contents": "\nReadings 3.11, Rmarkdown\nWhat can we do when the errors seem very far from normally distributed? Imagine all attempts to transform the data have failed. An alternative is to use nonparametric ANOVA.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/kdv2ayi2heooodf0a0j15swxld4ofll7.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/nn8v4t3ntvcsowvuy91b5lvoajju3ekf.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/a2evjeib97ay362iue2vhkxk18gbh7l4.png\")\n\n\n\n\nThe intuition is that, if all the groups have the same means, then no group should be consistently ranked higher than another. To implement this idea quantitatively,\nTransform the data to their ranks. The smallest of the \\(y_ij\\) becomes 1, the next smallest becomes 2, etc. Denote these ranks by \\(R_{ij}\\).\nCompute the test statistic \\[\n\\frac{1}{S^2}\\left[\\sum_{i = 1}^{a} \\frac{R_{i\\cdot}^2}{n_{i}} - \\frac{N\\left(N + 1\\right)^2}{4}\\right]\n\\] where we define \\(R_{i\\cdot}\\) to be the sum of the ranks in the \\(i^{th}\\) group, and \\[\nS^2 = \\frac{1}{N - 1} \\left[\\sum_{i, j} R_{ij}^2 - \\frac{N\\left(N + 1\\right)^2}{4}\\right].\n\\]\nCompare the test statistic to a \\(\\chi^2_{a -1}\\) reference distribution. If it seems too large to be plausible, reject the null hypothesis.\nWhere did this test statistic come from? It’s possible to show that the statistic is equivalent to\n\\[\n\\frac{\\sum_{i} n_{i}\\left(\\bar{R}_{i} - \\bar{R}\\right)^2}{\\frac{1}{N - 1}\\sum_{i, j} \\left(R_{ij} - \\bar{R}\\right)^2}\n\\] which compares the average rank in group \\(i\\) to the average rank overall, and standardizes by the overall variance of the ranks. The first formula is the one presented in the book, though, and it’s easier to calculate by hand.\nWhy not always use nonparametric ANOVA? If the data are actually normal, than this approach has less power than standard ANOVA. If you have doubts about validity, a safe approach is to try both. If the approaches approximately agree, default to standard ANOVA.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:00:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week4-1/",
    "title": "Random Effects",
    "description": "An introduction to random effects models",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-28",
    "categories": [],
    "contents": "\nReadings 3.9, Rmarkdown\nSometimes a factor has so many levels, that we can’t collect observations for all of them. Or, even if we could collect them, having one parameter for each would lead to a clumsy model.\nIn this ase, we typically settle for saying the effect of the factor on average, rather than than trying to estimating the effects of every single level of that factor.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/ig50uxftq5mz19t15w74kryh9ko0jm6e.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/f2e3b39odm4ejkyqinvpii24s9tegnqb.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/ujmby14i61prsex43i6drsgpgsu9lvjq.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/l9nclru13n0s5e52oiu1aasz66vbwr6m.png\")\n\n\n\n\nExamples,\nIs there a high school effect on college GPA? (instead of effects for individual schools)\nIs there a loom effect on fiber strength? (don’t care about individual looms)\n\nModel\nRandom effects models have the form, \\[\ny_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\n\\] where \\(\\tau_i \\sim \\mathcal{N}\\left(0, 2\\right)\\) and \\(\\epsilon_{ij} \\sim  \\mathcal{N}\\left(0, \\sigma^2\\right)\\). The crucial difference is that \\(i\\) is now thought of as random, not fixed.\nNotice that \\[\\begin{align}\n\\text{Var}\\left(y_{ij}\\right) &= \\text{Var}\\left(\\tau_j\\right) + \\text{Var}\\left(\\epsilon_{ij}\\right) \\\\\n&= \\sigma_{\\tau}^2 + \\sigma^2\n\\end{align}\\] More generally, the covariance matrix is block diagonal, with blocks of \\(\\sigma_{\\tau}^2\\) within groups.\nHypothesis Testing\nWe may want to test whether there is any variation in response across factor levels. Formally, \\[\nH_0: \\sigma_\\tau^2 &= 0 \\\\\nH_1: \\sigma^2 > 0\n\\]\nFor our test statistic, we can use the same one as before, \\(\\frac{MS_{\\text{treatments}}}{MS_{E}}\\) and for the same reasons as in fixed-effect ANOVA, this is \\(F\\)-distributed with \\((a - 1, N - a)\\) d.f. – we reject the null when this quantity is large.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:00:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week3-3/",
    "title": "Contrasts",
    "description": "Making pointed comparisons between treatment levels in ANOVA",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-23",
    "categories": [],
    "contents": "\nhttps://docs.google.com/document/d/12jrrcuA4oGYoVqSR-eqNhWYf0pW09ISaMgibEugwf8w/edit\n\n\ninclude_graphics(\"https://www.airmeet.com/event/3124e6e0-8b3d-11eb-adfc-b1c12ad96800?code=ffbe7ca8-0134-473b-9f02-83856e75c8f4\")\ninclude_graphics(\"https://www.airmeet.com/event/3124e6e0-8b3d-11eb-adfc-b1c12ad96800?code=ffbe7ca8-0134-473b-9f02-83856e75c8f4\")\n\n\n\n\nReadings 3.5, Rmarkdown\nWhat is a contrast? When we reject the null in ANOVA, we know at least one of the treatments deviates from the global average. But which one(s)?\nContrasts address this question. A contrast is a linear combination of the means, \\[\n\\Gamma(c)=\\sum_{i=1}^{a} c_{i} \\mu_{i}\n\\] For any particular \\(c\\), we test \\[\n\\begin{aligned}\n&H_{0}: \\Gamma(c)=0 \\\\\n&H_{1}: \\Gamma(c) \\neq 0\n\\end{aligned}\n\\]\nTo see that this is actually something worth doing, consider the case that we have 4 different means, \\(\\mu_1, \\mu_2, \\mu_3, \\mu_4\\),\nc = (1, 1, -1, -1): Are the first two means different from the last two, on average?\nc = (1, -1, 0, 0): Are the first two means equal to each other?\netc.\n\nTesting Contrasts\nRemember the hypothesis testing recipe. We need, (a) a test statistic and (b) a reference distribution for that statistic.\nOur best guess at \\(\\mu_i\\) is \\(\\bar{y}_i\\), so a reasonable statistic is,\n\\[\n\\hat{\\Gamma}(c)=\\sum_{i=1}^{a} c_{i} \\bar{y}_{i}\n\\]\nHow will we find its reference distribution? Under the null, this statistic is normally distributed with mean 0 and variance, \\[\n\\begin{aligned}\n\\operatorname{Var}(\\hat{\\Gamma}(c)) &=\\sum_{i=1}^{a} c_{i}^{2} \\operatorname{Var}\\left(\\bar{y}_{i}\\right) \\\\\n&=\\frac{\\sigma^{2}}{n} \\sum_{i=1}^{a} c_{i}^{2}\n\\end{aligned}\n\\] Standardizing our original statistic, we obtain,\n\\[\\begin{aligned}\n\\frac{\\hat{\\Gamma}(c)}{\\sqrt{\\operatorname{Var}(\\hat{\\Gamma}(c))}} &=\\frac{\\sum_{i=1}^{a} c_{i} \\bar{y}_{i}}{\\sqrt{\\frac{\\sigma^{2}}{n} \\sum_{i=1}^{a} c_{i}^{2}}} \\\\\n& \\approx \\frac{\\sum_{i=1}^{a} c_{i} \\bar{y}_{i}}{\\sqrt{\\frac{\\hat{\\sigma}^{2}}{n} \\sum_{i=1}^{a} c_{i}^{2}}}\n\\end{aligned}\\]\nTo estimate \\(\\sigma^2\\), we can use \\(\\hat{\\sigma}^2 := MS_E\\). This is a good choice, because it remains valid even when the null is untrue.\nSince we plugged-in the estimate \\(\\hat{\\sigma}^2\\), we have divided our normal distribution by the square root of a chi-square. The result is therefore t-distributed, with \\(N - a\\) df.\nConfidence Intervals for Contrasts\nIf we make the same computations as above, but without assuming that the null is true, we would find that, \\[\n\\mathbf{P}\\left(\\frac{\\sum_{i=1}^{a} c_{i} \\bar{y}_{i}-\\sum_{i=1}^{a} c_{i} \\mu_{i}}{\\sqrt{\\frac{\\hat{\\sigma}^{2}}{n} \\sum_{i=1}^{a} c_{i}^{2}}} \\in\\left[t_{\\text {left }}, t_{\\text {right }}\\right]\\right)=0.95\n\\] where we choose \\(t_{\\text{left}}\\) and \\(t_{\\text{right}}\\) to be the 0.025 and 0.975 quantiles of a \\(t\\)-distribution with \\(N - a\\) df.\nThe resulting confidence interval is,\n\\[\n\\left[\\sum_{i=1}^{a} c_{i} \\bar{y}_{i}-t_{\\text {right }} \\sqrt{\\frac{\\hat{\\sigma}^{2}}{n} \\sum_{i=1}^{a} c_{i}^{2}}, \\sum_{i=1}^{a} c_{i} \\bar{y}_{i}+t_{\\text {right }} \\sqrt{\\left.\\frac{\\hat{\\sigma}^{2}}{n} \\sum_{i=1}^{a} c_{i}^{2}\\right]}\\right.\n\\] This is an explicit formula that you can use in your computations, but don’t let the complexity of the symbols here confuse you. Returning to our original definitions, this is just, \\[\n\\left[\\hat{\\Gamma}(c)-t_{\\mathrm{left}} \\sqrt{\\widehat{\\operatorname{Var}}(\\hat{\\Gamma}(c))}, \\hat{\\Gamma}(c)+t_{\\mathrm{left}} \\sqrt{\\widehat{\\operatorname{Var}}(\\hat{\\Gamma}(c))}\\right]\n\\] where we’re writing \\(\\hat{\\text{Var}}\\) instead of \\(\\text{Var}\\) because we’re plugging in the estimate \\(\\hat{\\sigma}^2\\).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:00:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week3-4/",
    "title": "Multiple Comparisons",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-23",
    "categories": [],
    "contents": "\nReadings 3.5, Rmarkdown\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/tmez3gdyre3lth822zm2wimmes3nujyy.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/lc34pprk90p7evabiycjiw5c22xsm666.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/alo5fgwmkwrniwejs32ow05carx9uwna.png\")\n\n\n\n\nIf we planned in advance which contrasts we want to use, we are fine. But what if we hadn’t planned any, and go in search for significant results using different contrasts?\nImagine testing 100 hypothesis at = 0.05. Suppose they are all null). We would see 5 rejected null hypotheses on average.\nTherefore, if we want to allow ourselves some flexibility in searching over contrasts, we need to adapt our methodology. We should control the experimentwise error rate, the probability that any test results in a false positive.\nScheffe’s Method\nLet’s say we’re interested in \\(m\\) contrasts, \\(c_1, ,c_m\\). The idea is to widen our confidence intervals slightly, to make false positives rarer.\nHow much should the intervals be widened? It’s not obvious, but Scheffe found that we should multiply the endpoints of each of our \\(m\\) intervals by \\[\n\\sqrt{\\left(a - 1\\right)F_{0.025, a - 1, N - a}}\n\\] (this is for 95% confidence intervals).\nTukey’s Method\nIf we only care about the differences between pairs of group means, we can use Tukey’s method. All the contrasts now have the form, \\[\n\\Gamma\\left(c\\right) = \\mu_i - \\mu_j\n\\]\nWe’re going to make confidence intervals for these, and it’s natural to center them around, \\[\n\\hat{\\Gamma}\\left(c\\right) &= \\bar{y}_i - \\bar{y}_j.\n\\]\nHow wide should the intervals be? Tukey found a reference distribution for \\[\n\\frac{\\bar{y}_{\\max }-\\bar{y}_{\\min }}{\\frac{\\hat{\\sigma}}{\\sqrt{n}}}\n\\] where \\(\\bar{y}_{\\text{max}}\\) refers to the maximum group’s average across the \\(a\\) groups. From there, he tabulated the quantiles as \\(q_{\\alpha}(a, \\text{df})\\).\nIt turns out that the appropriate width of the confidence intervals can be derived from these quantiles, \\[\n\\left[\\left(\\bar{y}_{i}-\\bar{y}_{j}\\right)-q_{\\alpha}(a, \\text{df}) \\frac{\\hat{\\sigma}}{\\sqrt{n}},\\left(\\bar{y}_{i}-\\bar{y}_{j}\\right)+q_{\\alpha}(a, \\text{df}) \\frac{\\hat{\\sigma}}{\\sqrt{n}}\\right]\n\\]\nThis works because if the difference between the max and min group averages is contained within this interval, then all pairs \\(i, j\\) of differences are also contained in this interval.\nFisher’s Least Significant Difference\nFisher’s LSD is used to compare pairs of means. Unlike Tukey’s method, it doesn’t control the experimentwise error rate\nNotice that the variance of the differences is \\[\n\\begin{aligned}\n\\operatorname{Var}\\left(\\bar{y}_{i}-\\bar{y}_{j}\\right) &=\\operatorname{Var}\\left(\\bar{y}_{i}\\right)+\\operatorname{Var}\\left(\\bar{y}_{j}\\right) \\\\\n&=\\frac{\\sigma^{2}}{n_{i}}+\\frac{\\sigma^{2}}{n_{j}} \\\\\n& \\approx \\hat{\\sigma}^{2}\\left(\\frac{1}{n_{i}}+\\frac{1}{n_{j}}\\right)\n\\end{aligned}\n\\]\nFisher’s LSD compares each difference \\(\\left|y_- y_j\\right|\\) to the cutoff, \\[\nt_{\\text {right }} \\sqrt{\\hat{\\sigma}^{2}\\left(\\frac{1}{n_{i}}+\\frac{1}{n_{j}}\\right)}\n\\] and rejects the null that the pairs have equal means if the difference is larger.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:00:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week3-1/",
    "title": "ANOVA",
    "description": "The ANOVA model and sum-of-squares decomposition",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-21",
    "categories": [],
    "contents": "\nReadings 3.1 - 3.3, Rmarkdown\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/h3bbv7cjo0our7syhlsc0tdn5c22xyme.png\n\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/a8jqduhcmjzj9re22a81236k3enbtzzn.png\n\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/znpaeugwi14nxhuvz2lkmoo2ovv2jtx1.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/n836t4q718m2o16hluvglplqi4lshgcm.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/7prsedegwnp6wdghcw6vfv2rs7zsr7nj.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/l40cisieegn7u37ite50eu1yeoed8v88.png\")\n\n\n\n\nANOVA is used when we want to compare the effect of different treatments on a continuous response. For example,\nHow does the etch rate of a tool depend on its power setting?\nHow do an opera company’s different donation strategies compare with one another?\nHow does the average rental time compare across cars?\nVisually, it can be used whenever the data can be arranged as a series of boxplots.\nModel and Test Setup\nFormally, consider the model\n\\[\ny_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\n\\] where \\(i=1 \\dots a\\) and \\(j=1, \\dots, n\\) and the errors \\(\\epsilon_{ij} \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\) are independent.\n\\(i\\) indexes different groups\n\\(j\\) indexes the samples within groups.\n\\(a\\) is the number of groups\n\\(n\\) is the number of samples in each group\n\\(N=na\\) is the total number of samples\nOur null hypothesis is that none of the groups deviate from the global mean. The alternative is that at least one of the groups is different. Formally,\n\\[\nH_0: \\tau_1 = \\dots, = \\tau_{a} = 0 \\\\\nH_1: \\tau_{i} \\neq 0 \\text{ for at least one }i.\n\\]\nImportant Identities\nThe word “analysis” in ANOVA is used in the classical sense of to break something into its parts1. ANOVA breaks the observed variation into distinct components,\n\\[\n\\sum_{ij} \\color{#ff8200}{\\left(y_{ij} - \\bar{y}\\right)}^2 = n\\sum_{i} \\color{#447583}{\\left(\\bar{y}_i - \\bar{y}\\right)}^2 + \\sum_{i,j} \\color{#b090c2}{\\left(y_{ij} - \\bar{y}_{i}\\right)}^2\n\\] which is abbreviated as\n\\[\n\\color{#ff8200}{SS_{\\text{total}}} = \\color{#447583}{SS_{\\text{treatments}}} + \\color{#b090c2}{SS_{\\text{error}}}.\n\\] This is usually called the “ANOVA identity.”\nIf any of the groups had a mean that was different from the global mean, then you’d expect the  term to be larger than it would otherwise be. How large is large enough to reject?\nSince the variance within each group is \\(\\sigma^2\\), the variance of each \\(\\bar{y}_i\\) is \\(\\frac{\\sigma^2}{n}\\). The blue term looks like how we would usually estimate the variances of the \\(y_i\\), i.e.,\n\\[\n\\frac{1}{a}\\sum_{i}\\color{#447583}{\\left(y_i - \\bar{y}\\right)}^2 \\approx \\frac{\\sigma^2}{n}\n\\] 7. On the other hand, under the null, all the \\(y_{ij} \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\\), so we would also know,\n\\[\n\\frac{1}{N-a} \\sum_{i, j}\\color{#b090c2}{\\left(y_{i, j}-\\bar{y}_{i}\\right)}^{2} \\approx \\sigma^{2},\n\\]\nso under the null,\n\\[\n\\frac{\\frac{\\color{#447583}{SS_{\\text {treatments }}}}{a-1}}{\\color{#b090c2}{\\frac{SS_{\\text {error }}}{N-a}}} \\approx 1.\n\\]\nNote that under the alternative, it would be larger than 1.\nFrom our results in the probability review lectures, both the  and  are chi-squares, with \\(a - 1\\) and \\(N - a\\) d.f., respectively. It’s not obvious, but they’re also independent (this is called Cochran’s theorem). Therefore, the null reference distribution is an \\(F\\) distribution with \\((a - 1, N - a)\\) d.f.\nThe ANOVA Table\nIf we introduce \\(\\color{#447583}{MS_{\\text{treatments}}}\\) and \\(\\color{#b090c2}{MS_{E}}\\) for the numerator and denominator in the ratio above, we can organize this information as,\nThis is also how most computer output will be displayed, so it’s worth being familiar with.\n\ni.e. the oppositive of “synthesis.”↩︎\n",
    "preview": {},
    "last_modified": "2021-08-13T16:00:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week3-2/",
    "title": "Model Checking",
    "description": "How should we check the assumptions of the ANOVA model?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-21",
    "categories": [],
    "contents": "\nReadings 3.4, Rmarkdown\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/ka2t5b3awtqt0mqdqtm8o5zbzs8ur7xp.png\")\n\n\n\n\nRecall the ANOVA model, \\[\ny_{i j}=\\mu+\\tau_{i}+\\epsilon_{i j}\n\\] with independent errors \\(\\epsilon_{ij} \\sim \\mathcal{N}\\left(0,  \\sigma^2\\right)\\). There are a few ways that this model can fail,\nThere might be systematic variations besides the group deviations \\(\\tau_i\\).\nThe errors might not be normally distributed\nThe errors might not be independent\nThe variance might not be the same in each group\n\nTo see if the model is okay, it will be helpful to define residuals, \\[\\begin{align*}\ne_{i j} &=y_{i j}-\\hat{y}_{i j} \\\\\n&=y_{i j}-\\bar{y}_{i}.\n\\end{align*}\\] Residuals are our best guess of what the true random error \\(\\epsilon_{ij}\\) is like.\nNormal Probability Plots\nWe can’t check normality of \\(\\epsilon_{ij}\\) directly, but we can check normality of the residuals \\(e_{ij}\\) using normal probability plots.\nIn the plot to the right, it looks like the data are somewhat more tightly clustered near the middle, though the most extreme points are more extreme than expected. Do you see why?\nOf the ways that the model can fail, normality of the residuals is not the most severe, because you can often count on the central limit theorem to make the reference \\(F\\) distribution still approximately correct.\nPlotting Residuals\nThe way to check for systematic variation beyond the \\(\\tau_i\\)’s, try plotting residuals against measured variables. If you see “patterns,” those may correspond to missing terms in the model.\nWe can use plots to check for independence. For example, if you plot the residuals over time and you see clear trends, then the errors are likely correlated over time.\nIt’s often useful to plot residuals against the fitted values. This can reveal nonconstant variance across the groups \\(i\\).\nTesting Equality of Variances\nThere are formal tests to test whether the equal variance assumption of the ANOVA is valid (it’s very meta). The most common are,\nBartlett’s test\nThe Modified Levene test\n\nThe main difference is that the Modified Levene test is still valid even when the errors are not normally distributed. You don’t need to memorize the test statistics, but know that they exist, and be able to interpret associated computer output.\nTransformations\nWhat can you do if you detect nonconstant variance across groups? The most common fix is to use a variance stabilizing transformation. That is, apply some function \\(f(x)\\) to each data point and then perform the ANOVA.\nThere are various rules of thumb1, though the process is still somewhat informal,\n\\(f(x) = \\sqrt{x}\\) or \\(f(x) = \\sqrt{1 + x}\\) if the data seem Poisson\n\\(f(x) = \\log x\\) if the data seem lognormal\n\\(f(x) = \\arcsin\\left(\\sqrt{x}\\right)\\) if the data are binomial-derived fractions\n\nThese are special cases. More generally, if you notice \\(\\sigma \\propto \\mu^\\alpha\\), then setting \\(f(x) = x^{1 - \\alpha}\\) will stabilize the variance.\n\nIt’s not at all obvious why any of these transformations are effective – they are typically derived in introductory mathematical statistics courses.↩︎\n",
    "preview": {},
    "last_modified": "2021-08-13T16:00:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week2-3/",
    "title": "Testing Differences in Means",
    "description": "The basic principles of hypothesis testing.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-16",
    "categories": [],
    "contents": "\nReadings 2.4, Rmarkdown\nStatistics is about making general conclusions based on specific evidence. One approach is based on hypothesis testing: we have a theory about the general (a null hypothesis), and we want to see whether our specific sample is consistent with that theory. This philosophy is made quantitative by following a standard recipe,\nPose a null hypothesis about the world\nDefine a test statistic that should detect departures from that null hypothesis\nDetermine a reference distribution for that test statistic\nCompute the test statistic on your data, and see if it’s plausibly a draw from your reference distribution\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/e2vep3vvfvnz8v4kiilim1pjxe2w2upr.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/66793m770ob31z53fpgc4xoci4y0s7fi.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/r9dzi8ar8l7ic7h2xarnrdiwoab6tojz.png\")\n\n\n\n\nProceeding in this way, there are a few types of error\n\nTested rejected\nTest didn’t reject\nNull is true\nFalse alarm\nCorrect\nNull is false\nCorrect\nMissed detection\n\\(p\\)-values. “Rejected” or “Not rejected” is only a very coarse description of how the data conforms to a theory. \\(p\\)-values give a measure of the degree of (im)plausibility of a test statistic under a given null hypothesis. The specific measure of plausibility will depend on the form of the test – we will see a specific example in the next set of notes.\nTwo Sample t-test\nMotivating example: You have two ways of making concrete mortar. Is one stronger than the other? By default, you think they are equally strong. We can denote this (the null hypothesis) by,\n\\[\nH_0: \\mu_1 = \\mu_2\n\\]\nThe alternative hypothesis is that the strengths are not equal,\n\\[\nH_1: \\mu_1 \\neq \\mu_2\n\\]\nOur test statistic for detecting departures from this null will be,\n\\[\nt_0 := \\frac{\\bar{y}_1 - \\bar{y}_2}{S_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\] where we define the pooled standard deviation by,\n\\[\nS^2_p = \\frac{\\left(n_1 - 1\\right)S_1^2 + \\left(n_2 - 1\\right)S_2^2}{n_1 + n_2 -2}\n\\] and \\(S_1\\) and \\(S_2\\) are the usual standard deviations for each group individually. (consider what happens when \\(n_1 = n_2 = n\\))\nUnder the null hypothesis, this is a ratio between a standard normal and chi-square, so \\(t_0\\) is \\(t\\)-distributed with \\(n - 2\\) d.f. This gives reference distribution under the null.\nConfidence intervals\nInstead of thinking we know the mean and trying to reject it, why don’t we try to directly estimate it (with an error bar)? A 95% confidence interval is an interval \\([L, U]\\) satisfying,\n\\[\n\\mathbf{P}\\left(\\theta \\in \\left[L, U\\right]\\right) = 0.95\n\\]\nThe randomness here is in \\(L\\) and \\(U\\). If we were being more formal, we would write those as functions of the (random) sample,\n\\[\n\\left[L\\left(y_1, \\dots, y_n\\right), U\\left(y_1, \\dots, y_n\\right)\\right]\n\\]\nTo construct one for the two sample test, recall that, based on the \\(t\\)-distribution characterization from the probability-review lecture,\n\\[\nP\\left(\\left(\\frac{\\bar{y}_1 - \\bar{y}_2\\right) - \\left(\\mu_1 - \\mu_2\\right)}{S_p\\sqrt{\\frac{2}{n}}} \\in \\left[t_{0.025, 2\\left(n - 1\\right), t_{0.975, 2\\left(n - 1\\right)}}\\right]  \\right) = 0.95\n\\]\nTo simplify the algebra, let\n\\[\nT\\left(y\\right) := \\bar{y}_1 - \\bar{y}_2 \\\\\n\\theta := \\mu_1 - \\mu_2 \\\\\n\\hat{\\sigma} := S_p\\sqrt{\\frac{2}{n}} \\\\\nt_{0.025, 2\\left(n - 1\\right)} := t_{\\text{left}} \\\\\nt_{0.975, 2\\left(n - 1\\right)} := t_{\\text{right}}\n\\] so that the above expression reduces to,\n\\[\n\\mathbf{P}\\left(\\frac{T\\left(y\\right) - \\theta}{\\hat{\\sigma}} \\in \\left[t_{\\text{left}}, t_{\\text{right}}\\right]\\right) = 0.95\n\\]\nNow, if we rearrange terms, we find\n\\[\n\\mathbf{P}\\left(\\theta \\in \\left[T\\left(y\\right) - \\hat{\\sigma}t_{\\text{right}}, T\\left(y\\right) + \\hat{\\sigma}t_{\\text{left}}\\right]\\right) = 0.95\n\\] We can use the fact that \\(t_{\\text{left}} = t_{\\text{right}}\\) to simplify the expression further to\n\\[\n\\mathbf{P}\\left(\\theta \\in \\left[T\\left(y\\right) - \\hat{\\sigma}t_{\\text{right}}, T\\left(y\\right) + \\hat{\\sigma}t_{\\text{right}}\\right]\\right) = 0.95\n\\] This is exactly the property that a confidence interval has to satisfy. Plugging in the original expressions gives the confidence interval for the difference in means, assuming shared variance.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:00:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week2-4/",
    "title": "Diagnostics and Power",
    "description": "Tricks to make sure tests aren't applied blindly",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-16",
    "categories": [],
    "contents": "\nReadings 2.4, Rmarkdown\nThe reference distribution depends on three assumptions,\nSamples are independent. If they aren’t, then we’re pretending we have more samples than we actually do.\nThe standard deviations are equal.\nThe populations are normally distributed.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/n1a3bdzspet06ibsd1yebc1r6w7kzf3o.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/uzh72egfwwp251xzac932cej6kna1woz.png\")\n\n\n\n\nWe can check the last two assumptions using something called a normal probability plot. This plots the sample quantiles against the theoretical normal distribution’s quantiles.\nPower Analysis\nPeople will often call you asking about what a good sample size is for their experiment. A good way to answer this is to compute the power curves as a function of different signal strengths.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/06qu4t1q6jemmwto01vgd95jtzd0if3e.png\")\n\n\n\n\nOf course, we can never know the signal strength in advance. But we can test a few different plausible ranges, based on past experience.\nImportant Variations\nWhat if the variances are not equal? Our test statistic used a pooled standard deviation. If the variances aren’t equal, we could standardize differently,\n\\[\n\\frac{\\bar{y}_1 - \\bar{y}_2}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}.\n\\]\nThis is unfortunately not exactly \\(t\\)-distributed under the null. That said, the reference distribution can be well approximated by one, and almost any statistical package will let you compute corresponding \\(p\\)-values and confidence intervals.\nWhat if the variances are known? In this case, we can avoid using \\(S_1\\) and \\(S_2\\). Instead, we ought to standardize using the known standard deviations. Since there’s no additional randomness coming from estimation, the reference distribution is a standard normal, not a \\(t\\)-distribution.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:00:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week2-1/",
    "title": "Probability Review",
    "description": "Probability distributions, their properties, and relationships.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-14",
    "categories": [],
    "contents": "\nReadings , Rmarkdown\nThe most basic idea of statistics is that if you ran an experiment again, you would get different results i.e., there is randomness Probability is the calculus of randomness\nDefinitions\nIf \\(y\\) is a discrete random variable taking on values \\(y_{k}\\) with probability \\(p_{k}\\), then its mean is defined as \\(\\matbf{E}\\left[y\\right] = \\sum_{k} p_{k}y_{k}\\). If it is a continuous variable with density \\(p\\left(y\\right)\\), the corresponding quantity is \\(\\mathbf{E}\\left[y\\right] = \\int_{\\mathbf{R}} y p\\left(y) dy\\). Think of integral in the continuous case like the limit of a Riemann sum in calculus\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/rrvufphhkjkvqjkwgvu4pj712hn9tigt.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/8truwe0fd247iuq7m5p56f2ylx4nmewn.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/plbcevhwx1sq8f9rzvn1vdwr7x70syk5.png\")\n\n\n\n\nTo build intuition about this formula, consider some special cases,\nIf there are just two values with equal probability, it’s just a midpoint\nIf one of the probability weights is larger, it’s closer to the larger weight\nIf you have many values, it’s closer to the ones with large weight\nThe variance of a random variable Y is defined as \\(\\mathbf{V}\\left[y\\right] = \\mathbf{E}\\left[y - \\mathbf{E}\\left[y\\right]\\right]^2\\). This measures the typical distance of \\(Y\\) around its mean.\nUseful properties\nFor calculations, it’s often easier to use properties of mean and variance to reduce to simpler expressions, rather than using the formulas above. For example, expectation is a linear function,\n\\[\n\\mathbf{E}\\left[c_{1}y_{1} + c_{2}y_{2}\\right] = c_{1}\\mathbf{E}\\left[y_{1}\\right] + c_{2}\\mathbf{E}\\left[y_{2}\\right].\n\\]\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/pvfua6rtht3vubmz50yi3xry02t2elo3.png\")\n\n\n\n\nVariance is not linear, but the variance of a linear combination of two random variables can be found simply enough,\n\\[\n\\mathbf{V}\\left[c_1 y_1 + c_2 y_2\\right] = c_1^2 \\mathbf{V}\\left[y_1\\right] + \n  c_2^2 \\mathbf{V}\\left[y_2\\right] +\n  c_1 c_2 \\textbf{Cov}\\left[y_1, y_2\\right]\n\\]\nwhere we define the covariance as,\n\\[\n\\textbf{Cov}\\left[y_1, y_2\\right] = \\mathbf{E}\\left[\\left(y_1 - \\mathbf{E}\\left[y_1\\right]\\right)\\left(\ny_2 - \\mathbf{E}\\left[y_2\\right]\\right)\\right]\n\\]\nSampling and Estimators\nWhy is probability useful in statistics. From a high-level, statistics is concerned with drawing inferences from the specific to the general. Starting from a sample, we would like to say something true about the population. A typical strategy is to compute a statistic (a function of the sample) to say something about the probability distribution that it was drawn from (a property of the population).\nSuppose we have observed \\(n\\) samples \\(y_{1}, \\dots, y_{n}\\). Two very useful statistics are the sample mean,\n\\[\n\\bar{y} = \\frac{1}{n}\\sum_{i = 1}^{n}y_i\n\\] and the sample standard deviation \\[\nS = \\sqrt{\\frac{1}{n - 1}\\sum_{i = 1}^{n}\\left(y_i - \\bar{y}\\right)^2}\n\\]\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/o3li54yw6986aurviiljfmgmrmuh5wms.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/310ij7e4ki9xnn4doe191hks8k6h3o9z.png\")\n\n\n\n\nand the sample standard deviation,\nStatisticians have come up with a variety of properties that they would like their statistics to satisfy. Two common requirements are that the statistic be “unbiased” and “minimum variance.” Unbiased means it’s centered around the correct value, on average Minimum variance means it’s not too far from the correct value, on average.\nCentral limit theorem\nFor very many distributions, an appropriately rescaled version of the sample mean converges to a normal distribution. Specifically, if all the \\(y_i\\) are drawn i.i.d. from some distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then\n\\[\n\\frac{\\sqrt{n}\\left(\\bar{y} - \\mu\\right)}{\\sigma} \\to \\mathcal{N}\\left(0, 1\\right).\n\\]\nThis phenomenon is called the central limit theorem.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/2nue43dk0rw0v25swmbd552qk1bz1jze.png\")\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:00:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week2-2/",
    "title": "Common Distributions",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-14",
    "categories": [],
    "contents": "\nhttps://docs.google.com/document/d/1fJxl3kNL0EdbMTHMQub3IiKqthXqKdW6HUoB4s8gw9I/edit#\nReadings 2.3, Rmarkdown\nDon’t try to memorize the formulas for all the probability distributions! Instead, it’s much more useful to learn,\nThe relationships between distributions\nThe basic shapes of the distributions (unimodal? nonnegative? …)\nHow their shapes change then their parameters are changed We’ll give a refresher of some common probability distributions in these notes.\nChi-Square Distribution. This distribution arises as the sum of squares of standard normals. That is, if \\(z_[k] \\sim \\mathcal{N}\\left(0, 1\\right)\\), then \\(\\sum_{k} z_{k}^2 \\sim \\chi^2_{K}\\), a chi-square distribution with \\(K\\)-degrees of freedom (d.f.).\nThis distributions claim to fame is that if \\(y_i \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\\) independently, then\n\\[\n\\frac{1}{\\sigma^2}\\sum_{i = 1}^{n} \\left(y_i- \\bar{y}\\right)^2 \\sim \\chi^2_{n -1}\n\\] which is a nontrivial but very useful fact, since the expression on the right is similar to the usual estimator for the sample standard deviation. We’ll make use of connection when we construct some common hypothesis tests.\n\\(t\\) distribution. A \\(t\\) distribution with \\(k\\) d.f. arises as a ratio between a normal and the square root of a chi-square with K d.f.,\n\\[\n\\frac{\\mathcal{N}\\left(0, 1\\right)}{\\sqrt{\\frac{\\chi^2_{K}}{K}}}\n\\]\nThis seems like an esoteric fact, but notice that the usual way of standardizing the mean (when the true variance is unknown) has this form,\n\\[\n\\frac{\\sqrt{n}\\left(\\bar{y} - \\mu\\right)}{S}\n\\]\n\\(F\\) Distribution. The \\(F\\) distribution occurs as the ratio of independent chi-squares (suitably rescaled),\n\\[\nF_{u, v} = \\frac{\\frac{1}{u}\\chi^2_u}{\\frac{1}{v}\\chi^2_v}\n\\]\nSince chi-squares arise whenever we have sums of squares, this distribution will come in handy whenever we need to compare two different sums of squares.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/dv5tvok0m9vkqqmkd3c0woam5is7gzse.png\")\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:00:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-02-week1-1/",
    "title": "Principles and Vocabulary",
    "description": "An introduction to randomization, replication, and blocking.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-09",
    "categories": [],
    "contents": "\nReadings 1.1, 1.3, Rmarkdown\nWhat is an experiment?\n\nA test or series of runs in which purposeful changes are made to the input variables of a process or system so that we may observe and identify the reasons for changes that may be observed – Montgomery, pg. 1\n\nMore simply, in an experiment, our goal is to learn how inputs affect outputs. It’s not enough to passively watch – we need to see how turning certain “knobs” affects the system.\nTo illustrate, we can consider a planting example. There are a variety of factors that could influence how the plants grow (soil type, watering schedule, etc.). We could allocate different plots of land to trying different configurations of factors. At the end, we hope we can arrive at generalizable knowledge about which configurations we should use during future growing seasons.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/hzv2nghaxm87u7s0awfqttmsh2k3963h.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/c9dof66gs3b2fwp3rhx353e6tpojhqdn.png\")\n\n\n\n\nRandomization\nOne of three key principles of experimental design is randomization. The book says that randomization has been applied if the,\n\nAllocation of experimental material and the order in which the individual runs are performed are randomly determined – Montgomery, pg. 11\n\nMore simply – we should assign treatments using a coin toss (or random number generator). Why is this important? There are many factors besides treatment that can influence outcome. We don’t want these superfluous factors to bias our conclusions.\n[Treating the sickest patients] What could go wrong in the absence of randomization? Suppose we are at a hospital and are trying to see whether a new treatment is effective. If we don’t randomize, we might end up only treating sicker patients than usual. If the sickest patients have worse outcomes on average, then we might underestimate the effectiveness of our treatment.\nIf we randomize, the differences coming from these extraneous factors (like amount of sickness) will cancel out. However, if the treatment does have an effect, we will be able to detect it.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/cmm759fivv4g0xsg85k25b1cq32zn7gp.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/y531b9lly0twl9i987978dqp8kwzfze5.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/t4btwq8hyhkt14kcn0p2o8rwzg2magqa.png\")\n\n\n\n\nReplication\nReplication is the second of the three main principles of experimental design. A replicate is,\n\nAn independent run of each factor combination – Montgomery, pg. 12\n\nReplication is important because it helps us understand run-to-run variation. If we had only grown the plant once, we’d have no idea about the range of variation we’d expect even when fixing the influential factors. Moreover, if we can get many replicates, our estimates of hte mean will improve (orange -> red)\n\n\ninclude_graphics(\"https://uwmadison.box.com/s/vwu2gz8cm01l1fa2u3vc5fha8rsvxycx\")\n\n\n\n\nThe book highlights a distinction between replicates and repeated measures. Repeated measures are several measurements on the same experimental unit. Replicates are distinct experimental units drawn under the same overarching conditions.\n[Computer Chips]. If we were trying to build better computer chips, then repeated measures would be several measurements on the same chip. Replicates would be completely independent chips.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/rqu00u66f4szo40wfevwmesslkniz2d6.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/inwjp3ntywaoim7vs88wehl6m73wu3pb.png\")\n\n\n\n\nBlocking\nThe final major principle of experimental design is blocking, defined as,\n\nA design technique used to improve the precision with which comparisons among the factors of interest are made. Often used to reduce or eliminate variability from nuisance factors.\n\n[Shoe soles] Suppose that we want to test the difference between these purple and green shoe sole types (which wears down faster?).\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/iimo8kf3idq1s9rjb8i853zbrlg989c2.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/8rmn6uh1enadfmfriq646bjvr3q5hx0w.png\")\n\n\n\n\nTwo designs seem natural,\nDesign 1 [No blocking]: Each person is randomly assigned a shoe sole type.\nDesign 2 Blocking: Each person gets one of each shoe sole, and randomly wears on on the left / right foot.\nUnder design 1, any true shoe sole effect would be drowned out by the amount of walking each person did In the blocked design, consistent effects within individuals become detectable. In this example, blocking helped remove nuiscance variation resulting from some people walking more than others.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/rnuet4s27hhidnxde2ys5n3taw9ygjhy.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/8rmn6uh1enadfmfriq646bjvr3q5hx0w.png\")\n\n\n\n\n\n\n\n",
    "preview": "https://uwmadison.box.com/shared/static/hzv2nghaxm87u7s0awfqttmsh2k3963h.png",
    "last_modified": "2021-08-13T16:00:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week1-2/",
    "title": "Motivating Examples",
    "description": "Why are experiments run in the first place?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-09",
    "categories": [],
    "contents": "\nReadings 1.1, 1.2, 1.4, Rmarkdown\n[Golf] We can imagine someone’s golf score as being a function of many factors,\ngolf score = f(driver type, type of ball, ...)\nIn theory, we could manipulate these factors to see how they influenced golf score. If we considered only two factors at a time, each with two possible levels, this would be called a \\(2^2\\) design.\nWe can visualize the 4 possible configurations as corners of a square The golf score is the height of the plane.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/nl6161tped9imsph7qc8c830mmh4ru02.png\")\n\n\n\n\nMathematically,\n\\[\ny = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\epsilon\n\\]\nInteractions\nIt’s possible that the effect of one factor depends on the value of the other – this called an interaction between the two factors. If this happens, then the slopes along the edges are no longer parallel. The previous formula cannot capture this. Instead, we need,\n\\[\ny = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{12}x_{1}x_{2} + \\epsilon\n\\]\nbecause now the slopes can change depending on the value of the other factor.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/hwusoeowryssz905tm5qsf6s9vbkxvd9.png\")\n\n\n\n\nFor example, rearranging terms, we can see that the slope for \\(x_1\\) depends on the value of \\(x_2\\),\n\\[\ny = \\beta_{0} + \\left(\\beta_{1} + \\beta_{12}x_2\\right)x_{1} + \\beta_{2}x_{2} + \\epsilon.\n\\]\nCan you write an expression showing how the slope for \\(x_{2}\\) depends on \\(x_{1}\\)?\nFor each configuration of factors, it is better to play several rounds of golf. The more rounds we play, the better our estimates of the effects for each factor. This is a special case of what we discussed in the last notes; the more replicates, the better our estimates.\nMore than 2 factors\nSuppose we want to see how K different binary factors influence golf score. We can no longer visualize the effects as corners of a square, but we can still collect samples for each configuration of factors. This is called a \\(2^K\\) experiment.\nA challenge is that for large K, this means collecting lots of samples\nK = 3 means 8 configurations\nK = 4 means 16\netc.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/hwusoeowryssz905tm5qsf6s9vbkxvd9.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/2w0x5hzg03zk8113e3rcto58tocc4y6w.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/yzxaz23mgvezuny1ybgydrvpxazn3pm2.png\")\n\n\n\n\nExperimental design is often used in characterizing a process; i.e., how do each of the knobs affect the outcome? Alternatively, we may ask a simpler question – are there knobs that have no effect on the outcome? This is called factor screening. An example is the soldering experiment.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/da48ixumvvvh51y2mi0b6vkaeczqnsmb.png\")\n\n\n\n\nSometimes we care more about optimization. In this case, we don’t care so much about how each factor influences an outcome; we just want a combination of factors that maximizes it. We can visualize the outcome of hte process as a function of several continuous variables.\nIntuitively, our experimentation should proceed by first making a preliminary test and then proceeding in the direction of the max. This intuition is formalized in response surface methodology.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/stgfkm41btsdbnzdxkxs7osnd37hahdj.png\")\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T16:00:47-05:00",
    "input_file": {}
  }
]
