[
  {
    "path": "posts/2021-08-17-week14-2/",
    "title": "Variations on Nested Designs",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-12-16",
    "categories": [],
    "contents": "\nReadings 14.2, 14.3, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"readr\")\nlibrary(\"EBImage\")\nlibrary(\"reshape2\")\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\nNested designs are versatile; they can be applied to many levels of nesting and in conjunction with factors.\nGeneral Nested Designs\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/hacac7wv04k9ua657et7hzd8wg1827ln.png\"))\n\n\n\n\nFigure 1: A three level nested design.\n\n\n\nAlmost the same methodology used in 2-level nested designs carries over to more general levels. Graphically, the tree of effects grows deeper. Since there are more parameters at deeper levels of nesting, it is often a good idea to fit those parameters using random effects.\nSimultaneous Nesting and Factors\nNested terms can be included in standard factorial models. Consider the following setting (Example 14.2),\nAn industrial engineering is optimizing the assembly time of an electronic device.\nThere are different possible layouts and fixtures of the circuit boards which may influence the time to assembly.\nThe experiment must be run across factories. The operators who do the actual assembly will differ depending on layout.\n\n\n####(readImage(\"https://uwmadison.box.com/shared/static/ijhv501h8c07j2xvonxcsg4rkhjoh840.png\"))\n\n\n\nThe operator effect is nested within the layout effect. It should also be treated as a random effect, because we want to understand variation across all possible operators, when choosing a particular layout and fixture design. Therefore, a reasonable model is\n\\[\\begin{align*}\ny_{ijkl} &= \\mu + \\alpha_{i} + \\beta_{j} + \\tau_{k\\left(j\\right)} + \\left(\\alpha\\beta\\right)_{ij} + \\left(\\alpha \\tau\\right)_{ik\\left(j\\right)}\n\\end{align*}\\]\n\\(\\alpha_i\\): The fixture effect\n\\(\\beta_j\\): The layout effect\n\\(\\tau_{k\\left(j\\right)}\\): The operator random effect.\n\\(\\left(\\alpha\\beta\\right)_{ij}\\): An interaction effect between fixtures and layouts\n\\(\\left(\\alpha\\tau\\right)_{ik\\left(j\\right)}\\): A random interaction effect between the fixture and the operator.\nSince this model has both random and fixed effect terms, it is called a mixed effects model. It is fit below.\n\n\nassembly <- read_csv(\"https://uwmadison.box.com/shared/static/gvev45mtp69fb19ng37nlntyiy4x6dmj.csv\") %>%\n  mutate_at(vars(Operator, Layout, Fixture), as.factor)\n\n\n\n\n\nggplot(assembly) +\n  geom_point(aes(x = Operator, y = Time)) +\n  facet_grid(Fixture ~ Layout, scale = \"free_x\")\n\n\n\n\nFigure 2: Assembly time as a function of layout (columns) and fixture (rows). Note operators are nested within layouts.\n\n\n\n\n\nfit <- aov(Time ~ Fixture * Layout +  Error(Layout / Operator), data = assembly)\nsummary(fit)\n\n\n\nError: Layout\n       Df Sum Sq Mean Sq\nLayout  1  4.083   4.083\n\nError: Layout:Operator\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals  6  71.92   11.99               \n\nError: Within\n               Df Sum Sq Mean Sq F value   Pr(>F)    \nFixture         2  82.79   41.40  12.232 8.84e-05 ***\nFixture:Layout  2  19.04    9.52   2.813   0.0732 .  \nResiduals      36 121.83    3.38                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe ANOVA table suggests that layout doesn’t have much of an effect, but that certain fixtures are easier to assemble than others. The fact that there is an interaction between fixtures and operators suggests that the operators who are much worse at some fixtures than others could be retrained.\n\n\n\n",
    "preview": "posts/2021-08-17-week14-2/week14-2_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-09-09T11:30:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week14-3/",
    "title": "Split-Plot Designs",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-12-16",
    "categories": [],
    "contents": "\nReadings 14.4, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"readr\")\nlibrary(\"EBImage\")\nlibrary(\"reshape2\")\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/qtxsckab5vcxffrhbpepj9peq28m7zwn.png\"))\n\n\n\n\nFigure 1: A single field from a true factorial design in the irrigation example.\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/9s2cmkmgc45po62yzm8eltry8cy7rv42.png\"))\n\n\n\n\nFigure 2: An alternative split plot structure. The irrigation strategies are now rows, and each cell is a subplot.\n\n\n\nThere are times when we would like to perform a two-factor factorial experiment across blocks, but one of the factors is much more difficult to vary than the other. For example,\nWe want to assess the effect of different irrigation (\\(A\\)) and corn varieties (\\(B\\)) on total yield. There are three fields (Blocks) within which to gather data. It is hard to change irrigation strategy on small subplots of land, especially compared to corn variety.\nWe run a papermill, and want to compare pulp preparation strategies (\\(A\\)) and cooking temperatures (\\(B\\)). Samples are collected over three days (Blocks). It is hard to change the pulp preparation strategy from sample to sample — we would rather prepare a few big batches — but it’s easy to cook them at different temperatures.\nThis makes a true \\(2^{2}\\) factorial experiment impractical, because it would require randomizing over all combinations of \\(A\\) and \\(B\\) for every sample that we collect.\nIf we had 3 irrigation strategies and 6 corn varieties, we would need to divide each field into 18 subplots and randomize the assignment of irrigation x corn pairs.\nIt’s much easier to first divide each field into 3 large plots, and then randomly assign corn varieties to 6 subplots within each large plot.\nEffectively, practical considerations impose a restriction on randomization.\nModel\nThe model for a split-plot design is\n\\[\\begin{align*}\ny_{ijk} &= \\mu + \\tau_{k} + \\alpha_{i} + \\beta_{j} + \\left(\\tau\\alpha\\right)_{ki} + \\left(\\alpha\\beta\\right)_{ij} + \\epsilon_{ijk}\n\\end{align*}\\]\nwhere \\(\\epsilon \\sim N\\left(0, \\sigma^2\\right)\\) independently.\nThe terms can be interpreted as,\n\\(\\mu\\): The global response average.\n\\(\\tau_{k}\\): The effect of the \\(k^{th}\\) block (e.g., \\(k^{th}\\) field).\n\\(\\alpha_{i}\\): The effect of the \\(i^{th}\\) level of \\(A\\), the hard-to-granularly-randomize factor (e.g., \\(i^{th}\\) irrigation strategy).\n\\(\\beta_{j}\\): The effect of the \\(j^{th}\\) level of \\(B\\), the easy-to-granularly-randomize factor (e.g., \\(j^{th}\\) corn variety)\n\\(\\left(\\tau\\alpha\\right)_{ki}\\): An interaction factor between the \\(k^{th}\\) block and the \\(i^{th}\\) level of \\(A\\) (e.g., the \\(i^{th}\\) irrigation strategy within the \\(k^{th}\\) field might be unusually good).\n\\(\\left(\\alpha\\beta\\right)_{ij}\\): An interaction factor between the \\(i^{th}\\) level of \\(A\\) and the \\(j^{th}\\) level of \\(B\\).\nWe will typically not care about individual block effects, though we will care about the two different treatments. Therefore, it is common to\nUse random effects for \\(\\tau_{k}, \\left(\\tau\\alpha\\right)_{ki}\\).\nUse fixed-effects for \\(\\alpha_{i}\\) and \\(\\beta_{j}\\).\nThe expected mean squares associated with each of the terms above can be derived in closed form. Here, we will simply illustrate their use through the lme4 package. The data are from the papermill experiment described above.\n\n\npulp <- read_csv(\"https://uwmadison.box.com/shared/static/843r3mda46is46nbb5b6393u7caz8orq.csv\") %>%\n  mutate_at(vars(Day, Method, Temperature), as.factor)\n\n\n\n\n\nggplot(pulp) +\n  geom_point(\n    aes(x = Temperature, y = Strength, col = Day),\n    size = 3\n  ) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_grid(Method ~ .)\n\n\n\n\nFigure 3: Data from the papermill experiment.\n\n\n\n\n\nfit <- aov(Strength ~ Method * Temperature + Error(Day / Method), data = pulp)\nsummary(fit)\n\n\n\nError: Day\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals  2  77.56   38.78               \n\nError: Day:Method\n          Df Sum Sq Mean Sq F value Pr(>F)  \nMethod     2 128.39   64.19   7.078 0.0485 *\nResiduals  4  36.28    9.07                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n                   Df Sum Sq Mean Sq F value   Pr(>F)    \nTemperature         3  434.1  144.69  36.427 7.45e-08 ***\nMethod:Temperature  6   75.2   12.53   3.154   0.0271 *  \nResiduals          18   71.5    3.97                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSplit-Plot is not \\(2^{2}\\) factorial\nFitting a \\(2^{2}\\) factorial model when the data were collected with restrictions on randomization can lead to misleading results. The code below fits an ordinary \\(2^{2}\\) factorial model to the papermill data. Note the overconfidence about an effect of Method.\n\n\nfit <- aov(Strength ~ Method * Temperature, data = pulp)\nsummary(fit)\n\n\n                   Df Sum Sq Mean Sq F value   Pr(>F)    \nMethod              2  128.4   64.19   8.313  0.00181 ** \nTemperature         3  434.1  144.69  18.737 1.76e-06 ***\nMethod:Temperature  6   75.2   12.53   1.622  0.18426    \nResiduals          24  185.3    7.72                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n",
    "preview": "posts/2021-08-17-week14-3/week14-3_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-09-09T11:30:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week14-1/",
    "title": "Nested Designs",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-12-14",
    "categories": [],
    "contents": "\nReadings 14.1, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lme4)\nlibrary(readr)\nlibrary(EBImage)\nlibrary(reshape2)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/kbs80435ds41u0qp7typvmuiaibyjlpm.png\"))\n\n\n\n\nFigure 1: The nested structure in the suppliers example\n\n\n\nFactorial designs cover the situation where effects from multiple sources are crossed with one another. In many situations, however, effects are nested within each other.\nYou rely on several suppliers to provide raw material and are interested in whether some suppliers provide consistently better material. The material arrives in batches, and there is variation across batches. The batch effect is nested within the supplier effect.\nThe hardness of different alloys are to be compared. Several ingots are built from each alloy, and repeated measures are drawn from each ingot. The ingot effect is nested within the alloy effect.\nModel\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/g6yb4uu8nncnqc39rbjhh3kz8bv9jzwc.png\"))\n\n\n\n\nFigure 2: Notation used in nested designs. Parentheses denote the parent branch.\n\n\n\nWe will consider the two-stage design here. The data are imagined to be drawn according to \\[\\begin{align*}\ny_{ijk} &= \\mu + \\alpha_{i} + \\beta_{j\\left(i\\right)} + \\epsilon_{ijk}.\n\\end{align*}\\]\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/fy3jwyeou1bu6nyo1i2lypg8juj80z8k.png\"))\n\n\n\n\nFigure 3: Another way to look at the nested coefficient structure.\n\n\n\n\\(\\alpha_i\\) is the parent-effect (e.g., supplier effect). For identifiability, assume \\(\\sum_{i} \\alpha_i = 0\\).\n\\(\\beta_{j\\left(i\\right)}\\) is the nested-effect associated with parent \\(i\\) (e.g., effect of the \\(j^{th}\\) batch within the \\(i^{th}\\) supplier). For identifiability, assume \\(\\sum_{j}\\beta_{j\\left(i\\right)} = 0\\) for each \\(i\\).\n\\(\\epsilon_{ijk} \\sim N\\left(0, \\sigma^{2}\\right)\\) is independent noise.\nThe key difference from the usual factorial model is that the nested effects \\(\\beta_{j\\left(i\\right)}\\) are not the same across different parents \\(i\\).\nInference\nFixed-Effects\nFirst, imagine treating all the effects as fixed. In this case, there are two typical hypotheses of interest.\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/6bona7dg8dl6fbnfvetovq2cl9l5zo11.png\"))\n\n\n\n\nFigure 4: The nested effect term looks at the variation across child means along a single branch.\n\n\n\nNull parent-effect, \\[\\begin{align*}\n&H_{0}: \\alpha_{i} = 0 \\text{ for all } i \\\\\n&H_{1}: \\alpha_{i} \\neq 0 \\text{ for at least one } i\n\\end{align*}\\]\nNull child-effect, \\[\\begin{align*}\n&H_{0}: \\beta_{j\\left(i\\right)} = 0 \\text{ for all } i,j \\\\\n&H_{1}: \\beta_{j\\left(i\\right)} \\neq 0 \\text{ for at least one } i,j\n\\end{align*}\\]\nIn either case, a test is performed using a sum-of-squares decomposition, \\[\\begin{align*}\nSS_{T} &= SS_A + SS_{B\\left(A\\right)} + SS_{E}\n\\end{align*}\\] which is similar in structure to those we have seen before, except we now have a nested effect term, \\[\\begin{align*}\nSS_{B\\left(A\\right)} &= \\sum_{i = 1}^{a}\\sum_{j = 1}^{b} \\left(\\bar{y}_{ij\\cdot} - \\bar{y}_{i\\cdot\\cdot}\\right)^{2}.\n\\end{align*}\\]\nwhich measures how much the \\(j^{th}\\) effect within parent \\(i\\) varies from the average in that group. The distribution of the associated mean-squares can then be used to perform an ANOVA. Let’s see an implementation, based on the supplier-materials example above. (example 14.1)\n\n\npurity <- read_csv(\"https://uwmadison.box.com/shared/static/uub0t6lvii52rxyygb2yt4ph3vmionjz.csv\") %>%\n  mutate(\n    supplier = as.factor(supplier),\n    batch = as.factor(batch)\n  )\n\n\n\n\n\nggplot(purity) +\n  geom_point( aes(x = batch, y = purity) ) +\n  facet_wrap(~supplier)\n\n\n\n\nFigure 5: Data from the supplier example.\n\n\n\n\n\nfit <- lm(purity ~ supplier + supplier / batch, data = purity)\nanova(fit)\n\n\nAnalysis of Variance Table\n\nResponse: purity\n               Df Sum Sq Mean Sq F value  Pr(>F)  \nsupplier        2 15.056  7.5278  2.8526 0.07736 .\nsupplier:batch  9 69.917  7.7685  2.9439 0.01667 *\nResiduals      24 63.333  2.6389                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe A/B notation is just shorthand for A + A:B; i.e., a main effect for the parent A and an interaction term between the parent A and child B, as the code below makes clear.\n\n\nfit_explicit <- lm(purity ~ supplier + supplier : batch, data = purity)\nanova(fit_explicit)\n\n\nAnalysis of Variance Table\n\nResponse: purity\n               Df Sum Sq Mean Sq F value  Pr(>F)  \nsupplier        2 15.056  7.5278  2.8526 0.07736 .\nsupplier:batch  9 69.917  7.7685  2.9439 0.01667 *\nResiduals      24 63.333  2.6389                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRandom Effects\nRather than caring about specific parent or nested effects, we may simply want to know the typical variation due to either factor. For example, we may not care about the effect of the 2nd batch in the 3rd supplier, but we may be curious about the typical size of batch-to-batch variation. In this case, it makes sense to use random effects. We can use a random effect for just the nested factors,\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/twey7ufa1zeaq8lgjpdg1uu66a7fi2l4.png\"))\n\n\n\n\nFigure 6: We might imagine the batch effects are drawn from some distribution, and do inference on the variance of that distribution.\n\n\n\n\n\nfit <- aov(purity ~ supplier + Error(supplier/batch), data = purity)\nsummary(fit)\n\n\n\nError: supplier\n         Df Sum Sq Mean Sq\nsupplier  2  15.06   7.528\n\nError: supplier:batch\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals  9  69.92   7.769               \n\nError: Within\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals 24  63.33   2.639               \n\nor for both the nested and parent factors,\n\n\nfit <- lmer(purity ~ (1 | supplier / batch), data = purity)\nsummary(fit)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: purity ~ (1 | supplier/batch)\n   Data: purity\n\nREML criterion at convergence: 148.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.38226 -0.75533 -0.07592  0.57348  1.71092 \n\nRandom effects:\n Groups         Name        Variance  Std.Dev.\n batch:supplier (Intercept) 1.696e+00 1.302285\n supplier       (Intercept) 9.197e-07 0.000959\n Residual                   2.639e+00 1.624383\nNumber of obs: 36, groups:  batch:supplier, 12; supplier, 3\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   0.3611     0.4633   0.779\n\nHowever, it makes no sense to treat the parent as random, but its child as fixed.\nAn important detail to keep in mind is that, for nested designs, computation of \\(F\\)-statistics depends on which terms are treated as fixed or random. For example, suppose \\(B\\) is nested within \\(A\\). Then the \\(F\\) statistic for \\(A\\) is computed as\n\\(\\frac{MS_A}{MS_E}\\) if both \\(A\\) and \\(B\\) are fixed\n\\(\\frac{MS_A}{MS_B(A)}\\) if \\(A\\) is fixed but \\(B\\) is random\n\\(\\frac{MS_{B\\left(A\\right)}}{MS_{E}}\\) if both \\(A\\) and \\(B\\) are random\n\n\n\n",
    "preview": "posts/2021-08-17-week14-1/week14-1_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-09-09T11:30:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week13-2/",
    "title": "Optimal Response Surface Designs",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-12-09",
    "categories": [],
    "contents": "\nReadings 11.4, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(EBImage)\nlibrary(reshape2)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/6xoe5onf01gdpaef6g0qtcstbhb9w72n.png\"))\n\n\n\n\nFigure 1: The optimality definitions from our earlier discussion on factorial design.\n\n\n\nIn Chapter 6, we saw that \\(2^{K}\\) factorial designs are optimal in the linear setting. These results don’t immediately apply to response surfaces, though, for two reasons,\nSecond-order response surfaces are not necessarily linear.\nThe experimental region might be irregularly shaped, due to known constraints on operating conditions\nIn this setting, there will be no single design that clearly optimal, like there was before. Instead, the typical strategy is to compute the same optimality criteria from before, but to designs constructed through various heuristics.\nReminder: Optimality Measures\nWe can use the same optimality measures that were studied for linear regression.\n\\(D\\)-optimality reflects the variance in the coefficients of the associated linear model. A \\(D\\)-optimal design has minimal value of \\(\\left|X^{T}X\\right|^{-1}\\).\n\\(G\\)-optimality reflects the pointwise variance of the fitted surface. A \\(G\\)-optimal design minimizes the maximal value of \\(V\\left(\\hat{y}\\left(x\\right)\\right)\\).\n\\(V\\)-optimality also reflects the pointwise variance of the fitted surface, but with less focus on the worst case \\(x\\). A \\(V\\)-optimal design minimizes the average variance, \\(\\int_{R} V\\left(\\hat{y}\\left(x\\right)\\right)dx\\) over the experiment space \\(R\\).\nHeuristics\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/9pgowo8mgly6d8mm97t8sm324ggcio0y.png\"))\n\n\n\n\nFigure 2: One iteration of the point exchange algorithm, for a constrained response surface design.\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/yr2dyd75we3ebp1gwe36r5xe3p8841h8.png\"))\n\n\n\n\nFigure 3: One iteration of the point coordinate exchange algorithm, in the same setup.\n\n\n\nOnce a candidate design is proposed, we can evaluate its quality using the measures above. There are various heuristics for proposing new candidate designs,\nPoint exchange\nStart with a grid of points to consider performing runs at.\nSelect a subset (possibly at random). Call this the design set and the complement the candidate set.\nCompute an optimality criterion on the design set.\nTry swapping a pair of points from the design and candidate sets\nIs the optimality criterion is improved?\nIf it is, keep the swap in the next iteration.\n\nRepeat until the optimality criterion has converged.\n\nCoordinate exchange\nStart with a grid of points to consider performing runs at, call this the design set.\nFor each point in the design set,\nFor each factor \\(k\\),\nVary the value of factor \\(k\\) until it maximizes the chosen optimality criterion.\n\nRepeat until convergence\n\n\n",
    "preview": "posts/2021-08-17-week13-2/week13-2_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-09-09T11:29:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week13-3/",
    "title": "Mixture Experiments",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-12-09",
    "categories": [],
    "contents": "\nReadings 11.6, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(mixexp)\nlibrary(readr)\nlibrary(EBImage)\nlibrary(reshape2)\nlibrary(AlgDesign)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\nWhen we are cooking something up, we may want to find an optimal mixture of ingredients. For example, when making a fiber for a kind of yarn, there are three ingredients (polyethylene, polystyrene, and polypropylene) that are mixed at various fractions. What fraction would optimize the stretchiness of the yarn?\nWe’ll explore this problem as a special case of the general response surface problem.\nGeometry\nThe mixture setting induces specific geometric constraints.\nSuppose there are \\(P\\) total ingredients.\nLet \\(x_{p}\\) denote the fraction of ingredient \\(p\\).\nSince the \\(x_{p}\\) are mixture fractions, we have the constraints,\n\\(x_{p} \\in \\left[0, 1\\right]\\)\n\\(\\sum_{p = 1}^{P} x_{p} = 1\\)\nThe set of \\(x = \\left(x_{1}, \\dots, x_{P}\\right)\\) that satisfy these constraints can be geometrically represented by a simplex.\nCenter point has an equal amount of each ingredient\nCorners have 100% coming from one of the ingredients\nThere is nothing stopping us from fitting a response surface over the simplex.\nDesign Points\nWhat design points should we use?\n\n\nDesignPoints(SLD(3, 3))\n\n\n\n\nFigure 1: An example \\((3, 3)\\) simplex lattice design.\n\n\n\n\n\nDesignPoints(SLD(3, 5))\n\n\n\n\nFigure 2: An example \\((3, 5)\\) simplex lattice design.\n\n\n\n\n\nDesignPoints(SCD(3))\n\n\n\n\nFigure 3: Simplex centroid design across 3 mixture elements.\n\n\n\nSimplex lattice design: Choose some \\(m\\) which will reflect the granularity of our design. Consider combinations of integers \\(k_{p} \\in \\{0,1, \\dots, m\\}\\) such that \\(\\sum_{p= 1}^{P} k_{p} = m\\). Each such combination specifies a point \\[\\begin{align*}\n\\frac{1}{m}\\left(k_{1}, \\dots, k_{P}\\right)\n\\end{align*}\\] that is included in the simplex lattice design.\nAn alternative way to specify this is to draw a lattice grid with spacing \\(\\frac{1}{m}\\) in the unit cube, and then discard points that don’t lie in the simplex.\n\nSimplex centroid design\nCorners: Add all \\(P\\) permutations of the vector \\(\\left(1, 0, \\dots, 0\\right)\\).\nEdge midpoints: Add all \\({P \\choose 2}\\) permutations of \\(\\left(\\frac{1}{2}, \\frac{1}{2}, 0, \\dots, 0\\right)\\). These are midpoints between two corners, and so lie on edges of the simplex.\nFace centroids: Add all \\({P \\choose 3}\\) permutations of \\(\\left(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}, 0, \\dots, 0\\right)\\) which are the centers of faces defined by three corners.\n.. continue the pattern: For all \\(k \\leq P\\), add all \\({P \\choose k}\\) permutations of \\(\\left(\\frac{1}{k}, \\dots, \\frac{1}{k}, 0, \\dots, 0\\right)\\).\n\nThere are some common variations,\nIt’s common to augment the designs above with center points.\nSometimes it is useful to include axial points, which are samples along rays extending from corners of the simplex\nComputer-generated designs can be used, especially when there are constraints on feasible mixture values.\n\n\nmscd <- SCD(5) %>%\n  mutate(id = row_number()) %>%\n  melt(id.vars = \"id\")\nggplot(mscd) +\n  geom_tile(\n    aes(x = variable, y = id, fill = value)\n  ) +\n  scale_fill_viridis_c() +\n  coord_fixed() +\n  theme(\n    legend.position = \"right\",\n    axis.text = element_text(size = 8),\n    axis.title = element_blank()\n  )\n\n\n\n\nFigure 4: Simplex centroid design across 5 mixture elements.\n\n\n\nData Example\nWe’ll use the yarn data from Example 11.5. The experiment used a (3, 2) simplex lattice design to measure variation in yarn elongation as a function of the fractions of different materials used to make the base fiber. First, let’s try to see the dependence visually, though direct visualization on the simplex is challenging.\n\n\nyarn <- read_csv(\"https://uwmadison.box.com/shared/static/jghwbsnn6qjpwdr1lc97p9mbxk8qkwif.csv\")\nggplot(yarn) +\n  geom_point(\n    aes(x = x1, y = x2, size = sqrt(x3), col = elongation),\n    position = position_jitter(w = 0.1, h = 0.1)\n  ) +\n  scale_color_viridis_c() +\n  theme(legend.position = \"none\")\n\n\n\n\nNow, let’s fit a second-order polynomial to the data. Note that we include a -1 term in the fit below, to ensure the model does not fit an intercept term.\n\n\nfit <- lm(elongation ~ -1 + (x1 + x2 + x3) ^ 2, data = yarn)\nsummary(fit)\n\n\n\nCall:\nlm(formula = elongation ~ -1 + (x1 + x2 + x3)^2, data = yarn)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -0.80  -0.50  -0.30   0.65   1.30 \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)    \nx1     11.7000     0.6037  19.381 1.20e-08 ***\nx2      9.4000     0.6037  15.571 8.15e-08 ***\nx3     16.4000     0.6037  27.166 6.01e-10 ***\nx1:x2  19.0000     2.6082   7.285 4.64e-05 ***\nx1:x3  11.4000     2.6082   4.371  0.00180 ** \nx2:x3  -9.6000     2.6082  -3.681  0.00507 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8537 on 9 degrees of freedom\nMultiple R-squared:  0.9977,    Adjusted R-squared:  0.9962 \nF-statistic: 658.1 on 6 and 9 DF,  p-value: 2.271e-11\n\nWe can plot the associated fit. Compare with Figure 11.43.\n\n\nModelPlot(\n  fit, \n  dimensions = list(x1 = \"x1\", x2 = \"x2\", x3 = \"x3\"), \n  contour = TRUE\n)\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-17-week13-3/week13-3_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-09-09T11:29:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week13-1/",
    "title": "Blocking in Response Surface Designs",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-12-07",
    "categories": [],
    "contents": "\nReadings 11.4, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(EBImage)\nlibrary(reshape2)\nlibrary(rsm)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/hxsz37wvxcj93osymlckvep6da6ipoqy.png\"))\n\n\n\n\nFigure 1: By tying sampling center points in both the axial and factorial blocks, it becomes possible to estimate and correct for block effects.\n\n\n\nIt’s often impossible to collect all the samples for an experiment at once, which can lead to batch effects downstream. This motivates the use of blocked designs — how should we think of blocked designs in the context of response surface experiments?\nFirst, let’s build intuition through an example. Suppose we are using a central composite design and \\(K = 2\\), but we can’t complete all 10 runs at once. At most, during a single pass, we can collect 5 samples.\nIdea: Split the factorial and axial samples into two blocks, but tie them together at the center points.\nAny batch effects can be directly estimated through shifts in the average value at the center\nThe main technical condition that generalizes this idea is called block orthogonality. A design is called block orthogonal if,\nEach block \\(X_{b}\\) is orthogonal, i.e., \\(X_{b}^{T}X_{b}\\) = I when we stack all design points for the block into an \\(n_{b} \\times K\\) matrix \\(X_{b}\\).\nFor each factor \\(k\\) and each block \\(b\\), \\[\\begin{align*}\n\\frac{\\sum_{u = 1}^{n_{b}} x_{iu}^{2}}{\\sum_{u = 1}^{N} x_{iu}^{2}} = \\frac{n_{b}}{N}\n\\end{align*}\\] this ensures that no one block has undue influence over the eventual estimates.\nFor arbitrary number of factors \\(K\\), the central composite design turns can be made block orthogonal by dividing into two blocks (factor and axial points, tied at the center). It can be broken into even more blocks for particular choices of block size; this is summarized in Table 11.11.\nCode Example\nWe can use the same code we used to generate central composite designs before, but using the blocks argument.\n\n\ncodings <- list(\n  time_coded ~ (time - 35) / 5,\n  temp_coded ~ (temp - 150) / 5\n)\nccd_design <- ccd(~ time_coded + temp_coded, coding = codings)\nccd_design\n\n\n   run.order std.order     time     temp Block\n1          1         2 40.00000 145.0000     1\n2          2         7 35.00000 150.0000     1\n3          3         3 30.00000 155.0000     1\n4          4         5 35.00000 150.0000     1\n5          5         1 30.00000 145.0000     1\n6          6         8 35.00000 150.0000     1\n7          7         6 35.00000 150.0000     1\n8          8         4 40.00000 155.0000     1\n9          1         7 35.00000 150.0000     2\n10         2         4 35.00000 157.0711     2\n11         3         8 35.00000 150.0000     2\n12         4         3 35.00000 142.9289     2\n13         5         1 27.92893 150.0000     2\n14         6         2 42.07107 150.0000     2\n15         7         6 35.00000 150.0000     2\n16         8         5 35.00000 150.0000     2\n\nData are stored in coded form using these coding formulas ...\ntime_coded ~ (time - 35)/5\ntemp_coded ~ (temp - 150)/5\n\n\n\nggplot(decode.data(ccd_design)) +\n  geom_point(\n    aes(x = time, y = temp, col = Block),\n    position = position_jitter(w = 0.5, h = 0.5),\n    size = 3\n  ) +\n  scale_color_brewer(palette = \"Set2\") + \n  coord_fixed()\n\n\n\n\nSuppose we wanted to block more than just the factorial and the axial runs; for example, we may only be able to run 2 examples per batch. In this case, we might block the factorial runs by confounding a high-order interaction with the blocks. For example, the code below blocks the 32 runs of a \\(2^{5}\\) factorial into blocks of size 8.\n\n\ncodings <- list(\n  time_ ~ (time - 35) / 5,\n  temp_ ~ (temp - 150) / 5,\n  power_ ~ power,\n  rate_ ~ rate,\n  cooling_ ~ cooling\n)\nblocked_ccd <- ccd(~ time_ + temp_ + power_ + rate_ + cooling_, coding = codings, blocks = Block ~ c(time_ * temp_ * power_, power_ * rate_ * cooling_))\n\n\n\nFinally, let’s check the orthogonal blocking property for one of the blocks. Remember, we need to make sure that (1) the design within the block is diagonal and (2) the fraction of norm for a column is proportional to the number of samples within the block. This is checked below for Block 1.\n\n\nggplot(decode.data(blocked_ccd)) +\n  geom_point(\n    aes(x = time, y = temp, col = Block),\n    position = position_jitter(w = 1, h = 1),\n    size = 3, alpha = 0.6\n  ) +\n  scale_color_brewer(palette = \"Set2\") + \n  coord_fixed()\n\n\n\n\n\n\nx <- as.data.frame(blocked_ccd)\nxb <- x[x$Block == 1, ] %>%\n  select(ends_with(\"_\")) %>%\n  as.matrix()\nt(xb) %*% xb # orthogonality\n\n\n         time_ temp_ power_ rate_ cooling_\ntime_        8     0      0     0        0\ntemp_        0     8      0     0        0\npower_       0     0      8     0        0\nrate_        0     0      0     8        0\ncooling_     0     0      0     0        8\n\nsum(xb[, 1]^2) / sum(x$time ^ 2) # ||xb[1]||^2 / ||x[1]||^2\n\n\n[1] 0.1935484\n\n12 / nrow(blocked_ccd) # nb / N\n\n\n[1] 0.1935484\n\n\n\n\n",
    "preview": "posts/2021-08-17-week13-1/week13-1_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-09-09T11:29:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week12-3/",
    "title": "Optimizing Multiple Responses",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-12-02",
    "categories": [],
    "contents": "\nReadings 11.3, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(EBImage)\nlibrary(desirability)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(reshape2)\nlibrary(rsm)\nlibrary(stringr)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\nchem <- read_csv(\"https://uwmadison.box.com/shared/static/nbaj1m8j7tuaqmznjlrsgbzyhp9k61i8.csv\")\nstd <- function(x) {\n  (x - mean(x)) / sd(x)\n}\nmchem <- chem %>%\n  mutate_at(vars(yield, viscosity, molecular_weight), std) %>%\n  melt(measure.vars = c(\"yield\", \"viscosity\", \"molecular_weight\"))\n\n\n\n\n\nchem_coded <- coded.data(chem, time_coded ~ (time - 35) / 5, temp_coded ~ (temp - 155) / 5)\nfits <- list(\n  \"yield\" = rsm(yield ~ SO(temp_coded, time_coded), data = chem_coded),\n  \"viscosity\" = rsm(viscosity ~ SO(temp_coded, time_coded), data = chem_coded),\n  \"molecular_weight\" = rsm(molecular_weight ~ SO(temp_coded, time_coded), data = chem_coded)\n)\n\n\n\n\n\ncontour(fits[[1]], ~ time_coded + temp_coded, image = TRUE, asp = 1)\n\n\n\n\n\n\ncontour(fits[[2]], ~ time_coded + temp_coded, image = TRUE, asp = 1)\n\n\n\n\n\n\ncontour(fits[[3]], ~ time_coded + temp_coded, image = TRUE, asp = 1)\n\n\n\n\nFigure 1: Three separate response surfaces, fit to yield, viscosity, and molecular weight, respectively.\n\n\n\nIn more complicated systems, we may want to optimize several objectives simultaneously. More often than not, the goals will be at odds with one another. For example, in our running chemical process example, we want to maximize yield while maintaining a target viscosity and minimizing molecular weight. How can we use response surface methods when we have several competing objectives?\n\n\nggplot(mchem) +\n  geom_point(\n    aes(x = time, y = temp, col = value),\n    position = position_jitter(w = 0.3, h = 0.3)\n  ) +\n  facet_grid(. ~ variable) +\n  coord_fixed() +\n  scale_color_viridis_c() +\n  theme(legend.position = \"none\")\n\n\n\n\nOverlaying Contours\nThe most direct approach is to simply fit several response surfaces.\nVisually inspect results to find factor configurations with desirable values across each response.\nConstrained Optimization\nWhenever visual inspection is challenging, mathematical formalizations can offer support. One idea is to frame the multiple response surface problem as a constrained optimization.\nDefine acceptable ranges for responses \\(y_{2}\\left(x\\right), \\dots, y_{R}\\left(x\\right)\\) that are important, but not our main focus.\nOptimize the response \\(y_{1}\\left(x\\right)\\) that’s our main focus.\nFormally, we look for a configuration of factors \\(x_{\\ast}\\) that solves the optimization \\[\\begin{align*}\n\\underset{x}{\\text{maximize}}\\medspace &y_{1}\\left(x\\right) \\\\\n\\text{subject to }\\medspace &\\left(y_{2}\\left(x\\right), \\dots, y_{R}\\left(x\\right)\\right) \\in \\mathcal{C}\n\\end{align*}\\]\nwhere \\(C\\) is the predefined acceptable region for the secondary responses.\nDesirability Functions\nThe main downside of the constrained optimization approach is that it forces us to choose one response to prioritize over all others. What if we care about each response more or less equally?\nOne idea is to optimize a sort of (geometric) averaged response, \\[\\begin{align*}\n\\underset{x}{\\text{maximize}}\\medspace \\left[\\prod_{r = 1}^{R} y_{r}\\left(x\\right)\\right]^{\\frac{1}{R}}\n\\end{align*}\\]\n\n\nx_grid <- seq(-1, 1, .01)\nexample_funs <- data.frame(\n  x = x_grid,\n  d_max_0.5 = predict(dMax(0, 1, 0.5),  x_grid),\n  d_max_1 = predict(dMax(0, 1, 1),  x_grid),\n  d_max_2 = predict(dMax(0, 1, 2), x_grid),\n  d_min_0.5 = predict(dMin(-1, 0, 0.5), x_grid),\n  d_min_1 = predict(dMin(-1, 0, 1), x_grid),\n  d_min_2 = predict(dMin(-1, 0, 2), x_grid),\n  d_target_0.5 = predict(dTarget(-1, 0, 1, 0.5, 0.5), x_grid),\n  d_target_1 = predict(dTarget(-1, 0, 1), x_grid),\n  d_target_2 = predict(dTarget(-1, 0, 1, 2, 2), x_grid)\n) %>%\n  melt(id.vars = c(\"x\")) %>%\n  mutate(\n    type = str_replace(variable, \"[\\\\.0-9]+\", \"\"),\n    scale = str_extract(variable, \"[\\\\.0-9]+\")\n  )\nggplot(example_funs) +\n  geom_line(aes(x = x, y = value, col = scale)) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_grid(type ~ .)\n\n\n\n\nFigure 2: Example desirability functions, for maximizing, minimizing, and achieving a target response.\n\n\n\nThe issue with this idea is that it treats all responses exactly equally. What if we want to maximize some, but minimize others? What if we want some to be near some target value?\nThe solution is to use desirability functions. A few are plotted below. You can adjust their shape so that the \\(r^{th}\\) desirability function is large for the values of \\(y_{r}\\left(x\\right)\\) which are good (sloping down when you want to minimize, sloping up when you want to maximize).\nThen, instead of optimizing the raw averaged response, we optimize the averaged response after first passing through the desirability functions, \\[\\begin{align*}\n\\underset{x}{\\text{maximize}} \\medspace \\left[\\prod_{r = 1}^{R} d_{r}\\left(y_{r}\\left(x\\right)\\right)\\right]^{\\frac{1}{R}}\n\\end{align*}\\]\nData Example\nFor the chemical problem, we can define a desirability function per response, and then combine them into an overall objective. This is done using the desirability package.\n\n\nd_yield <- dMax(70, 95) # min / max / scale\nd_viscosity <- dTarget(55, 65, 75) # min / target / max\nd_weight <- dMin(2750, 4000) # min / max\nobjective <- dOverall(d_yield, d_viscosity, d_weight)\n\n\n\nNow, we can apply this objective function to predictions made by the three response surface fits we made above, in the section on overlaying contours. We’re evaluating the predictions over a grid of values in the range of the coded time and temperature.\n\n\ncoded <- as.data.frame(chem_coded)\nx_grid <- expand.grid(\n  time_coded = seq(min(coded$time_coded), max(coded$time_coded), .1),\n  temp_coded = seq(min(coded$temp_coded), max(coded$temp_coded), 0.1)\n)\n\n\n\n\n\ndesirabilities <- cbind(\n  x_grid,\n  y1 = predict(fits[[1]], x_grid),\n  y2 = predict(fits[[2]], x_grid),\n  y3 = predict(fits[[3]], x_grid)\n)\ny_hat <- desirabilities %>% \n  select(starts_with(\"y\"))\ndesirabilities$objective <- predict(objective, y_hat)\n\n\n\nAt this point, we can plot the desirability of each point in the time / temperature configuration space. Note that a second mode emerges along the low-temperature region – these are regions which have good viscosity and molecular weight properties, even though their yield isn’t as high. Try increasing the scale for yield to see what happens when you make the requirement for high yield more stringent.\n\n\nggplot(desirabilities) +\n  geom_tile(\n    aes(x = time_coded, y= temp_coded, fill = objective)\n  ) +\n  coord_fixed() +\n  scale_fill_viridis_c()\n\n\n\n\nFigure 3: Overall desirability, considering yield, viscosity, and molecular weight.\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-17-week12-3/week12-3_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-09-09T11:29:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week12-4/",
    "title": "Designs for Response Surfaces",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-12-02",
    "categories": [],
    "contents": "\nReadings 11.4.1-11.4.2, Rmarkdown\n\n\nlibrary(EBImage)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(reshape2)\nlibrary(rsm)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/uubzanl5c273c45mxazy0d06r7r0uz52.png\"))\n\n\n\n\nFigure 1: Initial runs are using a factorial design, while later runs use a CCD.\n\n\n\nWe have been fitting surfaces to find some configuration of factors that optimizes a response. Sometimes we found the optimum just from one experiment, but more often, we ran a series of experiments, gradually refining our understanding of the response surface.\nFor any of these experiments, what designs should we use?\nIn theory, any of the designs we have discussed in this course could be used, but we will prefer those which give us an accurate estimate of the response surface (especially around stationary points) using as few samples as possible.\nDesigns for First-Order Models\nIn first-order models, we are interested in linear effects associated with each factor. Reasonable choices in this setting are,\n\\(2^{K}\\) factorial designs\n\\(2^{K - p}\\) fractional factorial designs\n\\(2^{K}\\) and \\(2^{K - p}\\) designs that have been augmented with center points. These center points allow estimation of measurement error \\(\\sigma^2\\) even when there is no replication.\nDesigns for Second-Order Models\nFor second-order models, we need to estimation nonlinear and interaction effects, which means we need more intensive sampling. The most common choice is the,\nCentral Composite Design (CCD): As discussed in Section 6.8, this is a full factorial \\(2^{K}\\) design that has been supplemented by center and axial points.\nthough some alternatives are,\nBox-Behnken design: The associated samples are all on edges of the cube (none at vertices or in the interior).\nEquiradial design: This generalizes the simplex idea above. Instead of sampling at the corners of an equilateral triangle, we can sample at corners or regular polygons (+ center points)\nIn practice, you can use functions from the rsm package. This code produces a central composite design with three center points for both the factorial and axial runs. The package also has code for factorial, fractional factorial, and Box-Behnken designs.\n\n\ncodings <- list(\n  time_coded ~ (time - 35) / 5,\n  temp_coded ~ (temp - 150) / 5\n)\nccd_design <- ccd(~ time_coded + temp_coded, coding = codings, oneblock = TRUE)\nhead(ccd_design)\n\n\n  run.order std.order     time     temp\n1         1         4 40.00000 155.0000\n2         2         3 35.00000 142.9289\n3         3         1 30.00000 145.0000\n4         4         8 35.00000 150.0000\n5         5         5 35.00000 150.0000\n6         6         1 27.92893 150.0000\n\nData are stored in coded form using these coding formulas ...\ntime_coded ~ (time - 35)/5\ntemp_coded ~ (temp - 150)/5\n\n\n\nggplot(decode.data(ccd_design)) +\n  geom_point(\n    aes(x = time, y = temp),\n    position = position_jitter(w = 0.5, h = 0.5),\n    size = 3\n  ) +\n  coord_fixed()\n\n\n\n\nFigure 2: A generated CCD using the rsm package.\n\n\n\nUsing the same function, we can easily generate fractional or foldover designs, which are useful during the first-order phase of response surface modeling. For example, the code below generates a \\(2^{4 - 1}\\) design.\n\n\ncodings_x = list(\n  x1 ~ (x1_ - 10) / 3,\n  x2 ~ (x2_ - 20) / 6,\n  x3 ~ (x3_ + 1) / 4\n)\nffactorial <- cube(\n  ~ x1 + x2 + x3, \n  n0 = 0, \n  coding = codings_x,\n  generators = x4 ~ x1 * x2 * x3\n  )\n\n\n\nAnd if we want to foldover this fractional factorial, we can use foldover.\n\n\nfoldover(ffactorial, randomize = FALSE)\n\n\n\n\n\nmfactorial <- ffactorial %>%\n  head() %>%\n  melt(id.vars = \"std.order\") %>%\n  filter(variable != \"run.order\")\nggplot(mfactorial) +\n  geom_tile(\n    aes(x = variable, y = as.factor(std.order), fill = as.factor(value))\n  ) +\n  coord_fixed() +\n  labs(x = \"factor\", y = \"run\", fill = \"level\") +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nFigure 3: A \\(2^{4 - 1}\\) fractional factorial design, made using the rsm package.\n\n\n\n\n\nmfactorial <- foldover(ffactorial, randomize=FALSE) %>%\n  head() %>%\n  melt(id.vars = \"std.order\") %>%\n  filter(variable != \"run.order\")\nggplot(mfactorial) +\n  geom_tile(\n    aes(x = variable, y = as.factor(std.order), fill = as.factor(value))\n  ) +\n  coord_fixed() +\n  labs(x = \"factor\", y = \"run\", fill = \"level\") +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nFigure 4: The foldover of the previous fractional factorial design.\n\n\n\nVariance of Response Surfaces\nAt any point \\(x\\), it’s possible to estimate the variance of the surface at that point, denoted \\(V\\left(\\hat{y}\\left(x\\right)\\right)\\).\nFor any response surface, we can also imagine an accompanying variance surface.\nAway from factor configurations that we’ve sampled, we expect the variance to increase.\nIf there are only 2 factors, this surface can be directly visualized.\nBut in higher-dimensions, this is impossible.\nIn that case, use a variance dispersion graph to visualize the variance as a function of the distance from the center point of the design.\nSpecifically, plot the maximum, minimum, and average variances along rings that emanate from 0\n\nIf the variances are constant along each ring, the design is called rotatable\n\n\nccd_design <- ccd(~ time_coded + temp_coded, coding = codings, oneblock = TRUE)\npar(mfrow = c(1, 2))\nvarfcn(ccd_design, ~ SO(time_coded, temp_coded))\nvarfcn(ccd_design, ~ SO(time_coded, temp_coded), contour = TRUE, asp = 1)\n\n\n\n\nFigure 5: The scaled prediction variance functions for a rotatable CCD.\n\n\n\n\n\nccd_design <- ccd(~ time_coded + temp_coded, coding = codings, alpha=2, oneblock = TRUE)\npar(mfrow = c(1, 2))\nvarfcn(ccd_design, ~ SO(time_coded, temp_coded))\nvarfcn(ccd_design, ~ SO(time_coded, temp_coded), contour = TRUE, asp = 1)\n\n\n\n\nFigure 6: The analogous functions for a nonrotatable CCD. Note that the axis and diagonal variance curves don’t overlap.\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-17-week12-4/week12-4_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-09-09T11:29:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week12-1/",
    "title": "Method of Steepest Ascent",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-11-30",
    "categories": [],
    "contents": "\nReadings 11.1, 11.2, Rmarkdown\n\n\nlibrary(EBImage)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(reshape2)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/uubzanl5c273c45mxazy0d06r7r0uz52.png\"))\n\n\n\n\nFigure 1: In initial runs we will use first-order models. When we approach a potential optimum, we will switch to second-order models.\n\n\n\nWhen we last discussed response surfaces (in Chapter 5), we showed how to fit a regression surface to a fixed set of runs. This let us find configurations of factors that optimized some response of interest.\nHowever, the real power of response surfaces emerges when we think sequentially, using the results from one fit to plan a series of follow-up experiments, each bringing us closer to an optimal configuration of factors.\nFirst and Second-Order Models\nIt makes sense to gradually refine our designs as we approach a potential optimal point. At the start of experimentation, even a few small runs are likely to point us in the right direction. Near the end, we’ll want to tune a good configuration of factors into an optimal one.\nOne approach is to divide experimentation into two phases. We can start with first-order models, and then proceed to second-order when in the vicinity of the optimum.\nA first-order model is a linear model without any interactions or nonlinearities. It can be fit using relatively few samples; for example, unreplicated or fractional factorial designs. It is defined as, \\[\\begin{align*}\ny_{i} &= \\beta_{0} + \\sum_{k = 1}^{K}\\beta_{k}x_{ik} + \\epsilon_{i} \\\\\n&= x_{i}^{T}b + \\epsilon_{i}\n\\end{align*}\\]\nwhere we defined \\[\\begin{align*}\nx_{i} = \\begin{pmatrix} 1 \\\\\nx_{i1} \\\\\n\\vdots \\\\\nx_{iK}\n\\end{pmatrix},\n\\end{align*}\\] and \\[\\begin{align*}\nb = \\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_{K}\n\\end{pmatrix}\n\\end{align*}\\]\nA second-order model is a linear model with interactions and quadratic terms. These surfaces are more complex, so require more involved sampling, like replicated factorial or central composite designs. It is defined as, \\[\\begin{align*}\ny_{i} &= \\beta_0 + \\sum_{k = 1}^{K}\\beta_{kk}x_{k}^{2} + \\sum_{k < k_{\\prime}} \\beta_{k k^{\\prime}} x_{k}x_{k^{\\prime}} + \\epsilon_{i} \\\\\n&= x_{i}^{T}b + x_{i}^{T} B x_i + \\epsilon_{i}\n\\end{align*}\\]\nwhere we defined \\(B\\) as the symmetric \\(K \\times K\\) matrix with diagonal entries \\(\\beta_{kk}\\) and off-diagonal elements \\(\\frac{1}{2}\\beta_{kl} = \\frac{1}{2}\\beta_{lk}\\).\nMethod of Steepest Ascent\nThe method of steepest ascent uses the results of a first-order model to propose a new configuration of factors that brings us closer to the optimum. Specifically, we use the following recipe,\nUsing the current set of sampled factor configurations, fit the first-order (linear) surface, \\[\\begin{align*}\n\\hat{y}\\left(x\\right) &= x^{T}\\hat{b}\n\\end{align*}\\]\nFind the coordinate \\(k^{\\ast}\\) of the coefficient that has the largest magnitude, \\[\\begin{align*}\n k^{\\ast} &= \\arg \\max_{k \\in \\{1, \\dots, K\\}} \\left|\\hat{\\beta}_{k}\\right|\n\\end{align*}\\]\nwhich represents the factor to which the response is most sensitive. Define some reasonable stepsize \\(\\Delta x_{k^{\\ast}}\\) for this particular factor, so that the next run brings us closer to the optimum without being so far that the first-order model may not hold.\nUpdate each factor’s sampling values according to the sensitivities \\(\\hat{\\beta_{k}}\\) of the response to that factor, and adjusting for the stepsize defined in step (2). \\[\\begin{align*}\nx \\leftarrow x + \\frac{\\Delta x_{k^{\\ast}}}{\\beta_{k^{\\ast}}} b\n\\end{align*}\\]\nCode Example\nLet’s work through example 11.1. Here, an experimenter is trying to optimize yield as a function of reaction time and temperature. The current time / temperature setting is at 35 minutes and 155 degrees. A \\(2^{2}\\) factorial is run with four center points. Let’s visualize the data.\n\n\nchem <- read_csv(\"https://uwmadison.box.com/shared/static/4x9v5wtgu8w8i2kuzjdhtlhkj1tda701.csv\") %>%\n  group_split(seq) # split main from follow-up experiments\n\n\n\n\n\np <- ggplot(chem[[1]]) +\n  geom_point(\n    aes(x = time, y = temp, col = yield),\n    position = position_jitter(w = 0.5, h = 0.5),\n    size = 3\n  ) +\n  coord_fixed() +\n  scale_color_viridis_c()\np\n\n\n\n\nFigure 2: \\(2^2\\) factorial experiment around current operating conditions.\n\n\n\nLet’s code the data – this is a bit more involved than in factorial experiments, because the data don’t simply lie on the corners of a cube. We’ll use the coded.data function from the rsm package to handle this.\n\n\nlibrary(\"rsm\")\nchem_coded <- coded.data(chem[[1]], time_coded ~ (time - 35) / 5, temp_coded ~ (temp - 155) / 5)\n\n\n\nNow we can fit a first-order (FO) model to these data.\n\n\nfit <- rsm(yield ~ FO(time_coded , temp_coded), data = chem_coded)\nsummary(fit)\n\n\n\nCall:\nrsm(formula = yield ~ FO(time_coded, temp_coded), data = chem_coded)\n\n             Estimate Std. Error  t value  Pr(>|t|)    \n(Intercept) 40.444444   0.057288 705.9869 5.451e-16 ***\ntime_coded   0.775000   0.085932   9.0188  0.000104 ***\ntemp_coded   0.325000   0.085932   3.7821  0.009158 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nMultiple R-squared:  0.941, Adjusted R-squared:  0.9213 \nF-statistic: 47.82 on 2 and 6 DF,  p-value: 0.0002057\n\nAnalysis of Variance Table\n\nResponse: yield\n                           Df  Sum Sq Mean Sq F value    Pr(>F)\nFO(time_coded, temp_coded)  2 2.82500 1.41250 47.8213 0.0002057\nResiduals                   6 0.17722 0.02954                  \nLack of fit                 2 0.00522 0.00261  0.0607 0.9419341\nPure error                  4 0.17200 0.04300                  \n\nDirection of steepest ascent (at radius 1):\ntime_coded temp_coded \n 0.9221944  0.3867267 \n\nCorresponding increment in original units:\n    time     temp \n4.610972 1.933633 \n\nThe output looks almost exactly the same as the output for lm, except it also includes the direction of steepest ascent.\nFrom visual inspection, and based on this model’s \\(R^{2}\\), the linear fit seems quite good. But for reference, we can compare with a model with both two-way interactions (TWI) and a quadratic component (PQ). Neither of these terms seem to improve fit, so we’ll stick to the original first-order model. Compare the ANVOA with table 11.2.\n\n\nfit_iq <- rsm(yield ~ FO(time_coded, temp_coded) + TWI(time_coded , temp_coded) + PQ(time_coded, temp_coded), data = chem_coded)\nanova(fit_iq)\n\n\nAnalysis of Variance Table\n\nResponse: yield\n                            Df  Sum Sq Mean Sq F value   Pr(>F)   \nFO(time_coded, temp_coded)   2 2.82500 1.41250 32.8488 0.003294 **\nTWI(time_coded, temp_coded)  1 0.00250 0.00250  0.0581 0.821316   \nPQ(time_coded, temp_coded)   1 0.00272 0.00272  0.0633 0.813741   \nResiduals                    4 0.17200 0.04300                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nascent <- steepest(fit, dist = seq(0, 2, 0.4))\n\n\nPath of steepest ascent from ridge analysis:\n\nascent_long <- steepest(fit, dist = seq(0, 10, 1))\n\n\nPath of steepest ascent from ridge analysis:\n\n\n\np + \n  geom_point(\n    data = ascent %>% select(-`|`),\n    aes(x = time, y = temp, col = yhat),\n    shape = 2 \n  )\n\n\n\n\nFigure 3: A short extrapolation along the direction of steepest ascent.\n\n\n\nThe experimenter follows this path and finds that at each point, the yield does increase, up to a time of 85 and temperature of 175. At this new point, it seems worth running a second factorial experiment.\n\n\np + \n  geom_point(\n    data = ascent_long %>% select(-`|`),\n    aes(x = time, y = temp, col = yhat),\n    shape = 2 \n  ) +\n  geom_point(\n    aes(x = time, y = temp, col = yield),\n    position = position_jitter(w = 0.4, h = 0.4),\n    size = 3\n  ) +\n  geom_point(\n    data = chem[[2]],\n    aes(x = time, y = temp, col = yield),\n    position = position_jitter(w = 0.4, h = 0.4),\n    size = 3\n  )\n\n\n\n\nFigure 4: A longer extrapolation, up to the point where the experimenter noticed a decrease in yield, along with the follow-up experiment at that location.\n\n\n\n\n\nggplot(chem[[2]]) +\n  geom_point(\n    aes(x = time, y = temp, col = yield),\n    position = position_jitter(w = 0.4, h = 0.4),\n    size = 3\n  ) +\n  coord_fixed() +\n  scale_color_viridis_c()\n\n\n\n\nAt this point, it seems like we might be at a local maximum. We will likely want to perform more intensive sampling – more than just a \\(2^2\\) design with center points – in order to more precisely localize the optimum. For that, we’ll need to understand second-order methods and canonical analysis.\n\n\n\n",
    "preview": "posts/2021-08-17-week12-1/week12-1_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-09-09T11:28:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week12-2/",
    "title": "Canonical Analysis",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-11-30",
    "categories": [],
    "contents": "\nReadings 11.3, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"readr\")\nlibrary(\"EBImage\")\nlibrary(\"rsm\")\nlibrary(\"reshape2\")\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/oi55vfta5tpd7foy8ur3lzt6gq5fkjfn.png\"))\n\n\n\n\nFigure 1: First order models have no isolated optima.\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/qn1f8hrbb8uwtbflv02axa817m6tcmb7.png\"))\n\n\n\n\nFigure 2: Second order model with a local minimum.\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/x81swx5xslpzziywsupkuwpkyfggagis.png\"))\n\n\n\n\nFigure 3: Second order model with a saddle point.\n\n\n\nOne we are in the vicinity of a potentially optimal point, we need more subtle methods. There are two essential issues,\nWe need a way of deciding whether a point is optimal. The method of steepest ascent will always report a new direction to explore, even if the benefit is marginal. At some point we need to stop.\nWe need a better way of proposing new directions to explore, when the first-order model is insufficient. Since the method of steepest ascent relies on a first-order model, it will not give good directions when the surface is actually highly nonlinear.\nCanonical analysis helps solve both of these problems. First, we’ll define canonical analysis, and then we’ll illustrate its use in finding optimal points and proposing new directions.\nMathematical Setup\nA first-order model never has any isolated optima. It’s either perfectly flat, or increases forever in some direction. A second-order model, however, does potentially have optimal values.\nCalculus can help us identify these optima. Recall that, at a function’s maximum, its gradient is zero. This makes it natural to consider the gradient of a fitted second order surface, \\[\\begin{align*}\n\\nabla_{x}\\hat{y}\\left(x\\right) &= \\nabla_{x}\\left[x^{T}\\hat{b} + x^{T}\\hat{B}x\\right] \\\\\n&= \\hat{b} + 2\\hat{B}x\n\\end{align*}\\]\nso the function has a stationary point at \\[\\begin{align*}\nx_{\\ast} &= -\\frac{1}{2}\\hat{B}^{-1}\\hat{b}.\n\\end{align*}\\]\nWe’ll denote the value of the surface at this optimal point by \\[\\begin{align*}\n\\hat{y}_{\\ast} &= \\hat{b}^{T}x + x^{T}\\hat{B}\\hat{x} \\\\\n&= -\\frac{1}{2}\\hat{b}^{T}\\hat{B}^{-1}\\hat{b} + \\frac{1}{4}\\hat{b}^{T} \\hat{B}^{-1} \\hat{B} \\hat{B}^{-1} \\hat{b} \\\\\n&= -\\frac{1}{4}\\hat{b}^{T}\\hat{B}^{-1}\\hat{b}\n\\end{align*}\\]\nJust like in ordinary calculus, there are three possibilities,\nThe stationary point is a maximum\nThe stationary point is a minimum\nThe stationary point is a saddlepoint\nIn one-variable calculus, we’d just take second derivatives, and see whether the function curves up, down, or is flat. We’re in higher-dimensions, though, so we’ll use the generalization of the second-derivative test, called…\nCanonical Representation\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/ngiy01l2mtvem1o9e6diwawhypgf3jst.png\"))\n\n\n\n\nFigure 4: Geometric representation of the eigendecomposition.\n\n\n\nSince \\(\\hat{B}\\) is symmetric, we can find an eigendecomposition, \\[\\begin{align*}\n\\hat{B} &= U \\Lambda U^{T},\n\\end{align*}\\]\nwhere \\(U\\) are orthogonal eigenvectors and \\(\\Lambda\\) is a diagonal matrix containing eigenvalues \\(\\lambda_{k}\\). Define \\(w\\left(x\\right) = U^{T}\\left(x - x_{\\ast}\\right)\\). It turns out that our second-order fit can be equivalently expressed as \\[\\begin{align*}\n\\hat{y}\\left(x\\right) &= \\hat{y}_{\\ast} + w^T\\left(x\\right)\\Lambda w\\left(x\\right) \\\\\n&= \\hat{y}_{\\ast} + \\sum_{k = 1}^{K} \\lambda_{k}w^{2}_{k}\\left(x\\right)\n\\end{align*}\\]\nwhich is much simpler because there are no interaction terms between the \\(w_{k}\\)’s (like there are between \\(x_{k}\\)’s).\nProof1: The textbook skips the proof of this fact, but if you have seen some linear algebra, the approach is interesting. First, plug-in the value of \\(\\hat{y}_{\\ast}\\) from above, and expand the definition of \\(w\\left(x\\right)\\), \\[\\begin{align*}\n\\hat{y}\\left(x\\right) &= \\hat{y}_{\\ast} + w^{T}\\left(x\\right) \\Lambda w\\left(x\\right) \\\\\n&= -\\frac{1}{4}\\hat{b}^{T}\\hat{B}\\hat{b} + \\left(x - x_{\\ast}\\right)^{T}U\\Lambda U^{T} \\left(x - x_{\\ast}\\right).\n\\end{align*}\\]\nNow, use the definition of our original eigendecomposition \\(\\hat{B} = U \\Lambda U^{T}\\) and expand the quadratic, \\[\\begin{align*}\n\\hat{y}\\left(x\\right) &= -\\frac{1}{4}\\hat{b}^{T}\\hat{B}\\hat{b} + \\left(x - x_{\\ast}\\right)^{T}\\hat{B}\\left(x - x_{\\ast}\\right) \\\\\n&= -\\frac{1}{4}\\hat{b}^{T}\\hat{B}\\hat{b} + x^{T}\\hat{B}x - 2x^{T}\\hat{B}x_{\\ast} + x^{T}_{\\ast}\\hat{B}x_{\\ast}.\n\\end{align*}\\]\nTo simplify, let’s plug-in the expression for the stationary point \\(x_{\\ast} = -\\frac{1}{2}\\hat{B}^{-1}\\hat{b}\\) that we found above, \\[\\begin{align*}\n\\hat{y}\\left(x\\right) &= -\\frac{1}{4}\\hat{b}^{T}\\hat{B}\\hat{b} + x^{T}\\hat{B}x + x^{T}\\hat{B}\\hat{B}^{-1}\\hat{b} + \\frac{1}{4}\\hat{b}^{T}\\hat{B}^{-1}\\hat{B}\\hat{B}^{-1}\\hat{b} \\\\\n&= \\hat{b}^{T}x + x^{T}\\hat{B}x\n\\end{align*}\\]\nwhich was exactly our original definition of the second-order model.\nImplications\nThe representation of the second-order model by \\[\\begin{align}\n\\label{eq:canonical}\n\\hat{y}\\left(x\\right) &= \\hat{y}_{\\ast} + \\sum_{k = 1}^{K} \\lambda_{k}w^{2}_{k}\\left(x\\right)\n\\end{align}\\]\nis an important one, because\nIt provides the high-dimensional analog of the second-derivative test.\nIt suggests new configurations of \\(x\\) that might be better.\nTo see 1., suppose the \\(\\lambda_{k}\\)’s were all negative. Then, expression  is maximized when \\(w_{k}\\left(x\\right)\\) are all zero — any change in the \\(w_{k}\\left(x\\right)\\)’s from there can only ever decrease \\(\\hat{y}\\left(x\\right)\\).\nBut \\(w\\left(x\\right) = 0\\) happens at \\(x = x_{\\ast}\\), by definition of \\(w\\left(x\\right)\\).\nThis means \\(x_{\\ast}\\) is a maximum!\nSimilarly, when all \\(\\lambda_{k}\\) are all positive, then moving the \\(w_{k}\\left(x\\right)\\)’s by any amount can only increase \\(\\hat{y}\\left(x\\right)\\). This means we’re at a minimum!\nFinally, when some of the \\(\\lambda_{k}\\)’s are positive and others are negative, then we’re at a saddlepoint.\nIncreasing \\(w_{k}\\left(x\\right)\\) for \\(\\lambda_{k}\\)’s that are positive will increase \\(\\hat{y}\\left(x\\right)\\).\nIncreasing \\(w_{k}\\left(x\\right)\\) for \\(\\lambda_{k}\\)’s that are negative will decrease \\(\\hat{y}\\left(x\\right)\\).\nThis last discussion also gives us insight into 2.\nTo find configurations \\(x\\) that further increase the value of the response surface \\(\\hat{y}\\left(x\\right)\\), increase \\(w_{k}\\left(x\\right)\\) for those \\(k\\)’s where \\(\\lambda_{k}\\) is most positive.\nData Example\nWe’ll continue the chemical process experiment from last time. Here, the experimenter has performed a central composite design experiment, a refinement of the earlier factorial designs. The hope is that now a configuration that maximizes the yield can be precisely isolated.\n\n\nchem <- read_csv(\"https://uwmadison.box.com/shared/static/nbaj1m8j7tuaqmznjlrsgbzyhp9k61i8.csv\")\nggplot(chem) +\n  geom_point(\n    aes(x = time, y = temp, col = yield),\n    position = position_jitter(w = 0.3, h = 0.3)\n  ) +\n  coord_fixed() +\n  scale_color_viridis_c()\n\n\n\n\nAs before, we will code the data. We use SO to fit a second-order model. It appears that the linear and pure quadratic components are very strong, and that there is limited, if any, interaction between these two variables.\n\n\nchem_coded <- coded.data(chem, time_coded ~ (time - 35) / 5, temp_coded ~ (temp - 155) / 5)\nfit <- rsm(yield ~ SO(temp_coded, time_coded), data = chem_coded)\nanova(fit)\n\n\nAnalysis of Variance Table\n\nResponse: yield\n                            Df  Sum Sq Mean Sq  F value    Pr(>F)    \nFO(temp_coded, time_coded)   2 10.0430  5.0215  70.8143 2.267e-05 ***\nTWI(temp_coded, time_coded)  1  0.2500  0.2500   3.5256    0.1025    \nPQ(temp_coded, time_coded)   2 17.9537  8.9769 126.5944 3.194e-06 ***\nResiduals                    7  0.4964  0.0709                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNext, we can perform a canonical analysis, to make sure that we are at a stationary point. Since both of the eigenvalues are negative, this is in fact the case. The eigenvectors also tell us directions that (in this case) decrease the yield fastest.\n\n\nanalysis <- canonical(fit)\nanalysis\n\n\n$xs\ntemp_coded time_coded \n  4.305847  10.389230 \n\n$eigen\neigen() decomposition\n$values\n[1] -0.9634986 -1.4142867\n\n$vectors\n                 [,1]       [,2]\ntemp_coded -0.9571122 -0.2897174\ntime_coded -0.2897174  0.9571122\n\nWe can identify the specific location of the optimum, by decoding the canonical analysis.\n\n\nstationary <- code2val(analysis$xs, codings = codings(chem_coded))\nstationary\n\n\n     temp      time \n176.52923  86.94615 \n\n\n\nw1 <- code2val(analysis$xs + analysis$eigen$vectors[, 1], codings = codings(chem_coded))\nw2 <- code2val(analysis$xs + analysis$eigen$vectors[, 2], codings = codings(chem_coded))\n\n\n\nFinally, let’s plot the response surface, with the canonical points overlaid.\n\n\ncontour(fit, ~ time_coded + temp_coded, image = TRUE, asp = 1)\nsegments(stationary[2], stationary[1], w1[2], w1[1], col = \"red\")\nsegments(stationary[2], stationary[1], w2[2], w2[1], col = \"red\")\n\n\n\n\nFigure 5: The response surface, with canonical vectors overlaid.\n\n\n\n\nYou can skip this proof without worrying whether it will appear in later lectures or assignments / exams. But I encourage you to go on, it gives a flavor of more advanced topics in statistics.↩︎\n",
    "preview": "posts/2021-08-17-week12-2/week12-2_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-09-09T11:28:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week11-3/",
    "title": "Foldover in $2^{K - p}$ Designs",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-11-25",
    "categories": [],
    "contents": "\nReadings 8.6, 8.7, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(EBImage)\nlibrary(reshape2)\nlibrary(ggplot2)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\ncode <- function(x) ifelse(x == '-', -1, 1)\n\nplot_aliases <- function(fit, n_keep = NULL, n_sample=8) {\n  X <- model.matrix(fit)\n  aliases <- (1 / n_sample) * t(X) %*% X\n  \n  if (!is.null(n_keep)) {\n    aliases <- aliases[1:n_keep, 1:n_keep]\n  }\n  \n  maliases <- melt(aliases)\n  ggplot(maliases) +\n    geom_tile(aes(x = Var1, y = Var2, fill = as.factor(value)), col = \"black\") +\n    scale_fill_manual(values = c(\"-1\" = \"#be8fc5\", \"1\" = \"#648d90\", \"0\" = \"#e3e6e3\")) +\n    theme(\n      legend.position = \"bottom\",\n      axis.title = element_blank(),\n      axis.ticks = element_blank(),\n      axis.text = element_text(size = 6),\n      axis.text.x = element_text(angle = -90)\n    ) +\n    coord_fixed()\n}\n\ndaniel_plot <- function(effects, probs = c(0.4, 0.6)) { \n  qq <- qqnorm(effects, datax = TRUE)\n  qqline(effects, col = \"red\", probs = probs, datax = TRUE)\n  text(qq$x, qq$y, names(effects), pos=1)\n}\n\n\n\nThe main drawback of fractional designs is that we can end up with aliased effects. However, there are specific ways to follow-up initial experiments in a way that dealiases these effects.\nBest case: The hierarchical and hereditary principles allow strong effects to be isolated using only a \\(\\frac{1}{2^p}\\) fraction of the samples needed for the full design.\nWorst case: Strong effects remain aliased, and a follow-up dealiasing experiment is required.\nWe’ll discuss two dealiasing strategies, full-foldover and single-factor foldover. They are both particularly relevant in resolution III designs, where main effects can be confounded with order-2 interactions.\nFull Foldover\n\n\neye <- read_table2(\"https://uwmadison.box.com/shared/static/zh7majh2s6gesnu6f27fl17ncqfuwzev.txt\") %>%\n  mutate_at(vars(-Seq, -y), code)\n\n\n\n\n\nmdesign <- eye %>%\n  mutate(ix = row_number()) %>%\n  select(-y) %>%\n  melt(id.vars = c(\"ix\", \"Seq\"))\nggplot(mdesign) +\n  geom_tile(aes(x = variable, y = as.factor(ix), fill = as.factor(value))) +\n  scale_fill_brewer(palette = \"Set3\") +\n  facet_grid(Seq ~ ., scale = \"free_y\") +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 1: Setup for the \\(2^{7 - 4}\\) design with foldover used in the eye focus experiment. The second panel is the full foldover of the first.\n\n\n\nUse case: We want to delias main effects for all factors under study\nAssume that there aren’t any interaction effects with order > 2\n\nIdea: For a second fractional factorial run, reverse the signs of all factors\nWhat does this do? Let’s consider an the eye focus time example from the textbook (Example 8.7). The design is a \\(2^{7 - 3}_{III}\\) fractional factorial experiment (Seq == 1) that has then been folded over (Seq == 2) by reversing the signs of factors.\nLet’s consider the alias structure for the original fractional factorial, ignoring all interactions of order 3 and higher.\n\n\nfit <- lm(y ~ A * B * C * D * E * F * G, data = eye[eye$Seq == 1, ])\nplot_aliases(fit, 29)\n\n\n\n\nFigure 2: Alias pattern of the original \\(2^{7 - 3}\\) design.\n\n\n\nParsing this matrix, the effects derived from alias groups are \\[\\begin{align*}\n[A] = A + BD + CE + FG \\\\\n[B] = B + AD + CF + EG \\\\\n[C] = C + AE + BF + DG \\\\\n[D] = D + AB + CG + EF \\\\\n[E] = E + AC + BG + DF \\\\\n[F] = F + BC + BG + DE \\\\\n[G] = G + CD + BE + AF\n\\end{align*}\\]\n\n\nfit <- lm(y ~ A * B * C * D * E * F * G, data = eye[eye$Seq == 2, ])\nplot_aliases(fit, 29)\n\n\n\n\nFigure 3: Alias pattern after a full foldover. Note that the signs between main effects and their aliased interactions have switched.\n\n\n\nNow, suppose we reversed the signs of all the factors. What happens to the alias groups? The signs for the second order interactions flip! The resulting effect estimates are \\[\\begin{align*}\n[A]^{fold} = A - BD - CE - FG \\\\\n[B]^{fold} = B - AD - CF - EG \\\\\n[C]^{fold} = C - AE - BF - DG \\\\\n[D]^{fold} = D - AB - CG - EF \\\\\n[E]^{fold} = E - AC - BG - DF \\\\\n[F]^{fold} = F - BC - BG - DE \\\\\n[G]^{fold} = G - CD - BE - AF\n\\end{align*}\\]\nThe punchline is that we can now estimate the main effects without any aliasing, \\[\\begin{align*}\nA = \\frac{1}{2}\\left(\\left[A\\right] + \\left[A\\right]^{fold}\\right) \\\\\nB = \\frac{1}{2}\\left(\\left[B\\right] + \\left[B\\right]^{fold}\\right) \\\\\nC = \\frac{1}{2}\\left(\\left[C\\right] + \\left[C\\right]^{fold}\\right) \\\\\nD = \\frac{1}{2}\\left(\\left[D\\right] + \\left[D\\right]^{fold}\\right) \\\\\nE = \\frac{1}{2}\\left(\\left[E\\right] + \\left[E\\right]^{fold}\\right) \\\\\nF = \\frac{1}{2}\\left(\\left[F\\right] + \\left[F\\right]^{fold}\\right) \\\\\nG = \\frac{1}{2}\\left(\\left[G\\right] + \\left[G\\right]^{fold}\\right) \\\\\n\\end{align*}\\]\nThis is in fact a general principle for dealising main effects from second order interactions. When you flip the signs of all factors in an original fractional factorial design, you will get a cancellation of second-order terms when you add pairs of effect estimates.\nData Example\nLet’s use these ideas to study effects in the eye data experiment. Let’s first make a Daniel plot of the effects we’d find when running the fractional factorial before foldover.\n\n\neye1 <- eye[eye$Seq == 1, ]\nfit <- lm(y ~ A * B * C * D * E * F, data = eye1)\neffects <- coef(fit)[-1]\ndaniel_plot(effects, probs = c(0.1, 0.5))\n\n\n\n\nFigure 4: From the original fraction, it seems that the effects for [A], [B], and [D] are important.\n\n\n\nIt seems like \\(\\left[B\\right], \\left[D\\right]\\), and / or \\(\\left[A\\right]\\) are important. Inspecting the corresponding alias groups, and using the heredity principle, some plausible situations are\n\\(A, B, D\\) are important\n\\(A, B, AB\\) are important\n\\(A, D, AD\\) are important\n\\(B, D, BD\\) are important\nBut without more information, we can’t draw further conclusions. To that end, suppose we’ve run the full foldover experiment. Let’s estimate effects in this run and then add them to effects from before – this is how we can estimate the main effects.\n\n\nfit <- lm(y ~ A * B * C * D * E * F, data = eye[eye$Seq == 2, ])\neffects_fold <- coef(fit)[-1]\n(effects + effects_fold)[1:6]\n\n\n     A      B      C      D      E      F \n 1.475 38.050 -1.800 29.375  0.125  0.500 \n\nIt’s now clear that the main effect for \\(A\\) is in fact not important. The only plausible situation is that \\(B, D\\) and the \\(BD\\) interaction are strong.\nSingle-Factor Foldover\nUse case: We want to dealias main and interaction effects associated with a single factor in the study\nIdea: For a second fractional factorial run, reverse the signs of just the factor of interest.\nThe mechanics at work here are similar to those in the full foldover. As before, let’s focus attention on the \\(2^{7 - 3}_{III}\\). Remember that the effect estimates were, \\[\\begin{align*}\n[A] = A + BD + CE + FG \\\\\n[B] = B + AD + CF + EG \\\\\n[C] = C + AE + BF + DG \\\\\n[D] = D + AB + CG + EF \\\\\n[E] = E + AC + BG + DF \\\\\n[F] = F + BC + BG + DE \\\\\n[G] = G + CD + BE + AF\n\\end{align*}\\]\nSuppose we flip the sign of factor \\(D\\) in the follow-up run. The new effect estimates would be, \\[\\begin{align*}\n[A]^{fold} &= A - BD + CE + FG \\\\\n[B]^{fold} &= B - AD + CF + EG \\\\\n[C]^{fold} &= C + AE + BF + DG \\\\\n[D]^{fold} &= -D + AB + CG + EF \\\\\n[E]^{fold} &= E + AC + BG - DF \\\\\n[F]^{fold} &= F + BC + BG - DE \\\\\n[G]^{fold} &= G - CD + BE + AF\n\\end{align*}\\]\nYou can see this in the plot — all the signs from terms involving \\(D\\) are flipped.\nIn particular, notice that we can estimate the main effect of \\(D\\) using \\[\\begin{align*}\nD = \\frac{1}{2}\\left(\\left[D\\right] - \\left[D\\right]^{fold}\\right)\n\\end{align*}\\]\nand interactions involving \\(D\\) using, for example, \\[\\begin{align*}\nAD = \\frac{1}{2}\\left(\\left[B\\right] - \\left[B\\right]^{fold}\\right).\n\\end{align*}\\]\n\n\n\n",
    "preview": "posts/2021-08-17-week11-3/week11-3_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-09-09T11:28:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week11-1/",
    "title": "$2^{K - p}$ Fractional Factorial Designs",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-11-23",
    "categories": [],
    "contents": "\nReadings 8.3, 8.4, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(reshape2)\nlibrary(EBImage)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\ncode <- function(x) ifelse(x == '-', -1, 1)\nplot_aliases <- function(fit, n_keep = NULL, n_sample=16) {\n  X <- model.matrix(fit)\n  aliases <- (1 / n_sample) * t(X) %*% X\n  \n  if (!is.null(n_keep)) {\n    aliases <- aliases[1:n_keep, 1:n_keep]\n  }\n  \n  maliases <- melt(aliases)\n  ggplot(maliases) +\n    geom_tile(aes(x = Var1, y = Var2, fill = as.factor(value)), col = \"black\") +\n    scale_fill_brewer(palette = \"Set3\") +\n    theme(\n      legend.position = \"none\",\n      axis.title = element_blank(),\n      axis.ticks = element_blank(),\n      axis.text = element_text(size = 6),\n      axis.text.x = element_text(angle = -90)\n    ) +\n    coord_fixed()\n}\ndaniel_plot <- function(effects, probs = c(0.4, 0.6)) { \n  qq <- qqnorm(effects, datax = TRUE)\n  qqline(effects, col = \"red\", probs = probs, datax = TRUE)\n  text(qq$x, qq$y, names(effects), pos=1)\n}\n\n\n\n\\(2^{K - 1}\\) designs allow us to draw conclusions about \\(K\\) factors using only half of the runs that would be required for a full \\(2^{K}\\) factorial. It’s possible to generalize these ideas so that smaller fractions (\\(\\frac{1}{4}, \\frac{1}{8}, \\dots\\) of samples are required. When only a fraction \\(\\frac{1}{2^{p}}\\) is required, we call the resulting design a \\(2^{K - p}\\) fractional factorial design.\nRecipe\nWe can begin constructing a \\(2^{K - p}\\) design by building a full factorial design from \\(K - p\\) of the factors\nThe associated full factorial is called the basic design\n\nWe then need \\(p\\) generating relations, which confound the remaining \\(p\\) factors with terms from the full factorial design\nThe complete defining relation for a design is the set of columns that equal \\(I\\), the identity column\nThese can be found by looking at generating relations and their products\n\nHere’s the recipe in action.\nSuppose we want a \\(2^{6 - 2}\\) design.\nStudy 6 factors using 16 (not 64) runs.\n\nCreate a full factorial on the first four factors\nI’m only writing contrasts for the main effects\nYou can get the corresponding corner of the cube by isolating the positive signs for that row. E., the first two rows are \\((1)\\) and \\(a\\), and the last two are \\(bcd\\) and \\(abcd\\).\n\nA\nB\nC\nD\n-\n-\n-\n-\n+\n-\n-\n-\n-\n+\n-\n-\n-\n-\n+\n-\n-\n-\n-\n+\n+\n+\n-\n-\n+\n-\n+\n-\n+\n-\n-\n+\n-\n+\n+\n-\n-\n+\n-\n+\n-\n-\n+\n+\n+\n+\n+\n-\n+\n+\n-\n+\n+\n-\n+\n+\n-\n+\n+\n+\n+\n+\n+\n+\nStudy the last two factors through the defining relations \\(E = ABC\\) and \\(F = BCD\\). Our hope is that by defining them as high-order interactions, we have a chance at higher resolution.\nThe associated complete defining relations are \\(I = ABCE = BCDF = ADEF\\).\nTo see this, notice \\(E^2 = ABCE\\), but any term squared is just \\(I\\). The last relation comes from multiplying the two previous ones together.\n\nThe resulting design is\nA\nB\nC\nD\nE = ABC\nF = BCD\n-\n-\n-\n-\n-\n-\n+\n-\n-\n-\n+\n-\n-\n+\n-\n-\n+\n+\n-\n-\n+\n-\n+\n+\n-\n-\n-\n+\n-\n+\n+\n+\n-\n-\n-\n+\n+\n-\n+\n-\n-\n+\n+\n-\n-\n+\n+\n+\n-\n+\n+\n-\n-\n-\n-\n+\n-\n+\n+\n-\n-\n-\n+\n+\n+\n-\n+\n+\n+\n-\n+\n-\n+\n+\n-\n+\n-\n-\n+\n-\n+\n+\n-\n-\n-\n+\n+\n+\n-\n+\n+\n+\n+\n+\n+\n+\nLet’s analyze this design,\nThe alias groups are complicated looking, but they can be found by mindlessly multiplying the defining relations by each of the factors and combinations of factors.\nIn practice, you would use code to find the aliases (see below)\n\n\\[\\begin{align*}\nI&=A B C E=B C D F=A D E F \\\\\nA&=B C E=D E F=A B C D F \\\\\nB&=A C E=C D F=A B D E F \\\\\nC&=A B E=B D F=A C D E F \\\\\nD&=B C F=A E F=A B C D E \\\\\nE&=A B C=A D F=B C D E F \\\\\nF&=B C D=A D E=A B C E F \\\\\nA B&=C E=A C D F=B D E F \\\\\nA C&=B E=A B D F=C D E F \\\\\nA D&=E F=B C D E=A B C F \\\\\nA E&=B C=D F=A B C D E F \\\\\nA F&=D E=B C E F=A B C D \\\\\nB D&=C F=A C D E=A B E F \\\\\nB F&=C D=A C E F=A B D E \\\\\nA C D&=B D E=A B F=C E F \\\\\nA B D&=C D E=A C F=B E F\n\\end{align*}\\]\nFrom the alias groups, we can tell that the resolution is 4. Two-way interactions are confounded with one another, but not with any main effects.\nFor an exercise, you can try going through this process using an alternative confounding structure: Set \\(E = ABCD\\), and \\(F = ABC\\). It’s somewhat tedious, but will build your confidence with this type of design.\nCode Example\nLet’s use the \\(2^{6 - 2}\\) design that we just constructed on a dataset about injection molding. `r tufte::margin_note(“Apparently this is how paper clips are made.”) In any case, it’s the example in the book (Example 8.4). The 6 factors are, * A: mold temperature * B: screw speed * C: holding time * D: cycle time * E: gate size * F: holding pressure\n\n\ninjection <- read_table2(\"https://uwmadison.box.com/shared/static/uxd6sryqz32gbubwfhbdvsnqqkplqqef.txt\") %>%\n  mutate_at(vars(-Shrinkage), code)\nhead(injection)\n\n\n# A tibble: 6 × 7\n      A     B     C     D     E     F Shrinkage\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>     <dbl>\n1    -1    -1    -1    -1    -1    -1         6\n2     1    -1    -1    -1     1    -1        10\n3    -1     1    -1    -1     1     1        32\n4     1     1    -1    -1    -1     1        60\n5    -1    -1     1    -1     1     1         4\n6     1    -1     1    -1    -1     1        15\n\n\n\nmdesign <- injection %>%\n  mutate(id = row_number()) %>%\n  select(-Shrinkage) %>%\n  melt(id.vars = \"id\")\nggplot(mdesign) +\n  geom_tile(aes(x = variable, y = id, fill = as.factor(value))) +\n  coord_fixed() +\n  scale_fill_brewer(palette = \"Set3\")\n\n\n\n\nFigure 1: Visualizing a \\(2 ^ {6 - 2}\\) design for the injection experiment. Each column is a factor, each row is a run.\n\n\n\nWe can look at the aliasing structure by inspecting the design matrix.\n\n\nfit <- lm(Shrinkage ~ A * B * C * D * E * F, data = injection)\nplot_aliases(fit)\n\n\n\n\n\n\nplot_aliases(fit, 22)\n\n\n\n\nLet’s make a Daniel plot and look at effects.\n\n\ndaniel_plot(2 * coef(fit)[-1])\n\n\n\n\nLet’s refit to just the submodel.\n\n\nfit <- lm(Shrinkage ~ A * B, data = injection)\nanova(fit)\n\n\nAnalysis of Variance Table\n\nResponse: Shrinkage\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nA          1  770.1   770.1  37.149 5.377e-05 ***\nB          1 5076.6  5076.6 244.899 2.392e-09 ***\nA:B        1  564.1   564.1  27.211  0.000216 ***\nResiduals 12  248.8    20.7                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ninjection <- injection %>%\n  mutate(residual = resid(fit))\nggplot(injection) +\n  geom_point(aes(x = C, y = residual, col = Shrinkage)) +\n  scale_color_viridis_c() +\n  facet_grid(A ~ B)\n\n\n\n\nNotice that the residuals have lower variance at the setting \\(C = -1\\). Even though \\(C\\) doesn’t have a strong effect, that might be a useful fact, in case you want to select a configuration with the optimal response and low variability around that response.\nEvaluating Designs\nIn general, there will be several ways of constructing any \\(2^{p}\\) design. How do we know which one to use? Here are some criteria.\nResolution: We’ve discussed this before. Remember, a design has resolution \\(R\\) if no \\(p\\)-factor effect is aliased with an effect with fewer than \\(R - p\\) factors.\nAberration: This is the number of defining words of length equal to the resolution. Lower is better.\nThe example above had words \\(ABCE, BCDF\\) and \\(ADEF\\) and resolution 4, so aberration is 3.\n\n",
    "preview": "posts/2021-08-17-week11-1/week11-1_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-09-09T11:28:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week11-2/",
    "title": "Projection and Blocking in $2^{K - p}$ Designs",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-11-23",
    "categories": [],
    "contents": "\nReadings 8.3, 8.4, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(EBImage)\nlibrary(reshape2)\nlibrary(ggplot2)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/b94hfeyugeun1xywlm2ccls9am5l38xg.jpg\"))\n\n\n\n\nFigure 1: Our earlier picture of projection from 3 to 2 factors in a complete factorial design.\n\n\n\nTo be fluent in working with fractional factorial designs, it’s worth knowing how to project and block them. They provide useful alternative perspectives of the designs we have already constructed.\nProjection\nProjection makes it possible to turn a fractional factorial screening experiment into a full factorial experiment\nEffectively, you delete the columns of your design matrix involving the factors outside the ones you want to project down to\nDepending on what design you project, a \\(2^{K - p}\\) design will become either (1) a full factorial design or (2) a \\(2^{K - r}\\) fractional factorial design, where \\(r < p\\).\nBeware that conclusions will be somewhat tentative, because effects in the full factorial could be aliased with high-order effects from factors that have been projected out\nWarm-up: (Projecting a \\(2 ^ {4 - 1}\\) Design). Consider the \\(2 ^ {4 - 1}\\) design with defining relation \\(I = ABCD\\).\n\nA\nB\nC\nD\n-\n-\n-\n-\n+\n-\n-\n+\n-\n+\n-\n+\n-\n-\n+\n+\n+\n+\n-\n-\n+\n-\n+\n-\n-\n+\n+\n-\n+\n+\n+\n+\nWhat happens when you project out A?\nB\nC\nD\n-\n-\n-\n-\n-\n+\n+\n-\n+\n-\n+\n+\n+\n-\n-\n-\n+\n-\n+\n+\n-\n+\n+\n+\nYou can check that this has all corners of the cube B-C-D, so it’s a full \\(2^{3}\\) design.\nExample (Different projections of \\(2 ^ {6 - 2}\\) design). Let’s consider the example from our previous lecture\n\nA\nB\nC\nD\nE = ABC\nF = BCD\n-\n-\n-\n-\n-\n-\n+\n-\n-\n-\n+\n-\n-\n+\n-\n-\n+\n+\n-\n-\n+\n-\n+\n+\n-\n-\n-\n+\n-\n+\n+\n+\n-\n-\n-\n+\n+\n-\n+\n-\n-\n+\n+\n-\n-\n+\n+\n+\n-\n+\n+\n-\n-\n-\n-\n+\n-\n+\n+\n-\n-\n-\n+\n+\n+\n-\n+\n+\n+\n-\n+\n-\n+\n+\n-\n+\n-\n-\n+\n-\n+\n+\n-\n-\n-\n+\n+\n+\n-\n+\n+\n+\n+\n+\n+\n+\nThe study examined 6 factors over 16 runs.\nThe complete defining relations were \\(I = ABCE = BCDF = ADEF\\).\nWhat happens when we project out \\(A\\)?\nB\nC\nD\nE\nF\n\n-\n-\n-\n-\n-\n\n-\n-\n-\n+\n-\n\n+\n-\n-\n+\n+\n\n-\n+\n-\n+\n+\n\n-\n-\n+\n-\n+\n\n+\n-\n-\n-\n+\n\n-\n+\n-\n-\n+\n\n-\n-\n+\n+\n+\n\n+\n+\n-\n-\n-\n\n+\n-\n+\n+\n-\n\n-\n+\n+\n+\n-\n\n+\n+\n-\n+\n-\n\n+\n-\n+\n-\n-\n\n-\n+\n+\n-\n-\n\n+\n+\n+\n-\n+\n\n+\n+\n+\n+\n+\n\nEach configuration appears once. Since we have 16 rows still, this is evidently a \\(2^{5 - 1}\\) fractional factorial design.\nWhat happens if we project out \\(E\\) and \\(F\\) instead?\nThis is exactly the original “basic design.” I.e., the the full \\(2 ^ 4\\) factorial before using the generators.\nWe could try to project out two factors that don’t have such an obvious answer. So let’s project out \\(A\\) and \\(B\\).\nIt turns out we still get a \\(2 ^ 4\\) full factorial, but now on \\(C\\) through \\(F\\).\nC\nD\nE\nF\n\n\n-\n-\n-\n-\n\n\n-\n-\n+\n-\n\n\n-\n-\n+\n+\n\n\n+\n-\n+\n+\n\n\n-\n+\n-\n+\n\n\n-\n-\n-\n+\n\n\n+\n-\n-\n+\n\n\n-\n+\n+\n+\n\n\n+\n-\n-\n-\n\n\n-\n+\n+\n-\n\n\n+\n+\n+\n-\n\n\n+\n-\n+\n-\n\n\n-\n+\n-\n-\n\n\n+\n+\n-\n-\n\n\n+\n+\n-\n+\n\n\n+\n+\n+\n+\n\n\nYou might start suspecting that whenever we project out two variables, we will end up with a full \\(2^{4}\\) factorial. But this is not the case!\nConsider projecting out \\(A\\) and \\(E\\).\nNow we have a \\(2 ^ {4 - 1}\\) fractional factorial with 2 replicates.\nOnly half of the corners are present, but each one that appears is present twice.\nB\nC\nD\nF\n\n\n-\n-\n-\n-\n\n\n-\n-\n-\n-\n\n\n+\n-\n-\n+\n\n\n-\n+\n-\n+\n\n\n-\n-\n+\n+\n\n\n+\n-\n-\n+\n\n\n-\n+\n-\n+\n\n\n-\n-\n+\n+\n\n\n+\n+\n-\n-\n\n\n+\n-\n+\n-\n\n\n-\n+\n+\n-\n\n\n+\n+\n-\n-\n\n\n+\n-\n+\n-\n\n\n-\n+\n+\n-\n\n\n+\n+\n+\n+\n\n\n+\n+\n+\n+\n\n\nThis ends up being a general rule. If none of the defining words contain all the factors that you’re projecting out, then you’re left with a full factorial design; otherwise you get a fractional factorial with replicates.\nE.g., \\(AE\\) is part of the first defining relation, so we got a fractional factorial.\nExercise: Try projecting onto \\(A, B, C\\). Is it a fractional or full factorial? How many replicates each? Could you have determined this, just by looking at the defining relations?\nA design is said to have projectivity \\(v\\) is collapsing to any subset of \\(v\\) factors will result in a full factorial design. The example above has projectivity 3.\nBlocking\nLet’s say we’ve subsetted to a set of runs corresponding to a fractional factorial design. But, we still have to break the runs into batches, we have too many runs to be able to reasonably run them in one block. How can we appropriately block a fractional factorial design?\nThe typical solution is to choose a high-order effect and alias it with the batches. This divides the runs into two blocks, those where the effect is + and those where it is -.\nEnsure that the alias group doesn’t contain any low-order effects of interest (e.g., we don’t want the blocks to be confounded with any main effects)\nWe effectively sacrifice the alias group that is confounded with the block.\nExample: For our \\(2 ^{6 - 2}\\) design above, we can confound \\(ACD\\) with the block, to avoid aliasing any main effects with the block effect. This let us divide our runs into two blocks of size 8.\nA\nB\nC\nD\nE\nF\nBlock = ACD\n-\n-\n-\n-\n-\n-\n- (B1)\n+\n-\n-\n-\n+\n-\n+ (B2)\n-\n+\n-\n-\n+\n+\n- (B1)\n-\n-\n+\n-\n+\n+\n+ (B2)\n-\n-\n-\n+\n-\n+\n+ (B2)\n+\n+\n-\n-\n-\n+\n+ (B2)\n+\n-\n+\n-\n-\n+\n- (B1)\n+\n-\n-\n+\n+\n+\n- (B1)\n-\n+\n+\n-\n-\n-\n+ (B2)\n-\n+\n-\n+\n+\n-\n+ (B2)\n-\n-\n+\n+\n+\n-\n- (B1)\n+\n+\n+\n-\n+\n-\n- (B1)\n+\n+\n-\n+\n-\n-\n- (B1)\n+\n-\n+\n+\n-\n-\n+ (B2)\n-\n+\n+\n+\n-\n+\n- (B1)\n+\n+\n+\n+\n+\n+\n+ (B2)\n\n\n\n",
    "preview": "posts/2021-08-17-week11-2/week11-2_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-09-09T11:28:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week11-4/",
    "title": "Saturated Designs",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-11-23",
    "categories": [],
    "contents": "\nReadings 8.6, 8.7, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(EBImage)\nlibrary(reshape2)\nlibrary(ggplot2)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\nThere are designs that let you study \\(K\\) factors using only \\(N + 1\\) samples; such designs are called saturated. This is an extremely efficient use of samples.\nThe reason we don’t use saturated designs all the time are that (1) they are not available for all choices of \\(K\\) and (2) the resulting aliasing structure can make definitive inferences difficult. That said, it’s worth being familiar with a few saturated designs, since it can result in dramatically reduced sampling effort in some special cases.\nResolution III Designs\nWhen \\(K + 1\\) is a power of 2, then there are resolution III designs that use only \\(N = K + 1\\) samples.\nExamples\n\\(2^{3 - 1}_{III}\\): 3 factors in 4 samples\nGenerators: \\(C = AB\\)\n\nA\nB\nC\n-\n-\n+\n-\n+\n-\n+\n-\n-\n+\n+\n+\n\\(2^{7 - 4}_{III}\\): 7 factors in 8 samples\nGenerators: \\(D = AB, E = AC, F = BC, G = ABC\\)\nSee Table VIII(h) for alias groups.\n\n\\(2^{15 - 11}_{III}\\): 15 factors in 16 samples\nSee Table VII(h) for generators and alias groups (there are many)\n\nExercise: Give another example (e.g., for 31 factors?)\nPlackett-Burman Designs\nPlackett-Burman designs are a collection of design options working outside the usual fractional factorial paradigm — they don’t rely on the ideas of generators or defining relations that we’ve been using so far to subset full factorial designs into less-costly experiments.\nWe won’t describe their construction, which relies on techniques from abstract algebra. The properties that are most important to know are\nA saturated design can be found whenever \\(K + 1\\) is a multiple of 4, which is much more often than in the resolution III situation above\nTheir aliasing structure is typically more complicated. For example, when \\(K = 11\\), then every main effect is aliased with every pairwise interaction not including that main effect\nTheir projectivity properties are typically good.\nE.g., resolution III Plackett-Burman has projectivity 3, while any \\(2^{k -  p}_{III}\\) fractional factorial has only projectivity 2\nSo, we can collapse to any subset of 3 factors and get a full factorial\n\nTo use these designs, you have to look up the tables of +/-’s, saying which factors to keep on or off for each run. See e.g., Table 8.23/8.24. Alternatively, use the FrF2 R package.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-09T11:28:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week10-3/",
    "title": "Addition of Center Points",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-11-11",
    "categories": [],
    "contents": "\nReadings 6.8, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(EBImage)\nlibrary(ggplot2)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/n751tnavv2xgg50cj3z535t3s3dsbel0.png\"))\n\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/g6ge54chmmlv1guftm15fz4o4j8dz7ek.png\"))\n\n\n\n\nFigure 1: The main lesson from section 6.7\n\n\n\nIn section 6.7, we said that if we wanted an optimal design, we should place all our test points at the boundaries of factor’s values. This is certainly true when you have a linear factor effect. But what if the linearity assumption doesn’t hold? Which design should we pick?\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/6s1qw3mjzok5zrm7l384undexkb2ga2u.png\"))\n\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/z0rpppgsnte77f3vsl5dsrbvtyrtqne0.png\"))\n\n\n\n\nFigure 2: What if our effect is nonlinear?\n\n\n\nAt least in the first design, we would know that something is off! We could then include polynomial terms to fit curvature in the factor effect. This motivates the addition of center points.\nTesting\nIt’s possible to formally test whether there is significant quadratic curvature in the factor effect. We can add quadratic terms to our model and perform ANOVA on the associated term. Alternatively, we can look at \\[\\begin{align}\nSS_{\\text{curvature}} &= \\frac{n_{F}n_{C}\\left(\\bar{y}_{F} - \\bar{y}_{C}\\right)^2}{n_{F} + n_{C}}\n\\end{align}\\]\nwhere \\(\\bar{y}_{C}\\) is the average of the \\(n_{C}\\) samples at the center point and \\(\\bar{y}_{F}\\) is the average over all other points. A large value suggests that linearity is not plausible, and it turns out that it can be formally used in a \\(t\\)-test, but we will not develop that point further, since it’s only in the supplemental material for the chapter.\nCentral Composite Design\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/53o5ytugsbmpgvxcwmfy6053dfvrh3kv.png\"))\n\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/gz504o59rs4212x3v8mip87wbglq4xmw.png\"))\n\n\n\n\nFigure 3: Added center points in the \\(K = 1\\) and \\(K = 2\\) cases.\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/w03xrmafmle36svbr4h3mpjrcjprgef7.png\"))\n\n\n\n\nFigure 4: Geometry of the curvature test statistic.\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/ini2lw3vb7i89fga7ph3ywkygszaqgj6.png\"))\n\n\n\n\nFigure 5: A central composite design.\n\n\n\nIf we have an unreplicated design, then we cannot fit quadratic terms — we would have more parameters than samples, and the linear system would be underdetermined.\nA fix is to require samples at axial points.\nThis design is called a central composite design.\n\n\n\n",
    "preview": "posts/2021-08-17-week10-3/week10-3_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-09-09T11:27:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week10-4/",
    "title": "$2^{K - 1}$ Fractional Factorial Designs",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-11-11",
    "categories": [],
    "contents": "\nReadings 8.1 - 8.2, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(EBImage)\nlibrary(ggplot2)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\n# helper functions\ncode <- function(x) ifelse(x == '-', -1, 1)\ndaniel_plot <- function(effects, probs = c(0.4, 0.6)) { \n  qq <- qqnorm(effects, datax = TRUE)\n  qqline(effects, col = \"red\", probs = probs, datax = TRUE)\n  text(qq$x, qq$y, names(effects), pos=1)\n}\n\n\n\nThe trouble with \\(2^{K}\\) designs is that the number of samples required grows exponentially with the number of factors \\(K\\). The differences between 4 and 7 factors doesn’t sound like a lot, but it’s the difference between 16 and 128 experimental runs. Fractional factorial designs show how to study extra factors without having to make as many runs. There is no free lunch, but the tradeoffs that it creates are often worth it.\nPrinciples\nThere are two central ideas,\nSparsity of Effects: It’s often safe to assume that high-order interactions are rare.\nHeredity: It is unusual for an interaction to be significant without the corresponding main effects also being significant. For example, it would be unusual to have \\(A \\times B\\) be significant without at least one of \\(A\\) and \\(B\\) being significant.\nInitial Exploration\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/gu87vet64i970eonsbvtds7bsb31qbuy.png\"))\n\n\n\n\nFigure 1: A bad choice of samples to subset down to, which would make it impossible to estimate effects from \\(C\\).\n\n\n\nSuppose we want to study \\(A, B\\) and \\(C\\), but we only have a budget of 4 runs. We need to choose four corners of the cube to sample. Some choices are obviously bad. E.g., if we chose all the samples on one face, we wouldn’t see any variation in one factor. Here’s an idea that will lead us on the right path. Recall our old notation,\nlabel\nA\nB\nAB\n(1)\n-\n-\n+\na\n+\n-\n-\nb\n-\n+\n-\nab\n+\n+\n+\nActing on the sparsity of effects principle, let’s suppose that AB is null and invent a contrast for C that exactly matches AB. We’ll update the label to match the associated corner of the cube.\nlabel\nA\nB\nAB\nC\n(c)\n-\n-\n+\n+\na\n+\n-\n-\n-\nb\n-\n+\n-\n-\nabc\n+\n+\n+\n+\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/mmj9mcixl4y0o95hwf7h93vayw0eooc0.png\"))\n\n\n\n\nFigure 2: A more sensible fraction of samples to take.\n\n\n\nThis seems like a reasonable choice of 4 corners, making it possible to estimate at least all the main effects. This turns out to be a general strategy for subsetting the full design. But before we do another example, we need to define some vocabulary.\nVocabulary\n\\(2^{K - 1}\\): This denotes a fractional factorial design where we take \\(\\frac{1}{2}\\) of the samples in a \\(2^{K}\\) design.\nAliases: Above, we assumed that AB was null. Suppose that it weren’t, though. Notice that the contrasts used for the pairs below are all equal, \\[\\begin{align*}\n   \\left[AB\\right] &= \\frac{1}{2}\\left(c - a + b + abc\\right) = \\left[C\\right] \\\\\n      \\left[BC\\right] &= \\frac{1}{2}\\left(-c + a - b + abc\\right) = \\left[A\\right] \\\\\n      \\left[AC\\right] &= \\frac{1}{2}\\left(-c - a + b + abc\\right) = \\left[B\\right]\n\\end{align*}\\]\nand we have no conclusive way of distinguishing between these aliased effects, besides appeals to the hereditary principle or domain knowledge. To denote this unidentifiability, we will use bracket notation, for example, \\[\\begin{align*}\n\\left[A\\right] &= A + BC.\n\\end{align*}\\]\nGenerators: In the example, we set \\(C = AB\\). Multiplying both sides by \\(C\\) and using the fact that \\(C^2 = I\\), we find \\(ABC = I\\). The word ABC will be called the generator for this fraction.\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/pffnjyav7wexbujzf32vlhorcfxhfa6w.png\"))\n\n\n\n\nFigure 3: An example complementary fraction.\n\n\n\nComplementary designs: The complementary fraction of a fractional factorial design are the corners on which we didn’t take samples. Often, if there are strong aliased effects in an initial fractional design, the complementary fraction will be run in the next experiment.\nResolution: A fractional design has resolution \\(R\\) if no \\(p\\)-factor is aliased with an effect containing less than \\(R - p\\) factors.\n\\(R\\)\n\\(p\\)\n\\(< R - p\\)\nInterpretation\n3\n1\n\\(\\leq 2\\)\nMain effects aren’t aliased with other main effects, but could be aliased with two-way interactions\n4\n1\n\\(\\leq 3\\)\nMain effects aren’t aliased with any other main effects or with any two-way interactions, but could be aliased with three-way interactions\n\n2\n\\(\\leq 2\\)\nTwo-way interactions aren’t aliased with main effects.\n5\n1\n\\(\\leq 4\\)\nMain effects aren’t aliased with other main effects, two-way, or three way interactions.\n\n2\n\\(\\leq 3\\)\nTwo-way interactions aren’t aliased with with main effects or two-way interactions.\nProjection: In a fractional factorial, if we decided that we don’t care about a factor after all, we automatically end up with a \\(2 ^ {K - 1}\\) full-factorial for free.\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/b94hfeyugeun1xywlm2ccls9am5l38xg.jpg\"))\n\n\n\n\nFigure 4: Three projections of \\(2^3\\) designs into \\(2^2\\) designs, from the textbook.\n\n\n\n\nSequences: If we find a strong contrast for an aliased effect, it’s natural to try a follow-up experiment to resolve the ambiguity.\nCode Example\nLet’s suppose we only had only run half of the runs in the filtration example from week 9 [1]. The original experiment is a \\(2^ 4\\) design; we will choose the fraction corresponding to the word1 \\(I = ABCD\\)\nThis is a general principle for \\(2 ^ {K - 1}\\) designs: choose which configurations to run by defining the relation \\(K = A B C \\dots J\\). This turns a full \\(2^{K - 1}\\) factorial design, which only studied \\(K - 1\\) factors into a fractional \\(2^{K - 1}\\) factorial that studies \\(K\\) factors.\n\n\n\nWe can perform the fit. Note that there missing values in the estimated coefficients because some effects are aliased.\n\n\ndim(filtration)\n\n\n[1] 8 5\n\nfit <- lm(Rate ~ A * B * C * D, data = filtration)\nsummary(fit)\n\n\n\nCall:\nlm(formula = Rate ~ A * B * C * D, data = filtration)\n\nResiduals:\nALL 8 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (8 not defined because of singularities)\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)    70.75        NaN     NaN      NaN\nA               9.50        NaN     NaN      NaN\nB               0.75        NaN     NaN      NaN\nC               7.00        NaN     NaN      NaN\nD               8.25        NaN     NaN      NaN\nA:B            -0.50        NaN     NaN      NaN\nA:C            -9.25        NaN     NaN      NaN\nB:C             9.50        NaN     NaN      NaN\nA:D               NA         NA      NA       NA\nB:D               NA         NA      NA       NA\nC:D               NA         NA      NA       NA\nA:B:C             NA         NA      NA       NA\nA:B:D             NA         NA      NA       NA\nA:C:D             NA         NA      NA       NA\nB:C:D             NA         NA      NA       NA\nA:B:C:D           NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 7 and 0 DF,  p-value: NA\n\nTo back out which terms are aliased, let’s look carefully at the design matrix.\n\n\nX <- model.matrix(fit)\nt(X) %*% X != 0 # TRUE on off diagonals are aliases\n\n\n            (Intercept)     A     B     C     D   A:B   A:C   B:C\n(Intercept)        TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\nA                 FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\nB                 FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\nC                 FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\nD                 FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\nA:B               FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\nA:C               FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\nB:C               FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\nA:D               FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\nB:D               FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\nC:D               FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\nA:B:C             FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\nA:B:D             FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\nA:C:D             FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\nB:C:D             FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\nA:B:C:D            TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n              A:D   B:D   C:D A:B:C A:B:D A:C:D B:C:D A:B:C:D\n(Intercept) FALSE FALSE FALSE FALSE FALSE FALSE FALSE    TRUE\nA           FALSE FALSE FALSE FALSE FALSE FALSE  TRUE   FALSE\nB           FALSE FALSE FALSE FALSE FALSE  TRUE FALSE   FALSE\nC           FALSE FALSE FALSE FALSE  TRUE FALSE FALSE   FALSE\nD           FALSE FALSE FALSE  TRUE FALSE FALSE FALSE   FALSE\nA:B         FALSE FALSE  TRUE FALSE FALSE FALSE FALSE   FALSE\nA:C         FALSE  TRUE FALSE FALSE FALSE FALSE FALSE   FALSE\nB:C          TRUE FALSE FALSE FALSE FALSE FALSE FALSE   FALSE\nA:D          TRUE FALSE FALSE FALSE FALSE FALSE FALSE   FALSE\nB:D         FALSE  TRUE FALSE FALSE FALSE FALSE FALSE   FALSE\nC:D         FALSE FALSE  TRUE FALSE FALSE FALSE FALSE   FALSE\nA:B:C       FALSE FALSE FALSE  TRUE FALSE FALSE FALSE   FALSE\nA:B:D       FALSE FALSE FALSE FALSE  TRUE FALSE FALSE   FALSE\nA:C:D       FALSE FALSE FALSE FALSE FALSE  TRUE FALSE   FALSE\nB:C:D       FALSE FALSE FALSE FALSE FALSE FALSE  TRUE   FALSE\nA:B:C:D     FALSE FALSE FALSE FALSE FALSE FALSE FALSE    TRUE\n\nSo, revisiting the estimated fit, we can conclude that. \\[\\begin{align*}\n\\widehat{\\mu}+\\widehat{A B C D} &=70.75 \\\\\n\\widehat{A}+\\widehat{B C D} &=9.5 \\times 2=19 \\\\\n\\widehat{B}+\\widehat{A C D} &=0.75 \\times 2=1.5 \\\\\n\\widehat{C}+\\widehat{A B D} &=7 \\times 2=14 \\\\\n\\widehat{D}+\\widehat{A B C} &=8.25 \\times 2=16.5 \\\\\n\\widehat{A B}+\\widehat{C D} &=-0.5 \\times 2=-1 \\\\\n\\widehat{A C}+\\widehat{B D}=&-9.25 \\times 2=-18.5 \\\\\n\\widehat{B C}+\\widehat{A D} &=9.5 \\times 2=19\n\\end{align*}\\]\nLet’s see which of the effects are important, and see whether we can draw any reasonable conclusions, based on the hereditary principle.\n\n\n\nThe important effects look like,\n\\(A + BCD\\)\n\\(BC + AD\\)\n\\(D + ABC\\)\n\\(C + ABD\\)\n\\(AC + BD\\)\nIt seems like we should believe in the main effects \\(A, D\\), and \\(C\\). For the two-way interactions, \\(AC\\) and \\(AD\\) are more plausible, because the main effect for \\(B\\) was not significant.\n\nThe word \\(I = ABCD\\) emerged from defining \\(D = ABC\\).↩︎\n",
    "preview": "posts/2021-08-17-week10-4/week10-4_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-09-09T11:28:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week10-1/",
    "title": "$2^K$ Designs and Regression",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-11-09",
    "categories": [],
    "contents": "\nReadings 6.7, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(EBImage)\nlibrary(ggplot2)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\nIn all the code examples, we’ve been fitting \\(2^K\\) designs using the lm function. Why does this work? In these notes, we’ll see how our effect estimates can be viewed through the lens of linear regression.\nThe Design Matrix\nLet’s write our contrasts summary table linear algebraically. Remember our notation,\nA\nB\nC\nAB\nlabel\n-\n-\n-\n+\n(1)\n+\n-\n-\n-\na\n-\n+\n-\n-\nb\n-\n-\n+\n+\nc\n+\n+\n-\n+\nab\n+\n-\n+\n-\nac\n-\n+\n+\n-\nbc\n+\n+\n+\n+\nabc\nWe’ll translate this into\n\\[\\begin{align*}\nX &= \\begin{pmatrix}\n1 & -1 & -1 & - 1 & 1 \\\\\n1 & 1 & -1 & -1 & -1 \\\\\n1 & -1 & 1 & -1 & -1 \\\\\n1 & -1 & -1 & 1 & 1 \\\\\n1 & 1 & 1 & -1 & 1 \\\\\n1 & 1 & -1 & 1 & -1 \\\\\n1 & -1 & 1 & 1 & -1 \\\\\n1 & 1 & 1 & 1 & 1\n\\end{pmatrix} \\\\\n\\end{align*}\\] \\[\\begin{align*}\ny &= \\begin{pmatrix}\n(1) \\\\\na \\\\\nb \\\\\nc \\\\\nab \\\\\nac \\\\\nbc \\\\\nabc\n\\end{pmatrix}\n\\end{align*}\\]\nFor \\(X\\), we’ve just added an intercept column of \\(1\\)’s and concatenated it with the contrasts written as \\(\\pm 1\\)’s. As before, \\((1)\\) means the value of the sample taken at that corner of the cube, \\(a\\) means the sample at the \\(a\\) corner, etc.\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/8euuywv5ay4x7przznhsxdo269865h2a.png\"))\n\n\n\n\nLet’s observe some properties of this matrix,\nIt has \\(2^{K}\\) rows, one per replicate on each corner of the cube.\nIt has \\(2^{K}\\) columns. To see why, notice that there are\n1 intercept, \\(K\\) main effects, and \\({K \\choose 2}\\) two-way interactions, which adds up to \\(2 ^ K\\).\nMoreover, if our \\(K > 2\\), we’d have \\({K \\choose 3}\\) three-way interactions, etc. and that, by the binomial theorem, \\(\\left(1 + 1\\right)^{K} = \\sum_{j \\leq  K} {K \\choose j}1^{j}1^{K - j}\\)\n\nThe columns are orthogonal. Their norms are all \\(2^{K}\\). Hence \\(X^{T}X = 2^{K}I_{2^{K}}\\).\n\\(\\hat{\\beta}\\) and Effect Estimates\nIn linear regression, the least squares solution \\(\\hat{\\beta}\\) is the vector that optimizes\n\\[\\begin{align*}\n\\hat{beta} := \\arg\\min_{\\beta \\in \\mathbb{R}^{2^{K}}} \\|y - X\\beta\\|_{2}^{2}\n\\end{align*}\\]\nTo minimize a quadratic like this, we can differentiate and use the chain rule,\n\\[\\begin{align*}\n\\frac{\\partial}{\\partial \\beta}\\left[\\|y - X\\beta\\|_{2}^{2}\\right] &= 0 \\\\\n\\iff 2X^{T}\\left(y - X\\beta\\right) &= 0\n\\end{align*}\\]\nwhich implies that \\(\\hat{\\beta} = \\left(X^{T}X\\right)^{-1}X^{T}y\\). However, by the observations above, this means,\n\\[\\begin{align*}\n\\hat{\\beta} &= \\left(2^{K}I_{2^{K}}\\right)^{-1}X^T y \\\\\n&= \\frac{1}{2^{K}}X^{T}y.\n\\end{align*}\\]\nBut \\(X^{T}y\\) are exactly our contrasts (!),\n\\[\\begin{align*}\nX^{T}y &= \\begin{pmatrix}\n1 &  1 &  1 &  1 & 1 & 1 &  1 & 1 \\\\\n-1 & 1 & -1 & -1 & 1 & 1 & -1 & 1 \\\\\n-1 & -1 & 1 & -1 & 1 & -1 & 1 & 1 \\\\\n\\vdots & & & & & & & \\vdots\n\\end{pmatrix}\n\\begin{pmatrix}\n\\left(1\\right) \\\\\na \\\\\nb \\\\\nc \\\\\n\\vdots \n\\end{pmatrix}\n\\end{align*}\\]\ne.g., \\(A = \\frac{1}{2^{K - 1}}\\left[-(1) + a - b -c + ab + ac - bc + abc\\right]\\). The only difference between \\(\\hat{\\beta}\\) and our effect estimates is a factor of 2 in the scaling. Therefore, to estimate the effects in a \\(2^{K}\\) design, it’s enough to construct the \\(X, y\\) matrices above and plug them into standard linear regression programs.\nCode Example\nTo complete this discussion, let’s revisit the \\(2 ^ 4\\) design in the drill example. First, let’s verify that the \\(X\\) matrix used by lm is the same as the one in our conceptual discussion.\n\n\ncode <- function(x) ifelse(x == '-', -1, 1)\ndrill <- read_csv(\"https://uwmadison.box.com/shared/static/7l8bpcu36a12a8c0chlh4id0qezdnnoe.csv\") %>%\n  mutate_at(vars(-rate), code)\nfit <- lm(rate ~ A * B * C * D, drill)\nX <- model.matrix(fit)\nX\n\n\n   (Intercept)  A  B  C  D A:B A:C B:C A:D B:D C:D A:B:C A:B:D A:C:D\n1            1 -1 -1 -1 -1   1   1   1   1   1   1    -1    -1    -1\n2            1  1 -1 -1 -1  -1  -1   1  -1   1   1     1     1     1\n3            1 -1  1 -1 -1  -1   1  -1   1  -1   1     1     1    -1\n4            1 -1 -1  1 -1   1  -1  -1   1   1  -1     1    -1     1\n5            1 -1 -1 -1  1   1   1   1  -1  -1  -1    -1     1     1\n6            1  1  1 -1 -1   1  -1  -1  -1  -1   1    -1    -1     1\n7            1  1 -1  1 -1  -1   1  -1  -1   1  -1    -1     1    -1\n8            1  1 -1 -1  1  -1  -1   1   1  -1  -1     1    -1    -1\n9            1 -1  1  1 -1  -1  -1   1   1  -1  -1    -1     1     1\n10           1 -1  1 -1  1  -1   1  -1  -1   1  -1     1    -1     1\n11           1 -1 -1  1  1   1  -1  -1  -1  -1   1     1     1    -1\n12           1  1  1  1 -1   1   1   1  -1  -1  -1     1    -1    -1\n13           1  1 -1  1  1  -1   1  -1   1  -1   1    -1    -1     1\n14           1  1  1 -1  1   1  -1  -1   1   1  -1    -1     1    -1\n15           1 -1  1  1  1  -1  -1   1  -1   1   1    -1    -1    -1\n16           1  1  1  1  1   1   1   1   1   1   1     1     1     1\n   B:C:D A:B:C:D\n1     -1       1\n2     -1      -1\n3      1      -1\n4      1      -1\n5      1      -1\n6      1       1\n7      1       1\n8      1       1\n9     -1       1\n10    -1       1\n11    -1       1\n12    -1      -1\n13    -1      -1\n14    -1      -1\n15     1      -1\n16     1       1\nattr(,\"assign\")\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\nWe can also check the dimension and orthogonality of this matrix.\n\n\ndim(X)\n\n\n[1] 16 16\n\nt(X) %*% X\n\n\n            (Intercept)  A  B  C  D A:B A:C B:C A:D B:D C:D A:B:C\n(Intercept)          16  0  0  0  0   0   0   0   0   0   0     0\nA                     0 16  0  0  0   0   0   0   0   0   0     0\nB                     0  0 16  0  0   0   0   0   0   0   0     0\nC                     0  0  0 16  0   0   0   0   0   0   0     0\nD                     0  0  0  0 16   0   0   0   0   0   0     0\nA:B                   0  0  0  0  0  16   0   0   0   0   0     0\nA:C                   0  0  0  0  0   0  16   0   0   0   0     0\nB:C                   0  0  0  0  0   0   0  16   0   0   0     0\nA:D                   0  0  0  0  0   0   0   0  16   0   0     0\nB:D                   0  0  0  0  0   0   0   0   0  16   0     0\nC:D                   0  0  0  0  0   0   0   0   0   0  16     0\nA:B:C                 0  0  0  0  0   0   0   0   0   0   0    16\nA:B:D                 0  0  0  0  0   0   0   0   0   0   0     0\nA:C:D                 0  0  0  0  0   0   0   0   0   0   0     0\nB:C:D                 0  0  0  0  0   0   0   0   0   0   0     0\nA:B:C:D               0  0  0  0  0   0   0   0   0   0   0     0\n            A:B:D A:C:D B:C:D A:B:C:D\n(Intercept)     0     0     0       0\nA               0     0     0       0\nB               0     0     0       0\nC               0     0     0       0\nD               0     0     0       0\nA:B             0     0     0       0\nA:C             0     0     0       0\nB:C             0     0     0       0\nA:D             0     0     0       0\nB:D             0     0     0       0\nC:D             0     0     0       0\nA:B:C           0     0     0       0\nA:B:D          16     0     0       0\nA:C:D           0    16     0       0\nB:C:D           0     0    16       0\nA:B:C:D         0     0     0      16\n\nLet’s make sure that the formula we derived above is exactly what lm is doing.\n\n\n(1 / 16) * t(X) %*% drill$rate\n\n\n               [,1]\n(Intercept) 6.15250\nA           0.45875\nB           3.21875\nC           1.64625\nD           1.14500\nA:B         0.29500\nA:C         0.07750\nB:C         0.75500\nA:D         0.41875\nB:D         0.79625\nC:D         0.22375\nA:B:C       0.08125\nA:B:D       0.38000\nA:C:D       0.29250\nB:C:D       0.08750\nA:B:C:D     0.27125\n\ncoef(fit) # hand computation agrees\n\n\n(Intercept)           A           B           C           D \n    6.15250     0.45875     3.21875     1.64625     1.14500 \n        A:B         A:C         B:C         A:D         B:D \n    0.29500     0.07750     0.75500     0.41875     0.79625 \n        C:D       A:B:C       A:B:D       A:C:D       B:C:D \n    0.22375     0.08125     0.38000     0.29250     0.08750 \n    A:B:C:D \n    0.27125 \n\nFinally, let’s compare the fitted \\(\\hat{\\beta}\\) with our original effect estimates (at least, for the effect \\(A\\)).\n\n\ndrill$A\n\n\n [1] -1  1 -1 -1 -1  1  1  1 -1 -1 -1  1  1  1 -1  1\n\nest_A <- (1 / 8) * sum(drill[drill$A == 1, ]$rate - drill[drill$A == -1, ]$rate)\nest_A / 2\n\n\n[1] 0.45875\n\n\n\n\n",
    "preview": "posts/2021-08-17-week10-1/week10-1_files/figure-html5/unnamed-chunk-2-1.svg",
    "last_modified": "2021-10-04T16:33:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week10-2/",
    "title": "$2^K$ Designs are Optimal",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-11-09",
    "categories": [],
    "contents": "\nReadings 6.7, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(readr)\nlibrary(EBImage)\nlibrary(ggplot2)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\nf <- function(x, beta=c(-1, 3)) {\n  cbind(1, x) %*% matrix(beta, ncol = 1)\n}\n\nsamples <- function(f, x, sigma2=1) {\n f(x) + rnorm(length(x), 0, sigma2)\n}\n\nsimulate <- function(x, f, n_runs=1000) {\n  sim_data <- vector(length = n_runs, mode = \"list\")\n  for (i in seq_len(n_runs)) {\n    y <- samples(f, x)\n    fit <- lm(y ~ x)\n    sim_data[[i]] <- tibble(\n      rep = i,\n      x = x,\n      y = y,\n      intercept = coef(fit)[1],\n      slope = coef(fit)[2]\n    )\n  }\n  \n  bind_rows(sim_data)\n}\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/9qhkn4dnuqoou7dm02zgkexze5mlaz6z.png\"))\n\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/ywt9gxdakwz558bl6j57y1v25uncjjil.png\"))\n\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/mpi36bkvrhpuf3ls66jxkqy4mvqoqzfm.png\"))\n\n\n\n\nFigure 1: Three candidate designs when studying 2 factors, all using 4 samples. The \\(2^2\\) design is at the bottom.\n\n\n\nThis seems like a bold claim, before we can justify it, we need a sense of (1) the setting, i.e., what are alternative designs are up for consideration and (2) the criteria that will be used to call one design better than an another.\nSetting: We have a fixed budget of \\(n\\) samples. Without loss of generality, the factors take their values within the interval \\(\\left[-1, 1\\right]\\). A candidate design is any way of gathering \\(n\\) samples from among all the possible settings of the \\(K\\) factors.\nCriteria: We want to estimate the effects « as well as possible. » We also want to make good predictions at new factor combinations. We’ll have to make these notions precise.\nWe won’t give formal proofs of optimality (it’s beyond the scope of our book). But we will use simulations to get a sense of the key phenomena at work here, and we’ll create names (like \\(D-\\), \\(G-\\), and \\(I\\)-optimality) to describe what we see.\nSimulation\nFor simplicity, let’s consider \\(K = 1\\).\nWe know that we can use linear regression to estimate factor effects.\nSuppose that the underlying function is a true linear regression\nSuppose our budget is \\(n = 4\\).\nHow should we distributed our four points in order to achieve a good estimate of the underlying linear regression?\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/zm5ork632pktbbryzvs4ex6fbvd78z72.png\"))\n\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/48czl1on1krut0gquz5arqdb5a81ft40.png\"))\n\n\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/0s7m67r9k447irxr8jdzmta8h0ea2seg.png\"))\n\n\n\n\nFigure 2: Three candidate designs explored in our \\(K = 1\\) simulation.\n\n\n\nHere are three candidates,\nNear the origin: Place two points each at -0.1 and 0.1.\nEquispaced: Place the four points equally spaced between -1 and 1\nBoundaries: Place 2 points at -1 and 2 at 1\n\n\nN <- 10\nx <- c(rep(-0.1, 2), rep(0.1, 2))\nsim_data <- list()\nsim_data[[\"close\"]] <- simulate(x, f)\nx <- seq(-1, 1, length.out = 4)\nsim_data[[\"equi\"]] <- simulate(x, f)\nx <- c(rep(-1, 2), rep(1, 2))\nsim_data[[\"2k\"]] <- simulate(x, f)\nsim_df <- bind_rows(sim_data, .id = \"design\")\n\n\n\n\n\np <- ggplot(sim_df %>% filter(rep < 100)) +\n  geom_point(aes(x = x, y = y)) +\n  geom_abline(slope = 3, intercept = -1, col = \"red\") +\n  geom_abline(aes(slope = slope, intercept = intercept, color = design)) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_grid(design ~ .) +\n  transition_time(rep)\nanimate(p, fps=10)\n\n\n\n\nTo illustrate this idea, consider the animation here. Each frame is a random run from the simulation.\nThe red line is the true regression function.\nThe three panels correspond to the different placements of \\(x\\).\nThe black dots are random \\(y\\)’s that you observe when you sample at \\(x\\)\nThe orange, green, and blue lines are the regression fits corresponding to those \\(\\left(x, y\\right)\\) pairs\nAfter running this 1000 times, we get the figure to the right. We can alternatively make a histogram of the estimated slopes.\n\n\nggplot(sim_df) +\n  geom_point(aes(x = x, y = y), alpha = 0.05) +\n  geom_abline(aes(slope = slope, intercept = intercept, color = design), alpha = 0.1) +\n  geom_abline(slope = 3, intercept = -1, col = \"red\") +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_grid(design ~ .)\n\n\n\nggplot(sim_df) +\n  geom_histogram(\n    aes(x = slope, fill = design), \n    position = \"identity\", bins = 100, alpha = 0.6\n  ) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nObservations\nWhen we choose x’s close to the origin, the estimates are highly variable\nThe \\(2^{K}\\) approach, which places all samples at the extremes of the factor seems best, in two senses,\nNarrowest band of fitted regression lines\nNarrowest histogram of estimated slopes (around the truth)\n\nDefinitions\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/6xoe5onf01gdpaef6g0qtcstbhb9w72n.png\"))\n\n\n\n\nFigure 3: Summary of alternative optimality definitions.\n\n\n\nA design is \\(D\\)-optimal if \\(\\left|\\text{Cov}\\left(\\hat{\\beta}\\right)\\right|\\) is minimized.\nIn our picture, if the width of the histogram of \\(\\hat{\\beta}\\) is minimized\nThe determinant generalizes the notion of « size » to higher-dimensions (specifically, it’s related to volume)\n\nA design is \\(G\\)-optimal if \\(\\max_{x} \\text{Var}\\left(\\hat{y}\\left(x\\right)\\right)\\) is minimized\nIn our picture, if the maximum vertical spread of the prediction band is minimized\n\nA design is \\(V\\)-optimal if \\(\\int_{\\left[-1, 1\\right]^{K}} \\text{Var}\\left(\\hat{y}\\left(x\\right)\\right)dx\\) is minimized\nIn our picture, if the area of the prediction band is minimized\n\nWhile our simulation was only for \\(K = 1\\), it turns out that according to all three criteria, the \\(2^{K}\\)-design is optimal.\n\n\n\n",
    "preview": "posts/2021-08-17-week10-2/week10-2_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-09-09T11:27:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week9-2/",
    "title": "Examples of $2^K$ Designs",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-11-04",
    "categories": [],
    "contents": "\nReadings 6.6, Rmarkdown\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(EBImage)\nlibrary(tidyr)\nlibrary(rsm)\ntheme_set(theme_bw() + theme(legend.position = \"bottom\"))\n\n\n\n\n\n# helper functions\ncode <- function(x) ifelse(x == '-', -1, 1)\ndaniel_plot <- function(effects, probs = c(0.3, 0.7)) { \n  qq <- qqnorm(effects, datax = TRUE)\n  qqline(effects, col = \"red\", probs = probs, datax = TRUE)\n  text(qq$x, qq$y, names(effects), pos=1)\n}\n\n\n\nLike the corresponding section in the book, these notes introduce no new technical material. Instead, they illustrate end-to-end analysis workflows for \\(2^K\\) designs and highlight the types of judgments that need to be exercised in practice.\n\n\ndrill <- read_csv(\"https://uwmadison.box.com/shared/static/7l8bpcu36a12a8c0chlh4id0qezdnnoe.csv\") %>%\n  mutate_at(vars(-rate), code)\nggplot(drill) +\n  geom_point(aes(x = B, y = rate, col = as.factor(C))) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_grid(A ~ D)\n\n\n\n\nExample 6.3\nAn experiment was done to see how the “advance rate of a drill” varied as a function of four factors1, which we will call A, B, C, and D.\nAs a first pass at the analysis, we fit a full \\(2^4\\) model. The Daniel plot is below.\n\n\nfit <- lm(rate ~ A * B * C * D, data = drill)\neffects <- 2 * coef(fit)[-1]\ndaniel_plot(effects, c(0.35, 0.65))\n\n\n\nfit <- lm(rate ~ B * (C + D), data = drill)\ndrill_resid <- drill %>%\n  mutate(\n    residual = resid(fit),\n    y_hat = predict(fit)\n  )\nggplot(drill_resid) +\n  geom_point(aes(x = y_hat, y = residual))\n\n\n\n\nThis suggests dropping factor \\(A\\) in the fit. However, when we study the residuals, we notice they are heteroskedastic, with larger residuals associated with higher predicted values.\nSince the data are rates, we take a log-transform. We refit the full model, which suggests a much simpler set of factors, with no interactions. The residuals of the associated submodel also look much better now.\n\n\nfit <- lm(log(rate) ~ A * B * C * D, data = drill)\ndaniel_plot(2 * coef(fit)[-1])\n\n\n\nfit <- lm(log(rate) ~ B + C + D, data = drill)\ndrill_resid <- drill %>%\n  mutate(\n    residual = resid(fit),\n    y_hat = predict(fit)\n  )\nggplot(drill_resid) +\n  geom_point(aes(x = y_hat, y = residual))\n\n\n\n\nLessons:\nExamining residuals can motivate useful transformations of the data.\nIt’s a good thing replicates were made.\nExample 6.4\nAn experiment was done to see how defect rate in airplane windows varied according to four factors: temperature (A), clamp time (B), resin flow (C), and press closing time (D).\nThe estimated effects are displayed in the margin. The story seems simple,\n\n\nwindows <- read_csv(\"https://uwmadison.box.com/shared/static/62phufkeprheu9gu35mu1e75x6rc2shv.csv\") %>%\n  mutate_at(vars(-defects), code)\nggplot(windows) +\n  geom_point(aes(x = A, y = defects, col = as.factor(C))) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_grid(B ~ D)\n\n\n\n\n\n\nfit <- lm(defects ~ A * B * C * D, data = windows)\ndaniel_plot(2 * coef(fit)[-1])\n\n\n\nfit <- lm(defects ~ A + C, data = windows)\nwindows$residual <- resid(fit)\nggplot(windows) +\n  geom_point(aes(x = B, y = residual))\n\n\n\n\nTemperature (A) has a strong positive effect\nResin (C) flow has a slight negative effect.\nAs usual, we examine residuals. This reveals a kind of heteroskedasticity,\nWe don’t do any transforms, but instead recommend low temperature, high resin flow, and low clamp time (because lower clamp time -> lower variability)\nLessons:\nIn practice, it’s often useful to take variability into account, rather than just average response\nA residual plot can be directly actionable\nAside: Dispersion estimates\nThe heuristic in the previous example can be formalized.\nLet \\(S^2\\left(j^{+}\\right)\\) be an estimated standard deviation of responses when contrast \\(j\\) is active.\nTheory predicts that \\(\\log\\left(\\frac{S^{2}\\left(j^{+}\\right)}{S^{2}\\left(j^{-}\\right)}\\right)\\) will be approximately normal.\nWe call these the dispersions\n\nMotivates looking at normal probability plots of dispersions, to see if any factors have high discrepancies in spread, as a function of level\n\n\nM <- model.matrix(defects ~ A * B * C * D, data = windows)[, -1] # remove intercept\nprint(M)\n\n\n    A  B  C  D A:B A:C B:C A:D B:D C:D A:B:C A:B:D A:C:D B:C:D\n1  -1 -1 -1 -1   1   1   1   1   1   1    -1    -1    -1    -1\n2   1 -1 -1 -1  -1  -1   1  -1   1   1     1     1     1    -1\n3  -1  1 -1 -1  -1   1  -1   1  -1   1     1     1    -1     1\n4  -1 -1  1 -1   1  -1  -1   1   1  -1     1    -1     1     1\n5  -1 -1 -1  1   1   1   1  -1  -1  -1    -1     1     1     1\n6   1  1 -1 -1   1  -1  -1  -1  -1   1    -1    -1     1     1\n7   1 -1  1 -1  -1   1  -1  -1   1  -1    -1     1    -1     1\n8   1 -1 -1  1  -1  -1   1   1  -1  -1     1    -1    -1     1\n9  -1  1  1 -1  -1  -1   1   1  -1  -1    -1     1     1    -1\n10 -1  1 -1  1  -1   1  -1  -1   1  -1     1    -1     1    -1\n11 -1 -1  1  1   1  -1  -1  -1  -1   1     1     1    -1    -1\n12  1  1  1 -1   1   1   1  -1  -1  -1     1    -1    -1    -1\n13  1 -1  1  1  -1   1  -1   1  -1   1    -1    -1     1    -1\n14  1  1 -1  1   1  -1  -1   1   1  -1    -1     1    -1    -1\n15 -1  1  1  1  -1  -1   1  -1   1   1    -1    -1    -1     1\n16  1  1  1  1   1   1   1   1   1   1     1     1     1     1\n   A:B:C:D\n1        1\n2       -1\n3       -1\n4       -1\n5       -1\n6        1\n7        1\n8        1\n9        1\n10       1\n11       1\n12      -1\n13      -1\n14      -1\n15      -1\n16       1\n\n\n\nS <- list()\nfor (k in seq_len(ncol(M))) {\n  S[[k]] <- data.frame(\n    \"effect\" = colnames(M)[k],\n    \"sd_plus\" = sd(windows$residual[M[, k] == 1]),\n    \"sd_minus\" = sd(windows$residual[M[, k] == -1])\n  )\n}\nS <- do.call(rbind, S)\ns_ratio <- setNames(log(S$sd_plus / S$sd_minus), S$effect)\ndaniel_plot(s_ratio)\n\n\n\n\nExample 6.5\nAn \\(2^{4}\\) experiment is setup to improve semiconductor manufacturing.\nQuestion: How do temperature (A), time (B), pressure (C), and gas flow (D) affect oxide thickness of the wafers?\nFour wafers are put in the furnace at a time\nThese are repeated measures, not replicates!\nTherefore, take the average of the wafers, and treat this as an unreplicated design\n\nAn analysis of the variation in average thickness across factor configurations is displayed below.\n\n\noxide <- read_csv(\"https://uwmadison.box.com/shared/static/vyk6uoe3zbnonv4n6jcusbrocmt4cvru.csv\") %>%\n  pivot_longer(starts_with(\"wafer\"), names_to = \"variable\")\noxide_collapse <- oxide %>%\n  group_by(A, B, C, D) %>%\n  summarise(mean = mean(value), var = var(value))\nggplot(oxide) +\n  geom_point(aes(x = A, y = value, col = as.factor(B))) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_grid(C ~ D)\n\n\n\nfit <- lm(mean ~ A * B * C * D, data = oxide_collapse)\ndaniel_plot(2 * coef(fit)[-1])\n\n\n\nfit <- lm(mean ~ A * (B + C), data = oxide_collapse)\nsummary(fit) # compare with Table 6.20\n\n\n\nCall:\nlm(formula = mean ~ A * (B + C), data = oxide_collapse)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.125 -2.469  1.000  2.250  6.625 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  399.188      1.049 380.475  < 2e-16 ***\nA             21.562      1.049  20.552 1.64e-09 ***\nB              9.063      1.049   8.638 5.98e-06 ***\nC             -5.187      1.049  -4.944 0.000583 ***\nA:B            8.437      1.049   8.042 1.12e-05 ***\nA:C           -5.313      1.049  -5.063 0.000489 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.197 on 10 degrees of freedom\nMultiple R-squared:  0.9839,    Adjusted R-squared:  0.9759 \nF-statistic: 122.3 on 5 and 10 DF,  p-value: 1.237e-08\n\n\n\nimage(fit,  ~ A + B + C)\n\n\n\n\nIn addition to modeling the average across wafers, we can model the standard deviation. Potentially useful if we want to find configurations with more consistency in oxide thickness. The estimated effects for this model are shown below.\n\n\nfit <- lm(var ~ A * B * C * D, data = oxide_collapse)\ndaniel_plot(2 * coef(fit)[-1])\n\n\n\nfit <- lm(var ~ A + B * D, data = oxide_collapse)\n\n\n\nWe can now use the two response surfaces jointly to determine factor combinations that will have a target oxide thickness, and low variability around that.\n\n\nimage(fit, ~ A + B + D)\n\n\n\n\nWarning: What would have happened if we treated the repeated measures as true replicates?\n\n\nfit <- lm(value ~ A * B * C * D, data = oxide)\nsummary(fit)\n\n\n\nCall:\nlm(formula = value ~ A * B * C * D, data = oxide)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n    -6     -1      0      1      8 \n\nCoefficients:\n            Estimate Std. Error  t value Pr(>|t|)    \n(Intercept) 399.1875     0.3094 1290.369  < 2e-16 ***\nA            21.5625     0.3094   69.701  < 2e-16 ***\nB             9.0625     0.3094   29.294  < 2e-16 ***\nC            -5.1875     0.3094  -16.769  < 2e-16 ***\nD            -0.8125     0.3094   -2.626   0.0115 *  \nA:B           8.4375     0.3094   27.274  < 2e-16 ***\nA:C          -5.3125     0.3094  -17.173  < 2e-16 ***\nB:C           1.9375     0.3094    6.263 9.93e-08 ***\nA:D           0.5625     0.3094    1.818   0.0753 .  \nB:D          -1.9375     0.3094   -6.263 9.93e-08 ***\nC:D           0.5625     0.3094    1.818   0.0753 .  \nA:B:C        -0.1875     0.3094   -0.606   0.5473    \nA:B:D         1.4375     0.3094    4.647 2.65e-05 ***\nA:C:D        -0.0625     0.3094   -0.202   0.8407    \nB:C:D        -0.3125     0.3094   -1.010   0.3175    \nA:B:C:D       0.0625     0.3094    0.202   0.8407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.475 on 48 degrees of freedom\nMultiple R-squared:  0.9933,    Adjusted R-squared:  0.9912 \nF-statistic: 476.8 on 15 and 48 DF,  p-value: < 2.2e-16\n\nWe would incorrectly include that many factors are relevant when they aren’t — this happens because our estimate of \\(\\sigma^2\\) is too small. Can lead to lots of wasted effort.\nLessons:\nDon’t treat repeated measures as replicates, or we risk many false positive effects\nIt can be useful to model the variance of the response, rather than simply the mean\n\nThe factors are drill load, flow rate, rotational speed, and drilling mud, in case you’re curious.↩︎\n",
    "preview": "posts/2021-08-17-week9-2/week9-2_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-09-09T11:31:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-17-week9-1/",
    "title": "Unreplicated $2^K$ Designs",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-11-02",
    "categories": [],
    "contents": "\nReadings 6.5, Rmarkdown\n\n\nlibrary(BsMD)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\ntheme_set(theme_bw())\n\n\n\n\n\n# helper functions\ncode <- function(x) ifelse(x == '-', -1, 1)\ndaniel_plot <- function(effects) { \n  qq <- qqnorm(effects, datax = TRUE)\n  qqline(effects, col = \"red\", probs = c(0.3, 0.7), datax = TRUE)\n  text(qq$x, qq$y, names(effects), pos=4)\n}\n\n\n\nSometimes, people will only take only one measurement per factor configuration\nWhen \\(K\\) is large, replication can increase the number of samples needed substantially\nE.g., changing n from 1 to 2 when K = 5 means 32 more runs\n\nFor factor screening experiments, typically want to allocate samples to understanding new factors, rather than replicating known configurations\nWithout replicates to gauge measurement noise, we may encounter two opposite problems,\nMissing a true effect\nSpurious effects\nLet’s see how these problems arise, and discuss some solutions.\nMissing True Effects\nIf the effect is weak, then if only nearby levels are tested, the effect will be easy to miss\nFix: We can space out the levels at which we test each factor\n\n\nlibrary(\"EBImage\")\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/hvedijshgaw7so91kqcl1l8eunml4m9v.png\"))\n\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/olbjp5fubt4mm6j1fa7rthe0uoctymef.pnghttps://uwmadison.box.com/shared/static/8h0vt5kcb5ss0l9dlghsswnpf88q8267.png\"))\n\n\n\n\n\nSpurious Effects\nIf there are no replicates, then we can perfectly interpolate the data\nLeaves us with no degrees-of-freedom for estimating \\(\\sigma^2\\)\n\\(\\sigma^2\\) is needed to perform ANOVA\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/099eopgmbncci9ob9zjo1mvvguxinibn.png\"))\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/rfdz734pv6rxolgwz5fx215qb41qm0k6.png\"))\n\n\n\n\nFix: Pool together higher-order interactions\nHigh-order interactions are typically rare (sparsity of effects principle)\nCan use pooled interaction estimates to obtain \\(\\sigma^2\\)\nPooling can only make testing more conservative\n\n\n\nfiltration <- read_table2(\"https://uwmadison.box.com/shared/static/xxh05ngikmscnddbhg2l3v268jnu4jtc.txt\") %>%\n  mutate_at(vars(-Rate), as.factor)\nggplot(filtration) +\n  geom_point(aes(x = A, y = Rate, col = C)) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_grid(B ~ D)\n\n\n\n\nData Example\nThis is what happens when we try to estimate all interactions, even though there is only one replicate per factor configuration. We’ll use the filtration dataset, which asks how temperature (A), pressure (B), formaldehyde (C), and stirring rate (D) affect the filtration rate of the resulting product.\n\n\nfit <- lm(Rate ~ A * B * C * D, data = filtration)\nsummary(fit)\n\n\n\nCall:\nlm(formula = Rate ~ A * B * C * D, data = filtration)\n\nResiduals:\nALL 16 residuals are 0: no residual degrees of freedom!\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)       45        NaN     NaN      NaN\nA+                26        NaN     NaN      NaN\nB+                 3        NaN     NaN      NaN\nC+                23        NaN     NaN      NaN\nD+                -2        NaN     NaN      NaN\nA+:B+             -9        NaN     NaN      NaN\nA+:C+            -34        NaN     NaN      NaN\nB+:C+              9        NaN     NaN      NaN\nA+:D+             31        NaN     NaN      NaN\nB+:D+             -1        NaN     NaN      NaN\nC+:D+              9        NaN     NaN      NaN\nA+:B+:C+           2        NaN     NaN      NaN\nA+:B+:D+          11        NaN     NaN      NaN\nA+:C+:D+         -12        NaN     NaN      NaN\nB+:C+:D+         -16        NaN     NaN      NaN\nA+:B+:C+:D+       11        NaN     NaN      NaN\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 15 and 0 DF,  p-value: NA\n\nanova(fit)\n\n\nAnalysis of Variance Table\n\nResponse: Rate\n          Df  Sum Sq Mean Sq F value Pr(>F)\nA          1 1870.56 1870.56     NaN    NaN\nB          1   39.06   39.06     NaN    NaN\nC          1  390.06  390.06     NaN    NaN\nD          1  855.56  855.56     NaN    NaN\nA:B        1    0.06    0.06     NaN    NaN\nA:C        1 1314.06 1314.06     NaN    NaN\nB:C        1   22.56   22.56     NaN    NaN\nA:D        1 1105.56 1105.56     NaN    NaN\nB:D        1    0.56    0.56     NaN    NaN\nC:D        1    5.06    5.06     NaN    NaN\nA:B:C      1   14.06   14.06     NaN    NaN\nA:B:D      1   68.06   68.06     NaN    NaN\nA:C:D      1   10.56   10.56     NaN    NaN\nB:C:D      1   27.56   27.56     NaN    NaN\nA:B:C:D    1    7.56    7.56     NaN    NaN\nResiduals  0    0.00     NaN               \n\nIf we instead assume that all interactions terms of order 3 or higher are null, we can perform again perform ANOVA.\n\n\nfit <- lm(Rate ~ (A + B + C + D) ^ 2, data = filtration)\nsummary(fit)\n\n\n\nCall:\nlm(formula = Rate ~ (A + B + C + D)^2, data = filtration)\n\nResiduals:\n      1       2       3       4       5       6       7       8 \n-0.1875  2.8125  1.8125 -4.4375 -3.9375  1.3125  2.3125  0.3125 \n      9      10      11      12      13      14      15      16 \n-1.6875 -0.9375  0.0625  2.5625  5.8125 -3.1875 -4.1875  1.5625 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   45.188      4.192  10.779 0.000119 ***\nA+            23.000      5.056   4.549 0.006117 ** \nB+             1.000      5.056   0.198 0.851001    \nC+            26.750      5.056   5.291 0.003216 ** \nD+            -0.500      5.056  -0.099 0.925065    \nA+:B+          0.250      5.056   0.049 0.962478    \nA+:C+        -36.250      5.056  -7.170 0.000821 ***\nA+:D+         33.250      5.056   6.576 0.001220 ** \nB+:C+          4.750      5.056   0.939 0.390613    \nB+:D+         -0.750      5.056  -0.148 0.887871    \nC+:D+         -2.250      5.056  -0.445 0.674909    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.056 on 5 degrees of freedom\nMultiple R-squared:  0.9777,    Adjusted R-squared:  0.9331 \nF-statistic: 21.92 on 10 and 5 DF,  p-value: 0.001634\n\nanova(fit)\n\n\nAnalysis of Variance Table\n\nResponse: Rate\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nA          1 1870.56 1870.56 73.1760 0.0003596 ***\nB          1   39.06   39.06  1.5281 0.2712969    \nC          1  390.06  390.06 15.2592 0.0113371 *  \nD          1  855.56  855.56 33.4694 0.0021718 ** \nA:B        1    0.06    0.06  0.0024 0.9624777    \nA:C        1 1314.06 1314.06 51.4059 0.0008208 ***\nA:D        1 1105.56 1105.56 43.2494 0.0012200 ** \nB:C        1   22.56   22.56  0.8826 0.3906126    \nB:D        1    0.56    0.56  0.0220 0.8878710    \nC:D        1    5.06    5.06  0.1980 0.6749089    \nResiduals  5  127.81   25.56                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDesign Projections\nRelated to pooling, sometimes it is clear that a certain factor has no bearing on the response\nIn this case, we may consider removing that factor and all the interaction terms that include it\nBy collapsing the factor, we double the number of effective replicates\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/yhc2w1698namew8r0vgn7ej7546ev2wg.png\"))\n\n\n\n\n\nVisualing Effects\nGraphical methods provide useful summaries for evaluating interactions in the \\(2^K\\) model.\nDaniel Plots\nIf none of the factors had any influence on the response, then the effects would all be normally distributed around 0.\nIdea: make a normal probability plot of the effects, and look for those which deviate from identity line. These are likely real effects.\n\n\n\n# estimate effects\nfiltration_coded <- filtration %>%\n  mutate_at(vars(-Rate), code)\nfit_coded <- lm(Rate~A*B*C*D, data=filtration_coded)\neffects <- 2 * coef(fit_coded)[-1] # exclude intercept\ndaniel_plot(effects)\n\n\n\n\nLenth Plots\nAn alternative is to simply plot the effect sizes directly\n\n\n\nLenthPlot(fit_coded, cex.fac = 0.4)\n\n\n\n    alpha       PSE        ME       SME \n 0.050000  2.625000  6.747777 13.698960 \n\nLet \\(s_0 = 1.5 \\times \\text{median}\\left(\\text{Contrast}_j\\right)\\). Then, the notation here refers to\nPseudostandard error (PSE): \\(1.5 \\times \\text{median}\\left(\\left|c_j\\right| : \\left|c_j\\right| < 2.5 s_0\\right)\\) serves as an alternative to the usual standard error over contrasts, which is robust to outliers (it completely ignores \\(c_j\\)’s that are larger than \\(2.5 s_0\\).)\nMargin of error (ME): A version of the critical \\(t\\)-value that relies on the robust standard error. Defined as \\(t_{0.025,\\frac{m}{3}}\\times PSE\\), where \\(m\\) is the total number of effect estimates (columns in the Lenth plot).\nSimultaneous margin of error (SME): A more conservative version of ME, to protect against multiple comparisons.\n\n\n\n",
    "preview": "posts/2021-08-17-week9-1/week9-1_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-09-09T11:30:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-16-week8-3/",
    "title": "$2 ^ 3$ Factorial Design",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-28",
    "categories": [],
    "contents": "\nReadings 6.3 - 6.4, Rmarkdown\n\n\n\nHow can we generalize the \\(2^2\\) analysis so that we can do studies that inspect many factors simultaneously (all at two levels each)? For now, we’ll analyze the 3 factor case (the \\(2^3\\) design), but with an eye out for more general patterns\nFor the \\(2^3\\) design, we have 8 factor configurations\nVisualize as corners of a cube\nCall the third factor \\(C\\).\n\n\n\n\nThe example below reads in a dataset with 3 factors. We can use facet_grid to see effects across all 8 configurations.\n\n\nlibrary(readr)\nlibrary(dplyr)\nplasma <- read.table(\"https://uwmadison.box.com/shared/static/f3sggiltyl5ycw1gu1vq7uv7omp4pjdg.txt\", header = TRUE)\nggplot(plasma) +\n  geom_point(aes(A, Rate)) +\n  facet_grid(B ~ C)\n\n\n\n\nEffect Estimates\nOur table notation can be extended to deal with all 8 corners of the cube.\nA\nB\nC\nlabel\n-\n-\n-\n(1)\n+\n-\n-\na\n-\n+\n-\nb\n-\n-\n+\nc\n+\n+\n-\nab\n+\n-\n+\nac\n-\n+\n+\nbc\n+\n+\n+\nabc\nThe main effect estimates can be made by subtracting the + from the - corners. Equivalently, this can be viewed as the difference in averages,\nwhen the factor is on vs. off\nbetween one face of the cube and its opposite\n\nFor example, to estimate the main effect of \\(A\\), we can use, \\[A = \\frac{1}{2^2 n}\\left[\\left(a + ab + ac + abc\\right) - \\left(\\left(1\\right) + b + c + bc\\right)\\right]\\]\nTo estimate interactions, we compare how the average effects of a variable change when we condition on the value of another variable. For example, for the interaction \\(AB\\), notice that\nB\nAverage A Effect\n+\n\\(\\frac{1}{2^2 n}\\left[\\left(abc - bc\\right) + \\left(ab - b\\right)\\right]\\)\n-\n\\(\\frac{1}{2^2 n}\\left[\\left(ac - c\\right) + \\left(a - \\left(1\\right)\\right)\\right]\\)\nwhich inspires the definition,\n\\[\nAB = \\frac{1}{2^2 n}\\left[abc - bc + ab - b - ac + c - a + \\left(1\\right)\\right]\n\\]\nNotice that the associated contrast can be obtained by multiplying the columns in the table above.\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/wwsniua1ce0q376oy2ffrtzf4ivzup99.png\"))\n\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/hapjw92oqij5oiyddt29nvtq6y1xzxxu.png\"))\n\n\n\n\nA\nB\nC\nAB\nlabel\n-\n-\n-\n+\n(1)\n+\n-\n-\n-\na\n-\n+\n-\n-\nb\n-\n-\n+\n+\nc\n+\n+\n-\n+\nab\n+\n-\n+\n-\nac\n-\n+\n+\n-\nbc\n+\n+\n+\n+\nabc\nWe won’t prove why this works, but you can use it as a device for avoiding having to memorize everything. The three-way interaction is defined as the change in two-way interactions across the two values for the third variable. It’s contrast can be derived also by multiplying the relevant columns from the table above.\n\n\n\n",
    "preview": "posts/2021-08-16-week8-3/week8-3_files/figure-html5/unnamed-chunk-3-1.svg",
    "last_modified": "2021-10-04T16:33:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-16-week8-4/",
    "title": "Interpreting effects in $2 ^ 3$ Designs",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-28",
    "categories": [],
    "contents": "\nReadings 6.3 - 6.4, Rmarkdown\n\n\n\nAs before, the \\(SS\\) terms can be obtained by squaring the contrasts and dividing by the number of data points. This lets us build the associated ANOVA table,\n\n\nlibrary(readr)\nlibrary(dplyr)\nplasma <- read.table(\"https://uwmadison.box.com/shared/static/f3sggiltyl5ycw1gu1vq7uv7omp4pjdg.txt\", header = TRUE) %>%\n  mutate_at(vars(-Rate), as.factor)\n\n\n\n\n\nfit <- lm(Rate ~ A * B * C, data = plasma)\nsummary(aov(fit))\n\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nA            1  41311   41311  18.339 0.002679 ** \nB            1    218     218   0.097 0.763911    \nC            1 374850  374850 166.411 1.23e-06 ***\nA:B          1   2475    2475   1.099 0.325168    \nA:C          1  94403   94403  41.909 0.000193 ***\nB:C          1     18      18   0.008 0.930849    \nA:B:C        1    127     127   0.056 0.818586    \nResiduals    8  18020    2253                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nConsider the regression view of this situation. The full model would be \\[y_{i} = \\beta_0 + \\sum_{k = 1}^{3} \\beta_k x_{ik} + \\sum_{\\text{pairs } k, k^\\prime} \\beta_{k k^\\prime} x_{ik}x_{ik^{\\prime}} + \\epsilon_{i}\\] though we will often be interested in whether a submodel (which discards some of the main or interaction effects) can do as well.\nTo compare a full model with a submodel, we can use the relative sums of squares,\n\\[R^2 = \\frac{SS_{\\text{Model}}}{SS_{\\text{Total}}} = 1 - \\frac{SS_{E}}{SS_{\\text{Total}}}\\]\n\n\n\nInstead of trying to understand the entire model’s importance, we might want to understand the importance of specific terms. For this, it’s useful to have an uncertainty estimate. Here is an example calculation,\nVariance estimate for effect of A\n\\[\\begin{align*}\n\\text{Var}\\left(\\text{Effect }A\\right) &= \\text{Var}\\left(\\frac{1}{2^{K - 1} n}\\left(a - b - c + ab + ...\\right)\\right) \\\\\n&= \\left(\\frac{1}{2^{K - 1} n}\\right)^2\\text{Var}\\left(a - b - ac + ab + ...\\right)\n\\end{align*}\\]\nBut remember that \\(a\\) refers to the sum of all samples at corner \\(a\\), and likewise for \\(b\\), \\(ac\\), etc., \\[\\begin{align*}\n\\text{Var}\\left(a - b - ac + ab + ...\\right) &= \\text{Var}\\left(\\sum_{\\text{corner } a}y_{i} - \\sum_{\\text{corner }b}y_{i} - \\sum_{\\text{corner }ac}y_{i} + ...\\right) \\\\\n&= \\sum_{\\text{corner } a}\\text{Var}\\left(y_i\\right) + \\sum_{\\text{corner }b}\\text{Var}\\left(y_i\\right) + ...  \\\\\n&= 2^K n \\sigma^2\n\\end{align*}\\]\nso at the end of the day, we get \\[\\begin{align*}\n\\text{Var}\\left(\\text{Effect }A\\right) &= \\frac{\\sigma^2}{2^{K - 2}n}\n\\end{align*}\\]\nand we can estimate \\(\\sigma^2\\) by the error sum fo squares \\(S^2\\). From these variance estimates, we can build confidence intervals that summarize all the effects.\n\n\nsummary(fit)\n\n\n\nCall:\nlm(formula = Rate ~ A * B * C, data = plasma)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-65.50 -11.12   0.00  11.12  65.50 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   577.00      33.56  17.193 1.33e-07 ***\nA+             82.50      47.46   1.738  0.12036    \nB+             40.00      47.46   0.843  0.42382    \nC+            467.50      47.46   9.850 9.50e-06 ***\nA+:B+         -61.00      67.12  -0.909  0.39000    \nA+:C+        -318.50      67.12  -4.745  0.00145 ** \nB+:C+         -15.50      67.12  -0.231  0.82317    \nA+:B+:C+       22.50      94.92   0.237  0.81859    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 47.46 on 8 degrees of freedom\nMultiple R-squared:  0.9661,    Adjusted R-squared:  0.9364 \nF-statistic: 32.56 on 7 and 8 DF,  p-value: 2.896e-05\n\nGeneralization: \\(2^K\\) designs\nEverything we’ve spoken about can be generalized to the case of arbitrary numbers of factors. For example, the table notation can be used to get effect estimate for interaction ABCD listed before equation 6.22 in the book, and the sum of squares remain just the normalized square of the contrasts.\n\n\n\n",
    "preview": "posts/2021-08-16-week8-4/week8-4_files/figure-html5/unnamed-chunk-4-1.svg",
    "last_modified": "2021-10-04T16:33:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-16-week8-1/",
    "title": "$2^2$ Factorial Designs",
    "description": "Two factors each with two levels.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-26",
    "categories": [],
    "contents": "\nReadings 6.1, Rmarkdown\n\n\n\nA special case of general factorial design is the \\(2^K\\) design. This arises when there are \\(K\\) factors, but only 2 levels for each factor. We assume \\(n\\) samples at each configuration of factor levels. With only two levels for each factor, these experiments aren’t useful for teasing out subtle variations across levels of a factor. However they are useful for determining which of a large number of factors might be worth investigating further (factor screening).\n\n\n\nThe simplest case of a \\(2^K\\) design is when the number of factors \\(K = 2\\). The experimental design can be represented by corners of a square.\nNotation\nIt will be handy to define ways of indexing corners of the square. One approach is to write + or - for whether we are at a low or high level for that factor. Alternatively, we can represent the corner by all the letters that are at high levels.\nA\nB\nlabel\n-\n-\n(1)\n+\n-\na\n-\n+\nb\n+\n+\nab\nWe abuse notation and write \\(a\\) to represent the total of the response values at the corner +-, rather than just the index of that corner.\nEstimating effects\nSince there are only two levels for each factor, there are transparent formulas for estimating main and interaction effects.\nThe main effect for A summaries the average change in the response when A is activated. It is defined as \\[\\begin{align}\n A &= \\frac{1}{2n}\\left(\\left(ab +a\\right) - \\left(b + (1)\\right)\\right)\n \\end{align}\\] which is the average of the responses on the edge where A is active minus the average when A is inactive. The definition for B is analogous.\nThe interaction effect measures the degree to which the effect of A changes depending on whether or not B is active. It is defined as \\[\\begin{align}\n AB &= \\frac{1}{2n}\\left[\\left(ab - b\\right) + \\left(a - \\left(1\\right)\\right)\\right]\n \\end{align}\\] Notice that the role of A and B is symmetric — we could read the interaction as how the effect of B changes depending on whether A is active.\n\n\n\n\nAll these effect estimates can be summarized by our tabular notation,\nlabel\neffect A\neffect B\neffect AB\n(1)\n-\n-\n+\na\n+\n-\n-\nb\n-\n+\n-\nab\n+\n+\n+\nCode Example\nThis code example shows how a plot can help detect an interaction effect. We read in a dataset on agricultural yield when varying two factors, A and B.\n\n\nlibrary(readr)\nlibrary(dplyr)\nyield <- read_table2(\"https://uwmadison.box.com/shared/static/bfwd6us8xsii4uelzftg1azu2f7z77mk.txt\")\n\n\n\nWe can either use facet_wrap in ggplot to split a plot of the data into separate panels or use the base R interaction.plot function. Both plots show how the effect of factor A varies as the value of factor B changes. In this case, the effect of factor A is slightly smaller (lower slope) when factor B is inactive. In the next lecture though, we’ll see that this difference is not significant (it could very well happen by change under the model with no interaction term).\n\n\nggplot(yield) +\n  geom_point(aes(A, Yield)) +\n  facet_wrap(~B)\n\n\n\ninteraction.plot(yield$A, yield$B, yield$Yield)\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-16-week8-1/week8-1_files/figure-html5/unnamed-chunk-2-1.svg",
    "last_modified": "2021-10-04T16:33:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-16-week8-2/",
    "title": "Interpreting Effects in $2^2$ Designs",
    "description": "Drawing conclusions from parameter estimates.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-26",
    "categories": [],
    "contents": "\nReadings 6.2, Rmarkdown\n\n\n\nWe will often want to know whether estimated main or interaction effects are significant. We can use ANOVA, though we have to be cautious when \\(n\\) is small.\nThe numerators in the effect estimate expressions will be called contrasts for the estimated effect\nFor example, the contrast for the effect of \\(A\\) is \\(ab + a - b -  \\left(1\\right)\\).\n\nThe associated sum of squares is \\[\n \\frac{1}{2^2 n}\\left(\\text{Contrast}\\right)^2\n \\] for example, \\[\n SS_A = \\frac{1}{2^2 n}\\left[ab + a - b - (1)\\right]^2\n \\]\nThe associated ANOVA decomposition is \\[\n SS_{\\text{Total}} = SS_A + SS_B + SS_{AB} + SS_E\n \\] and since the factors all have two levels, the df’s for the main and interaction terms are all 1. The df of \\(SS_T\\) is \\(n 2^2 - 1\\) (number of samples minus one). Taking the ratio between main and interaction \\(SS\\) terms and \\(MS_{E} (= \\frac{1}{4\\left(n - 1\\right)}SS_{E})\\) gives the basis for \\(F\\)-statistics in the ANOVA table.\nRegression View\nAnother way of summarizing the \\(2^2\\) model is to write a regression, \\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\] where the \\(x_k\\)’s take on one of two values, depending on whether or not factor \\(k\\) is active.\nWe’ve only included main effects. An interaction would be added via \\(\\beta_{12} x_{1}x_{2}\\)\nIf the factors are binary (on vs. off), we can use a binary encoding.\n\n\n\n\nWhat if our factors are actually continuous?\nWe could code the variables, converting low and high levels to \\({-1, 1}\\).\nThe model will still apply to all values in interval \\([-1, 1]\\).\nAn added benefit is that this coding (a) makes scales comparable and (b) induces orthogonality (roughly, it makes variables less correlated)\n\nWe will illustrate these ideas on a yield dataset. There are 12 samples total, three replicates at each corner of the square.\n\n\nlibrary(readr)\nyield <- read_table2(\"https://uwmadison.box.com/shared/static/bfwd6us8xsii4uelzftg1azu2f7z77mk.txt\")\nyield\n\n\n# A tibble: 12 × 4\n   A     B     Rep   Yield\n   <chr> <chr> <chr> <dbl>\n 1 -     -     I        28\n 2 +     -     I        36\n 3 -     +     I        18\n 4 +     +     I        31\n 5 -     -     II       25\n 6 +     -     II       32\n 7 -     +     II       19\n 8 +     +     II       30\n 9 -     -     III      27\n10 +     -     III      32\n11 -     +     III      23\n12 +     +     III      29\n\nThe same lm + aov approach used in general factorial designs applies to \\(2^{K}\\) designs. The only difficulty is that the data were originally coded as + and -, and we need -1 and 1’s. For this, we’ve prepared a small function called coded and used it to create new columns, cA and cB which convert those symbols into their numeric equivalents. From the ANOVA table below, we can see that both factors have strong effects, though A’s is stronger than B’s. We can also see that there is no detectable interaction effect, which is consistent with the plot from the previous notes.\n\n\ncoded <- function(x) ifelse(x == '-', -1, 1)\nyield <- yield %>%\n  mutate(cA = coded(A), cB = coded(B))\nfit <- lm(Yield ~ cA * cB, data = yield)\nsummary(aov(fit))\n\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncA           1 208.33  208.33  53.191 8.44e-05 ***\ncB           1  75.00   75.00  19.149  0.00236 ** \ncA:cB        1   8.33    8.33   2.128  0.18278    \nResiduals    8  31.33    3.92                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe ANOVA table only describes whether a factor has any relationship with the response – it doesn’t describe in what way the response changes when the factors are turned on or off. For this, we need to look at the full model summary. In the output below, the Intercept estimate (27.5) gives the response when all the factors are turned off. The cA and cB estimates (4.17 and -2.5) describe how the response changes when those factors are turned on. The interaction effect measures how the effect differs from the additive effect when both factors are active (it is 0.833 larger than what would be expected if we just added cA and cB).\n\n\nsummary(fit)\n\n\n\nCall:\nlm(formula = Yield ~ cA * cB, data = yield)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.000 -1.333 -0.500  1.083  3.000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  27.5000     0.5713  48.135 3.84e-11 ***\ncA            4.1667     0.5713   7.293 8.44e-05 ***\ncB           -2.5000     0.5713  -4.376  0.00236 ** \ncA:cB         0.8333     0.5713   1.459  0.18278    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.979 on 8 degrees of freedom\nMultiple R-squared:  0.903, Adjusted R-squared:  0.8666 \nF-statistic: 24.82 on 3 and 8 DF,  p-value: 0.0002093\n\nWe can use this fit to build a response surface as well. This is a plot of the yield from the top down – darker colors correspond to higher yields. If the interaction estimate was 0, the lines would be exactly parallel to one another.\n\n\nlibrary(rsm)\nimage(fit, ~ cA + cB)\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-16-week8-2/week8-2_files/figure-html5/unnamed-chunk-2-1.svg",
    "last_modified": "2021-10-04T16:33:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-16-week7-3/",
    "title": "General Factorial Designs",
    "description": "Factorial designs with arbitrary numbers of factors",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-21",
    "categories": [],
    "contents": "\nReadings 5.4, Rmarkdown\n\n\n\nWe’ll discuss three factor factorial designs, with the hope that what we learn will generalize to arbitrary numbers of factors. In the three factor design, we use the model\n\\[y_{ijkl} = \\mu + \\tau_i + \\beta_j + \\gamma_k +\n\\left(\\tau \\beta\\right)_{ij} + \\left(\\tau \\gamma\\right)_{ik} + \\left(\\beta\n\\gamma\\right)_{jk} + \\left(\\tau \\beta \\gamma\\right)_{ijk} + \\epsilon_{ijkl}\\]\nwhere \\(\\epsilon_{ijkl} \\sim N\\left(0, \\sigma^2\\right)\\). Suppose that the first, second, and third factors have \\(a, b\\), and \\(c\\) levels, respectively.\nWe’re dangerously close to getting lost in index purgatory, but notice certain symmetries,\nWe have main effects for each factor\n\\(\\tau_i, \\beta_j, \\gamma_k\\)\n\nWe have two-way interactions for each pair of factors\n\\(\\left(\\tau\\beta\\right)_{ij}, \\dots\\)\n\nWe have a three-way interaction, between all factors\n\\(\\left(\\tau\\beta\\gamma\\right)_{ijk}\\)\n\n\n\n\n\nWe can calculate sum-of-squares terms for each of the terms. Notice that there are also certain symmetries in the degrees of freedom,\n\\(SS_A = a - 1\\)\n\\(SS_B = b - 1\\)\n\\(SS_C = c - 1\\)\n\\(SS_{AB} = (a - 1)(b - 1)\\)\n\\(SS_{BC} = (b - 1)(c - 1)\\)\n…\n\\(SS_{ABC} = (a - 1)(b - 1)(c - 1)\\)\nWhat do you think is the pattern for arbitrary \\(K\\).\nFor testing, we will compare these sums-of-squares to \\(SS_E\\), which has \\(abc(n - 1)\\) degrees of freedom. The \\(F\\)-statistics for any of the terms above can be found by dividing the associate mean squares against \\(MS_E\\). Hence, we can test whether any of the terms is nonzero for at least one value of its index.\nData Example\nLet’s look at a \\(2^3\\) design (3 factors with two levels each). The goal is to see how the etch rate on a chip varies as we change (A) gap between electrodes,\npower level, and (C) gas flow rate.\n\n\nplasma <- read.table(\"https://uwmadison.box.com/shared/static/f3sggiltyl5ycw1gu1vq7uv7omp4pjdg.txt\", header=TRUE)\n\n\n\nLooking at the data, there seems to be a strong interaction between A (the x-axis) and C (the pairs of columns): the slope of the effect of A switches when we go from one C configuration to the other.\n\n\nggplot(plasma) +\n  geom_point(aes(A, Rate)) +\n  facet_grid(B ~ C)\n\n\n\n\nWe can quantify the strength of these relationships by estimating the model and evaluating the relevant \\(F\\)-statistics. The * syntax refers to all main and interaction effects derived from the linked variables.\n\n\nfit <- lm(Rate ~ A * B * C, plasma)\nsummary(aov(fit))\n\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nA            1  41311   41311  18.339 0.002679 ** \nB            1    218     218   0.097 0.763911    \nC            1 374850  374850 166.411 1.23e-06 ***\nA:B          1   2475    2475   1.099 0.325168    \nA:C          1  94403   94403  41.909 0.000193 ***\nB:C          1     18      18   0.008 0.930849    \nA:B:C        1    127     127   0.056 0.818586    \nResiduals    8  18020    2253                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n",
    "preview": "posts/2021-08-16-week7-3/week7-3_files/figure-html5/unnamed-chunk-2-1.svg",
    "last_modified": "2021-10-04T16:32:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-16-week7-4/",
    "title": "An Introduction to Response Surfaces",
    "description": "Flexibly modeling the relationship between factors and a response.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-21",
    "categories": [],
    "contents": "\nReadings 5.5, Rmarkdown\n\n\n\nWe’ve really pushed a particular recipe for all our hypothesis testing approaches,\nWrite down a model with various factor level effects\nWrite down a sum-of-squares identity, and get the corresponding degrees-of-freedom\nGet a test statistic and its reference distribution, for testing each factor We’ve used this in ANOVA, RCBD, general factorial designs…\n\nWe’re now going to introduce a quite different approach based on response surfaces. The idea is simple: use a flexible (nonlinear) function from experimental inputs (combinations of factor levels) to the response of interest. This will work as long as the response varies smoothly as factor inputs are perturbed. The estimated function will be a good representation of how varying the factors affects the response.\nMoreover, if we have successfully estimated this function, then we’ll be able to use the fit to (a) determine important influences and (b) find configurations that optimize the response (e.g., maximize profit[^Or if you are disgruntled, minimize profit]).\nHow should we fit these flexible functions?\nPolynomial regression: include terms like \\(x_i^2, x_i^3, x_i^2 x_j, ...\\)\nSpline regression: Include polynomial terms, but split across different regions of the input space. This is generally more stable than polynomial regression.\nReally, you can use whatever function fitter that you want. Unfortunately, this idea will have to wait till near the end of the course for a more complete elaboration.\n\nData Example\nLet’s look at this idea using the battery data from before. We’ll treat temperature as a continuous variable, so that it makes sense to talk about a response surface[^really, a curve over temperature].\n\n\nlibrary(dplyr)\nlibrary(readr)\nbattery <- read_table2(\"https://uwmadison.box.com/shared/static/vmxs2wcsdxkdjujp85nw5kvk83xz4gl9.txt\") %>%\n  mutate(Material = as.factor(Material))\n\n\n\nWe fit a quadratic regression to define the surface. The result let’s us make predictions for battery life at temperatures that we haven’t observed. First, we fit a model that uses both Material and a quadratic expansion of Temperature.\n\n\nfit <- lm(Life ~ Material * poly(Temperature, 2), data = battery)\n\n\n\nTo visualize the response surface, we compute predictions across a find grid of temperature values for each material.\n\n\nsurface <- expand.grid(\n  Temperature = seq(15, 125, by = 1),\n  Material = unique(battery$Material) \n  )\nsurface$Life <- predict(fit, surface)\n\nggplot(battery, aes(Temperature, Life)) +\n  geom_point() +\n  geom_line(data = surface) +\n  facet_wrap(~ Material)\n\n\n\n\nCompare the associated fit with Table 5.15.\n\n\nsummary(fit)\n\n\n\nCall:\nlm(formula = Life ~ Material * poly(Temperature, 2), data = battery)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-60.750 -14.625   1.375  17.937  45.250 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(>|t|)\n(Intercept)                       83.167      7.501  11.087 1.48e-11\nMaterial2                         25.167     10.608   2.372 0.025059\nMaterial3                         41.917     10.608   3.951 0.000503\npoly(Temperature, 2)1           -189.223     45.007  -4.204 0.000257\npoly(Temperature, 2)2            109.955     45.007   2.443 0.021385\nMaterial2:poly(Temperature, 2)1  -71.035     63.650  -1.116 0.274242\nMaterial3:poly(Temperature, 2)1   45.928     63.650   0.722 0.476759\nMaterial2:poly(Temperature, 2)2 -158.392     63.650  -2.488 0.019293\nMaterial3:poly(Temperature, 2)2 -197.636     63.650  -3.105 0.004434\n                                   \n(Intercept)                     ***\nMaterial2                       *  \nMaterial3                       ***\npoly(Temperature, 2)1           ***\npoly(Temperature, 2)2           *  \nMaterial2:poly(Temperature, 2)1    \nMaterial3:poly(Temperature, 2)1    \nMaterial2:poly(Temperature, 2)2 *  \nMaterial3:poly(Temperature, 2)2 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.98 on 27 degrees of freedom\nMultiple R-squared:  0.7652,    Adjusted R-squared:  0.6956 \nF-statistic:    11 on 8 and 27 DF,  p-value: 9.426e-07\n\n\n\n\n",
    "preview": "posts/2021-08-16-week7-4/week7-4_files/figure-html5/unnamed-chunk-4-1.svg",
    "last_modified": "2021-10-04T16:32:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-16-week7-1/",
    "title": "Two-Factor Factorial Design",
    "description": "Modeling and testing with two factors of interest",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\nReadings 5.3, Rmarkdown\nRecall that in a factorial design, we want to study the influences of multiple experimental factors on some responses of interest. The simplest situation is when there are two factors of interest, and the associated designs are two-factor factorial designs.\nWe will assume a model of the form, \\[\ny_{ijk} = \\mu + \\tau_i + \\beta_j + \\left(\\tau\\beta\\right)_{ij} + \\epsilon_{ij}\n\\] where \\(\\epsilon_{ijk} \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\) independently. For identifiability, we have to assume that the sums \\(\\sum_{i} \\tau_i =  \\sum_{j} \\beta_{j}= \\sum_{ij} \\left(\\tau\\beta\\right)_{ij}=0\\). We’ll suppose \\(i\\) ranges from \\(1, \\dots, a\\), \\(j\\) ranges from \\(1, \\dots, b\\) and \\(k\\) ranges from \\(1,\\dots, n\\).\nWhat do the indices mean?\n\\(i\\) indexes levels of the first factor (temperature).\n\\(j\\) indexes levels of the second factor (material).\n\\(k\\) indexes multiple replicates at a particular factor combination \\(ij\\) (battery lifetimes, for fixed temperature \\(\\times\\) material combination)\n\nWhat do the greek letters mean?\n\\(\\mu\\): An intercept term, representing a global mean.\n\\(\\tau_i\\): The effect of the \\(i^{th}\\) level of factor 1 on the average response (effect of temperature setting \\(i\\))\n\\(\\beta_{j}\\): The effect of the \\(j^{th}\\) level of factor 2 on the average response (effect of material \\(j\\))\n\\(\\left(\\tau\\beta\\right)_{ij}\\):The interaction / synergy between the \\(i^{th}\\) level of factor 1 and the \\(j^{th}\\) level of factor 2. (e.g. long / short response times, depending on particular material and temperature settings)\n\\(\\epsilon_{ijk}\\): The random variation we’d observe if we drew many samples at a fixed setting of the two factors. Caution: \\(\\beta_j\\) now indexes a treatment of interest. It is not just nuisance blocking variation.\n\n\n\n\nFigure 1: A graphical view of all the parameters in a factorial design.\n\n\n\nTesting\nUp until now, we’ve only asked whether one particular factor had an influence on the response, as the levels were changed. Now, we care about each of the factors, and each gets a hypothesis test.\n(Hypothesis A) For the first factor, \\[\\begin{align}\nH_0 &: \\tau_1 = \\dots = \\tau_a = 0 \\\\\nH_{1} &: \\tau_{i} \\neq 0 \\text{ for at least one } i\n\\end{align}\\]\n(Hypothesis B) For the second factor, \\[\\begin{align}\nH_0 &: \\beta_1 = \\dots = \\beta_{b} = 0 \\\\\nH_{1} &: \\beta_{j} \\neq 0 \\text{ for at least one } j\n\\end{align}\\]\n(Hypothesis AB) Are there interaction effects? \\[\\begin{align}\nH_0: \\left(\\tau\\beta\\right)_{ij}= \\dots = \\left(\\tau\\beta\\right)_{ij} = 0 \\\\\nH_1: \\left(\\tau\\beta\\right)_{ij} &= 0 \\text{ for at least one } ij \\text{ combination}\n\\end{align}\\]\nFor each of these hypothesis tests, we’re going to need a test statistic that’s sensitive to departures from the null. We’re also going to need their reference distributions.\nMiraculously, we have the identity, \\[\\begin{align*}\n\\sum_{i j k}\\left(y_{i j k}-\\bar{y}\\right)^{2}=& b n \\sum_{i}\\left(\\bar{y}_{i . .}-\\bar{y}\\right)^{2}+\\\\\n& a n \\sum_{j}\\left(\\bar{y}_{.j.} -\\bar{y}\\right)^{2}+\\\\\n& n \\sum_{i, j}\\left(\\bar{y}_{i j.}-\\bar{y}_{i. .}+\\bar{y}_{. j.}-\\bar{y}\\right)^{2}+\\\\\n& \\sum_{i, j, k}\\left(y_{i j k}-\\bar{y}_{i j.} .\\right)^{2}\n\\end{align*}\\] which we’ll denote \\(SS_{\\text{Total}}=SS_{A}+SS_{B}+SS_{AB}+SS_{E}\\).\nThe punchline is that if \\(SS_{A}\\) is large, we have evidence against hypothesis A, if \\(SS_{B}\\) is large we have evidence against hypothesis B, and if \\(SS_{AB}\\) is large, we have evidence against hypothesis AB.\n\n\n\nDividing \\(SS\\) terms by their degrees of freedom (d.f.) gives \\(MS_{A}, MS_{B}, MS_{AB}\\), and \\(MS_{E}\\). The d.f. are derived from the number of levels for each factor, but a proof is beyond the scope of this course.\n| term | d.f.|\n| $SS_{A} | \\(a - 1\\) | | $SS_{B} | \\(b - 1\\) | | $SS_{AB} | \\(\\left(a - 1\\right)\\left(b - 1\\right)\\) | | $SS_{E} | \\(ab\\left(n - 1\\right)\\) |\nWe can define corresponding mean squares by dividing by the degrees of freedom. For each hypothesis, we get a corresponding \\(F\\)-statistic.\n\n\n\nCode Example\nWe’ll consider an experiment that studied the effect of material and temperature on battery lifetimes. These are the two factors of interest, and each is measured at 3 levels.\n\n\n\n\n# A tibble: 36 × 3\n   Material Temperature  Life\n   <fct>    <fct>       <dbl>\n 1 1        15            130\n 2 1        15            155\n 3 1        15             74\n 4 1        15            180\n 5 1        70             34\n 6 1        70             40\n 7 1        70             80\n 8 1        70             75\n 9 1        125            20\n10 1        125            70\n# … with 26 more rows\n\nBefore testing for effects, we can plot the influence of each factor. The code below uses facet_wrap to split the plot across the three material types. There seems to be a clear temperature effect, though the effects are not exactly the same across material. This suggests that an interaction is present, though we will need a test to quantify the strength of this pattern.\n\n\n\nWe can fit the two factor model using lm. We use the syntax Material * Temperature to fit all main effects and interactions involving those variables; it is shorthand for the more explicit notation Material + Temperature + Material : Temperature (here, the : denotes an interaction). The Material and Temperature rows of the ANOVA table give main effects, the Material:Temperature row gives the interaction effect, and the Residuals row corresponds to the \\(SS_{E}\\) and \\(MS_{E}\\) terms.\n\n                     Df Sum Sq Mean Sq F value   Pr(>F)    \nMaterial              2  10684    5342   7.911  0.00198 ** \nTemperature           2  39119   19559  28.968 1.91e-07 ***\nMaterial:Temperature  4   9614    2403   3.560  0.01861 *  \nResiduals            27  18231     675                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nBased on the \\(p\\)-values, we conclude that there is a very strong temperature effect, a strong material effect, and a noticeable (though slight) interaction. We can in fact develop more granular interpretations by looking at individual coefficients in the fitted model. The runs from the first material and lowest temperature are used as a reference point and absorbed into the intercept – everything else is viewed as the expected deviation from that reference. For example, it seems that, on average, the temperature 70 and 125 configurations are both about 80 life-units shorter than runs at temperature 15. The significant interaction between material 3 and temperature 70 suggests that this combination lives about 80 units longer than we would expect if no interaction were present.\n\n\nCall:\nlm(formula = Life ~ Material * Temperature, data = battery)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-60.750 -14.625   1.375  17.938  45.250 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                134.75      12.99  10.371 6.46e-11 ***\nMaterial2                   21.00      18.37   1.143 0.263107    \nMaterial3                    9.25      18.37   0.503 0.618747    \nTemperature70              -77.50      18.37  -4.218 0.000248 ***\nTemperature125             -77.25      18.37  -4.204 0.000257 ***\nMaterial2:Temperature70     41.50      25.98   1.597 0.121886    \nMaterial3:Temperature70     79.25      25.98   3.050 0.005083 ** \nMaterial2:Temperature125   -29.00      25.98  -1.116 0.274242    \nMaterial3:Temperature125    18.75      25.98   0.722 0.476759    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.98 on 27 degrees of freedom\nMultiple R-squared:  0.7652,    Adjusted R-squared:  0.6956 \nF-statistic:    11 on 8 and 27 DF,  p-value: 9.426e-07\n\n\n\n\n",
    "preview": "posts/2021-08-16-week7-1/figure/parameters.png",
    "last_modified": "2021-10-04T16:32:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-16-week7-2/",
    "title": "Following-up Two-Factor Fits",
    "description": "Multiple comparisons, model checking, and other post-estimation checks.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\nReadings 5.3, Rmarkdown\n\n\n\nWe’ll cover the analogs of performing multiple comparisons, and doing model diagnostics for the two-factor model. Almost everything will be familiar from our experience with single-factor models.\n\n\nlibrary(readr)\nlibrary(dplyr)\nbattery <- read_table2(\"https://uwmadison.box.com/shared/static/vmxs2wcsdxkdjujp85nw5kvk83xz4gl9.txt\") %>%\n  mutate_at(vars(-Life), as.factor)\nfit <- lm(Life ~ Material * Temperature, data=battery)\n\n\n\nMultiple Comparisons\nWe may want to use contrasts, to find out exactly how a particular factor is associated with the response.\nSubtlety: If the factor under investigation interacts with the other one, its effects will depend on that other factor.\nSolution: Fix a level for the other factor, and study the influence of levels for the factor of interest.\nExample: Fix temperature, and use Tukey’s HSD to study pairwise difference between materials, for that fixed temperature.\n\n\n\n\nFigure 1: All pairwise contrasts between levels of one factor, restricted to a single level of another.\n\n\n\n\n\nlibrary(emmeans)\nemmeans(fit, pairwise ~ Material | Temperature)$contrasts\n\n\nTemperature = 15:\n contrast estimate   SE df t.ratio p.value\n 1 - 2      -21.00 18.4 27  -1.143  0.4967\n 1 - 3       -9.25 18.4 27  -0.503  0.8703\n 2 - 3       11.75 18.4 27   0.639  0.7998\n\nTemperature = 70:\n contrast estimate   SE df t.ratio p.value\n 1 - 2      -62.50 18.4 27  -3.402  0.0058\n 1 - 3      -88.50 18.4 27  -4.817  0.0001\n 2 - 3      -26.00 18.4 27  -1.415  0.3475\n\nTemperature = 125:\n contrast estimate   SE df t.ratio p.value\n 1 - 2        8.00 18.4 27   0.435  0.9012\n 1 - 3      -28.00 18.4 27  -1.524  0.2959\n 2 - 3      -36.00 18.4 27  -1.959  0.1419\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\nModel Checking\nOur key assumptions are independence, normality, and equal variances for the \\(\\epsilon_{ijk}\\)’s. Our diagnostics are based on residuals.\n\n\nbattery <- battery %>%\n  mutate(resid = resid(fit), y_hat = predict(fit)) # create two new columns\nggplot(battery) +\n  geom_point(aes(Temperature, resid))\n\n\n\n\nPlot residuals against each of the two factors\n\n\nggplot(battery) +\n  geom_point(aes(y_hat, resid))\n\n\n\n\nPlot fitted vs. residual value\n\n\nqqnorm(battery$resid)\nqqline(battery$resid, col = \"red\")\n\n\n\n\nMake normal probability plots\nChoosing the Sample Size\nHow should you choose how many replicates to have at each combination of the two factors?\nSimulate a model using a particular configuration of coefficients.\nSee how your power to detect effects varies as you increase the sample size.\n\nFor example, in the block below, we simulate data from where the true \\(\\tau_i\\)’s and \\(\\beta_j\\)’s are \\(\\left(0, 1, 2\\right)\\) and \\(\\left(0, 1\\right)\\), respectively. We increase the number of samples from 2 to 10 and draw 50 replicates each. Fitting a linear model to each dataset allows us to see how the sample size influences our final \\(p\\)-values.\n\n\ntau <- c(0, 1, 2)\nbeta <- c(0, 1)\nns <- seq(2, 10, by = 2)\nb <- 1 # a counter variable\nsims <- list()\nfor (k in seq_along(ns)) {\n  for (sim_rep in seq_len(50)) {\n    for (i in seq_along(tau)) {\n      for (j in seq_along(beta)) {\n        sims[[b]] <- data.frame(\n            \"factor_1\" = i,\n            \"factor_2\" = j,\n            \"sample_size\" = ns[k],\n            \"replicate\" = seq_len(ns[k]),\n            \"value\" = rnorm(ns[k], tau[i] + beta[j]),\n            \"sim_rep\" = sim_rep\n        )\n        b <- b + 1\n      }\n    }\n  }\n}\nsims <- bind_rows(sims)\n\n\n\nThe block below visualizes an example replicate. We have 50 datasets like this for every value of \\(n\\).\n\n\nggplot(sims %>% filter(sim_rep == 1)) + # visualize only the first replicate\n  geom_point(aes(factor_1, value)) +\n  facet_grid(sample_size ~ factor_2)\n\n\n\n\nNext, we fit a linear model to each simulation replicate and plot the associated \\(p\\)-values. The code to split the datasets and fit an lm to each is somewhat complicated – the important thing to takeaway here is that we can use a simulation to see exactly how the \\(p\\)-values decrease as a function of the sample size (and hence allow us to draw stronger conclusions).\n\n\nlibrary(broom)\nlibrary(purrr)\nlibrary(tidyr)\n\npower <- sims %>% # get p.values and estimates from every single run\n  split(list(.$sample_size, .$sim_rep)) %>%\n  map_dfr(~ tidy(lm(value ~ factor_1 + factor_2, data = .)), .id = \"group\") %>%\n  separate(group, c(\"sample_size\", \"sim_rep\"), convert = TRUE)\n\nggplot(power) +\n  geom_jitter(aes(sample_size, p.value)) +\n  geom_hline(yintercept = 0.05, col = \"red\") +\n  facet_wrap(~ term)\n\n\n\n\nInteractions\nIf we only have one replicate per cell, then we can’t estimate an interaction effect. If we tried, we’d be able to perfectly fit the data, so there would be no way to estimate \\(\\sigma^2\\).\nAs a general recipe, to check for interactions, we can perform residual analysis on the main effects model. Alternatively, we could use Tukey’s additivity test, which checks whether a multiplicative form of the interaction is present, i.e., does \\[y_{ijk} = \\mu + \\tau_i + \\beta_j + \\gamma \\tau_i \\beta_j\\] fit significantly better than the main effects model?\n\n\n\n",
    "preview": "posts/2021-08-16-week7-2/week7-2_files/figure-html5/unnamed-chunk-3-1.svg",
    "last_modified": "2021-10-04T16:32:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-16-week6-1/",
    "title": "Balanced Incomplete Block Designs",
    "description": "An alternative to RCBDs in the limited sample setting",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-12",
    "categories": [],
    "contents": "\nReadings 4.4, Rmarkdown\nRandomized Complete Block Designs are good when you can run each treatment in each block. What should we do if we have an upper limit on the number of samples we can obtain for each block? For example,\nThe amount of material per block gets exhausted after 3 samples are prepared, and we want to study 4 treatments\nWe’re interested in three shoe sole types\nWe’ll have to settle on an Incomplete Block Design: not all treatments appear in each block. We should assign treatments randomly. We will call a design balanced if any pair of treatments appears together in a block the same number of times.\n\n\n\nFigure 1: An example BIBD with 3 treatments total and 2 treatments run per each block.\n\n\n\n\n\n\nFigure 2: An example BIBD with 4 treatments total and 2 treatments run per each block.\n\n\n\nSome helpful notation,\n\\(a\\): Number of treatment levels\n\\(b\\): Number of blocks\n\\(k\\): Number of treatments per block\n\\(r\\): Number of blocks per treatment\n\\(\\lambda\\): Number of appearances for each level pair\n\n\n\n\nFigure 3: Circles mark which treatments are run in which blocks. We need to ensure balance across this diagram.\n\n\n\nAn identity that relates these variables is \\(\\lambda (a - 1) = r(k - 1)\\). We can derive it by counting the number of samples that occur in blocks containing treatment A, excluding the samples given treatment A.\n\n\n\nFigure 4: A way of counting samples that leads to the right hand side of the identity in item (4).\n\n\n\n\n\n\nFigure 5: A way of counting samples that leads to the left hand side of the identity in item (4).\n\n\n\nHypothesis Testing\nThe model is the same as in the RCBD, \\[\ny_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij}\n\\] where \\(\\epsilon_{ij} \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\) independently and \\(\\sum_{i} \\tau_i= \\sum_{j} \\beta_{j}=0\\). The difference is that we don’t observe \\(y_{ij}\\) for all combinations of blocks and treatments.\n\n\n\nFigure 6: Each column is a single block, colors are treatments. We choose only a subset of treatments within each block.\n\n\n\nTo test the hypothesis,\n\\[\\begin{aligned}\nH_0 &: \\tau_1 = \\dots = \\tau_a = 0 \\\\\nH_{1} &: \\tau_{i} \\neq 0 \\text{ for at least one } i\n\\end{aligned}\\]\nwe can use a decomposition, \\[\nSS_{\\text{Total}} = SS_{\\text{Treatments}\\left(adjusted\\right)} + SS_{\\text{Block}} + SS_{E}\n\\]\nThe \\(SS_{\\text{Total}}\\) and \\(SS_{\\text{Block}}\\) terms are familiar, except they are only summed over terms that are observed, \\[\\begin{align}\nSS_{\\text{Total}} &= \\sum_{i, j} \\left(y_{ij} - \\bar{y}_{\\cdot\\cdot}\\right)^2 \\\\\nSS_{\\text{Block}} &= k \\sum_{j} \\left(\\bar{y}_{\\cdot j} - \\bar{y}_{\\cdot\\cdot}\\right)^2\n\\end{align}\\]\nBut \\(SS_{\\text{Treatments}}\\) is not the expression you might guess at first. The reason is each treatment only appears in a subset of blocks, which might bias conclusions. An adjusted expression corrects for this bias, \\[\\begin{align*}\n SS_{\\text{Treatments }(adjusted )}=\\frac{k}{\\lambda a} \\sum_{i=1}^{a}\\left(\\bar{y}_{i}-\\frac{1}{k} \\sum_{j \\in B(j)} \\bar{y}_{\\cdot j}\\right)^2\n \\end{align*}\\] where \\(B\\left(j\\right)\\) indexes the blocks within which treatment \\(i\\) was administered.\n\n\n\nFigure 7: The transparent points are not actually observed. Ignoring this fact might lead to incorrect treatment estimates (treatments in the ‘high’ blocks might be incorrectly be thought to be larger in general).\n\n\n\n\\(SS_E\\) is obtained through subtraction, and turns out to have \\(N - a - b + 1\\) degrees of freedom. Therefore,\n\\[\\begin{aligned}\n\\frac{M S_{\\text {Treatment }(\\text { adjusted })}}{M S_{E}} &:=\\frac{\\frac{1}{a-1} S S_{\\text {Treatment }(\\text { adjusted })}}{\\frac{1}{N-a-b+1} S S_{E}} \\\\\n& \\sim F(a-1, N-a-b+1)\n\\end{aligned}\\]\nwhich can be used as the basis for a hypothesis test.\nCode Example\nBefore analyzing a BIBD experiment, it’s worth knowing how to construct a design given experimental constraints. Consider the following questions,\nSuppose we have \\(a\\) treatments and only \\(k\\) can be applied per block. How many blocks would we need to collect before we could have a balanced design?\nOnce we know how many blocks we need in an experiment, how can we find a specific configuration of treatments to run in each block?\n\nThe functions BIBsize and find.BIB answer these two questions. In the code below, we imagine having 5 treatments of interest, but can only run 3 in each block. The result of BIBsize is used to choose a specific design in find.BIB. Each row is a block, and the columns give the three treatments to run for each block.\n\n\n\n\n\nlibrary(crossdes)\nlibrary(daewr)\nBIBsize(t = 5, k = 3) # we should use 10 blocks\n\n\nPosible BIB design with b= 10  and r= 6  lambda= 3 \n\nfind.BIB(trt = 5, k = 3, b = 10)\n\n\n      [,1] [,2] [,3]\n [1,]    1    3    5\n [2,]    2    3    4\n [3,]    2    4    5\n [4,]    3    4    5\n [5,]    1    2    5\n [6,]    1    2    3\n [7,]    1    4    5\n [8,]    1    3    4\n [9,]    1    2    4\n[10,]    2    3    5\n\nNext, we’ll analyze a BIBD experiment. We download the catalyst experiment, which studied the effect of different catalysts on chemical reaction time.\n\n\nlibrary(readr)\nlibrary(dplyr)\ncatalyst <- read_table2(\"https://uwmadison.box.com/shared/static/2tfwo6oohyffw0x299105rj54iabkw4u.txt\") %>%\n  mutate_at(vars(-Time), as.factor) # use as.factor on all but the Time column\n\n\n\nTo get a sense of the data, we’ll make some plots of reaction time, both against batch ID and catalyst type. Note that we only run three catalysts per batch. There seem to be definitive batch-to-batch differences, but it’s unclear whether any of these catalysts are really any different from the others.\n\n\nlibrary(ggbeeswarm)\nggplot(catalyst) +\n  geom_beeswarm(aes(x = Batch, y = Time, col = Catalyst)) +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\nTo estimate the effects for each catalyst, we can use lm. The associated ANOVA gives evidence against the null that the catalysts are equal, and the corresponding confidence intervals suggests that catalyst 4 has a longer reaction time than would be believable under the null. This is a nice situation in which something that wasn’t obvious visually becomes more clear when we apply a principled testing procedure. Compare the ANOVA table with Table 4.25.\n\n\nfit <- lm(Time ~ ., data = catalyst)\naov_table <- aov(fit)\nsummary(aov_table) # only the unadjusted block mean square\n\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \nBatch        3  55.00  18.333   28.20 0.00147 **\nCatalyst     3  22.75   7.583   11.67 0.01074 * \nResiduals    5   3.25   0.650                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint(fit)\n\n\n                 2.5 %     97.5 %\n(Intercept) 70.6671254 73.8328746\nBatch2       0.3301889  3.9198111\nBatch3      -6.5448111 -2.9551889\nBatch4      -2.6698111  0.9198111\nCatalyst2   -1.5448111  2.0448111\nCatalyst3   -1.1698111  2.4198111\nCatalyst4    1.8301889  5.4198111\n\nWe can also compute contrasts in the way that we’re used to. Here, we’re studying two hypotheses related to catalyst 4 (do it and catalyst 1 have larger reaction times than the others? is catalyst 4 larger than catalyst 3?).\n\n\nlibrary(gmodels)\ncontrasts <- matrix(\n  c(1, -1, -1, 1,\n    0, 0, 1, -1),\n  nrow = 2, byrow=TRUE\n)\n\nfit.contrast(aov(fit), \"Catalyst\", contrasts)\n\n\n                         Estimate Std. Error   t value    Pr(>|t|)\nCatalyst c=( 1 -1 -1 1 )     2.75  0.9874209  2.785033 0.038671213\nCatalyst c=( 0 0 1 -1 )     -3.00  0.6982120 -4.296689 0.007739734\nattr(,\"class\")\n[1] \"fit_contrast\"\n\nWe can also control for multiple comparisons, but we need to use a different function, since our usual PostHocTest isn’t implemented to cover the case of incomplete block designs. Instead, we can use the lsmeans function. Unsurprisingly, the most significant tests seem to be highlighting the discrepancy between catalyst 4 and the others.\n\n\nlibrary(lsmeans)\nlsmeans(fit, pairwise ~ Catalyst, adjust = \"Tukey\") # tukey's test\n\n\n$lsmeans\n Catalyst lsmean    SE df lower.CL upper.CL\n 1          71.4 0.487  5     70.1     72.6\n 2          71.6 0.487  5     70.4     72.9\n 3          72.0 0.487  5     70.7     73.3\n 4          75.0 0.487  5     73.7     76.3\n\nResults are averaged over the levels of: Batch \nConfidence level used: 0.95 \n\n$contrasts\n contrast estimate    SE df t.ratio p.value\n 1 - 2      -0.250 0.698  5  -0.358  0.9825\n 1 - 3      -0.625 0.698  5  -0.895  0.8085\n 1 - 4      -3.625 0.698  5  -5.192  0.0130\n 2 - 3      -0.375 0.698  5  -0.537  0.9462\n 2 - 4      -3.375 0.698  5  -4.834  0.0175\n 3 - 4      -3.000 0.698  5  -4.297  0.0281\n\nResults are averaged over the levels of: Batch \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\n\n\n",
    "preview": "posts/2021-08-16-week6-1/figure/bibd-identity1.png",
    "last_modified": "2021-10-04T16:32:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-16-week6-2/",
    "title": "Introduction to Factorial Designs",
    "description": "Characterizing multiple facotrs in a single experiment.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-12",
    "categories": [],
    "contents": "\nReadings 5.1 - 5.2, Rmarkdown\nTo this point, we’ve only studied experiments with one factor of interest. How should we design experiments when there are several factors, all of interest?\nThis is the most common situation we encounter in practice, and the remainder of the course is going to be devoted to solutions to this problem, all based on the idea of factorial designs.\nFor example, consider,\nBattery life: An engineer wants to study how long a battery will last, as a function of temperature and material used in construction.\nBottling: The fill height of a bottling machine depends on percent carbonation, operating pressure, and assembly line speed, but the precise relationship is currently unknown.\n\nVisualizing Factorial Designs\nWhen we have two factors and a continuous response, we can visualize the relationship either in 3D, or from the top down.\nIf the factor has only two levels, the possible configurations are corners square (hypercube, in higher dimensions).\nIf it can vary continuously, we get a surface.\n\n\n\n\nFigure 1: A two factor experiment viewed in 3D.\n\n\n\n\n\n\nFigure 2: A two factor experiment viewed from the top down.\n\n\n\nWhen we have more than two factors, we have to pick a subset of factors to visualize at a time, and show multiple surfaces for each configuration of other factors.\nMain and Interaction Effects\nThe main effect is defined as the average change in the response when increasing a factor by one unit, where the average is taken over values of all other factors\nIt’s possible that the change in response of one factor depends on the value of other factors. In this case, we say there is an interaction between the factor and the factors that cause changes in effect sizes.\n\n\n\nFigure 3: A two factor experiment with an interaction effect, viewed in 3D\n\n\n\n\n\n\nFigure 4: A two factor experiment with an interaction effect, viewed from the top down\n\n\n\nWe can encode an interaction through a regression equation \\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_{1} x_{2}\n\\]\nWhen \\(\\beta_{12}\\) is nonzero, there is an interaction between \\(x_1\\) and \\(x_2\\).\nIn this case, the slopes of \\(x_1\\) and \\(x_2\\) are not fixed, and each depends on the value of the other variable.\n\n",
    "preview": "posts/2021-08-16-week6-2/figure/22_factorial.png",
    "last_modified": "2021-10-04T16:32:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-06-week5-3/",
    "title": "RCBD with Random Block Effects",
    "description": "The random effects analog of RCBD designs",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-07",
    "categories": [],
    "contents": "\nReadings 4.1, Rmarkdown\nAs in standard random effects, sometimes the blocks are from a larger population. For example, in the medical device example, we care about a ``resin effect,’’ but don’t care about each individual batch.\nThe model is setup as before,\n\\[\ny_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij}\n\\] except now both \\(\\beta_j \\sim \\mathcal{N}\\left(0, \\sigma_{\\beta}^2\\right)\\) and \\(\\epsilon_{ij} \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\), all independently.\n\n\n\nObservations are correlated within each block are correlated. The calculation to demonstrate this is similar to the one used to show that observations \\(i\\) within levels of an ordinary random effects model are correlated.\nHypothesis Testing\nWe’re interested in whether any of the treatments have an effect,\n\\[\\begin{align*}\nH_0 &: \\tau_1 = \\dots = \\tau_a = 0 \\\\\nH_{1} &: \\tau_{i} \\neq 0 \\text{ for at least one } i\n\\end{align*}\\]\nWe won’t show it, but it turns that \\[\\begin{align*}\n\\mathbf{E}\\left[MS_{\\text{Treatment}}\\right] &= \\sigma^2 + \\frac{b \\sum_{i = 1}^{a} \\tau_i^2}{a - 1} \\\\\n\\mathbf{E}\\left[MS_{\\text{Block}}\\right] &= \\sigma^2 + a \\sigma^2_{\\beta} \\\\\n\\mathbf{E}\\left[MS_{E}\\right] &= \\sigma^2\n\\end{align*}\\] so we should reject the null when \\(MS_{\\text{Treatment}}\\) is much larger than \\(MS_E\\).\nIn fact, as in the fixed block case, \\[\n\\frac{MS_{\\text{Treatment}}}{MS_{E}} \\sim F\\left(a - 1, \\left(a - 1\\right)\\left(b - 1\\right)\\right)\n\\] so we can use the same F-distribution cutoff when testing whether any treatment effects are nonzero.\nEstimation\nAs in random effects for the completely randomized design, we can use either the method of moments or maximum likelihood. The method of moments estimators are \\[\\begin{align*}\n\\hat{\\sigma}    &= MS_{E} \\\\\n\\hat{\\sigma}^2_{\\beta} &= \\frac{1}{a}\\left[MS_{\\text{Block}} - MS_{E}\\right]\n\\end{align*}\\]\nFinding confidence intervals continues to be a challenge for the method of moments approach. In this case, maximum likelihood is preferred. This method is shown in the computer example accompanying these notes.\n\n\n\nCode Example\nWe’ll continue with the graft dataset. This reads in the data as in the previous notes.\n\n\nlibrary(readr)\nlibrary(tidyr)\ngraft <- read_table2(\"https://uwmadison.box.com/shared/static/0ciblk4z2f3k6zizbj4plg3q33w1d0n6.txt\") %>%\n  pivot_longer(-Pressure, names_to = \"batch\", values_to = \"yield\")\ngraft$Pressure <- as.factor(graft$Pressure)\n\n\n\nAs with ordinary random effects ANOVA, we can fit an RCBD with random block effects using lmer. Notice that we only use the (1 | -) notation for the batch variable. This means that the batches are viewed as random but pressure is considered fixed. This is why in the summary output, we have a variance component \\(\\sigma^2_{\\beta}\\) for the batches, but separate fixed effects for across pressure levels.\n\n\nlibrary(lme4)\nfit <- lmer(yield ~ Pressure + (1 | batch), data = graft)\nfit\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: yield ~ Pressure + (1 | batch)\n   Data: graft\nREML criterion at convergence: 112.0424\nRandom effects:\n Groups   Name        Std.Dev.\n batch    (Intercept) 2.789   \n Residual             2.707   \nNumber of obs: 24, groups:  batch, 6\nFixed Effects:\n (Intercept)  Pressure8700  Pressure8900  Pressure9100  \n      92.817        -1.133        -3.900        -7.050  \n\n\n\n\n",
    "preview": "posts/2021-08-06-week5-3/week5-3_files/figure-html5/unnamed-chunk-1-1.svg",
    "last_modified": "2021-10-04T16:52:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-06-week5-4/",
    "title": "Latin Squares, part 1",
    "description": "An alternative to RCBDs that works with two nuisance factors.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-07",
    "categories": [],
    "contents": "\nReadings 4.2, 4.3, Rmarkdown\nRCBD is useful when we have one nuisance factor. But what if we have two?\nWe’re testing a manufacturing procedure, but raw materials come in batches and different operators have different skills.\nWe’re testing different diets on cows over a series of days, but there will both cow and day effects.\nAssume two nuisance factors, each with \\(p\\) levels. Furthermore, assume that we care about \\(p\\) different treatments.\nThe intuition for Latin Squares is similar to the intuition for RCBD, > Make sure to run each treatment on each nuisance factor exactly once.\nFor example, in the manufacturing example, each operator should see each treatment once, and each batch of materials should be used for each treatment\nSetup\nLatin Squares are \\(p\\times p\\) arrays, filled with \\(p\\) letters, so that each letter appears exactly once in each row and each column. For example, here is a table when \\(p = 3\\).\nA\nB\nC\nB\nC\nA\nC\nA\nB\nWhy do we care? It tells us how we can implement the design idea above. First, randomly choose a \\(p\\times p\\) Latin square\nThe rows are levels of the first blocking factor.\nThe columns are levels of the second blocking factor.\nThe letters are the treatment levels\n\nThen, the experiment consists of \\(p^2\\) runs, one for each of the pairs of blocking levels, with treatment specified by the cell’s label.\nModel Setting\nInstead of just one set of block effects, we’ll have two sets, \\(\\alpha_i\\) and \\(\\beta_k\\). This results in, \\[\\begin{align*}\ny_{ijk} &= \\mu + \\alpha_i + \\tau_j + \\beta_k + \\epsilon_{ijk}\n\\end{align*}\\] where \\(\\epsilon_{ijk}\\sim \\mathcal{N}\\left(0,\\sigma^2\\right)\\) independently. Note that each of the indices \\(i, j\\) and \\(k\\) range from \\(1, \\dots, p\\).\nTo make the model identifiable, we assume \\[\n\\sum_{i = 1}^{p} \\alpha_i = \\sum_{j = 1}^{p} \\tau_j = \\sum_{k = 1}^{p} \\beta_{k} = 0\n\\]\nHypothesis Testing\nOur test hasn’t changed at all, \\[\\begin{align*}\nH_0 &: \\tau_1 = \\dots = \\tau_a = 0 \\\\\nH_{1} &: \\tau_{i} \\neq 0 \\text{ for at least one } i\n\\end{align*}\\]\nBut now we need to account for block-to-block variation across both nuisance factors. The formula isn’t pretty, but it’s exactly analogous to the decompositions we’ve seen before,\n\\[\n\\begin{aligned}\n\\sum_{i=1}^{p} \\sum_{j=1}^{p} \\sum_{k=1}^{p}\\left(y_{i j k}-\\bar{y} . . .\\right)^{2}=& p \\sum_{i=1}^{p}\\left(\\bar{y}_{i . .}-\\bar{y} . . .\\right)^{2}+\\\\\n& p \\sum_{j=1}^{p}\\left(\\bar{y}_{\\cdot j} \\cdot \\bar{y}_{\\cdots} .\\right)^{2}+\\\\\n& p \\sum_{k=1}^{p}\\left(\\bar{y}_{\\cdot \\cdot k}-\\bar{y} . .\\right)^{2}+\\\\\n& \\sum_{i=1}^{p} \\sum_{j=1}^{p} \\sum_{k=1}^{p}\\left(y_{i j k}-\\bar{y}_{i . .}-\\bar{y}_{. j .}+\\bar{y} . .\\right)^{2}\n\\end{aligned}\n\\]\nThis is abbreviated as, \\[\\begin{align*}\n  SS_{\\text{Total}} = &SS_{\\text{Rows}} + \\\\\n  &SS_{\\text{Columns}} +\\\\\n  &SS_{\\text{Treatment}} + \\\\\n  &SS_{E}\n  \\end{align*}\\] and we define\n\\(MS_{\\text{Rows}}=\\frac{1}{p - 1}SS_{\\text{Rows}}\\)\n\\(MS_{\\text{Treatment}}=\\frac{1}{p - 1}SS_{\\text{Treatment}}\\)\n\\(MS_{\\text{Columns}}=\\frac{1}{p - 1}SS_{\\text{Columns}}\\)\n\\(MS_{E}=\\frac{1}{(p - 1)(p - 2)}SS_{E}\\)\nIt turns out that \\[\n  \\frac{MS_{\\text{Treatment}}}{MS_{E}} \\sim F\\left(p - 1, \\left(p - 1\\right)\\left(p - 2\\right)\\right)\n  \\] which forms the basis for the test: reject the null if the ratio lies above the \\(1 - \\alpha\\) quantile of this \\(F\\)-distribution.\nCode Example\nWe’ll analyze the results of a study that used a latin square in its design. Compare the table below with table 4.9 in the book.\n\n\nlibrary(readr)\nlibrary(dplyr)\nrocket <- read_table2(\"https://uwmadison.box.com/shared/static/ac68766l3zcjog1ldrzki3lis74bbd71.txt\") %>%\n  mutate_at(vars(-BurningRate), as.factor) # convert all but BurningRate to factor\nrocket\n\n\n# A tibble: 25 × 4\n   Batch Operator Formulation BurningRate\n   <fct> <fct>    <fct>             <dbl>\n 1 1     1        a                    24\n 2 1     2        b                    20\n 3 1     3        c                    19\n 4 1     4        d                    24\n 5 1     5        e                    24\n 6 2     1        b                    17\n 7 2     2        c                    24\n 8 2     3        d                    30\n 9 2     4        e                    27\n10 2     5        a                    36\n# … with 15 more rows\n\nGiven this design, we can fit the model using a linear model. Here, \\(\\alpha_{i}, \\tau_{j}\\), and \\(\\beta_{k}\\) are the batch, formulation, and operator, respectively. We’ll use the shorthand y ~ . to refer to the model using all the other variables in the data frame as inputs. Compare the ANOVA table below with Table 4.12.\n\n\n#fit <- lm(BurningRate ~ Batch + Operator + Formulation, data = rocket) # gives exact same result\nfit <- lm(BurningRate ~ ., data = rocket)\nsummary(aov(fit))\n\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \nBatch        4     68   17.00   1.594 0.23906   \nOperator     4    150   37.50   3.516 0.04037 * \nFormulation  4    330   82.50   7.734 0.00254 **\nResiduals   12    128   10.67                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThere is an operator effect, but no detectable variation across batches. Controlling for batch and operater, there is a significant difference across formulations.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-04T16:52:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-06-week5-5/",
    "title": "Latin Squares, part 2",
    "description": "Extensions of Latin Squares.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-07",
    "categories": [],
    "contents": "\nReadings 4.2, 4.3, Rmarkdown\nWhen \\(p\\) is small, the test from the previous notes can have low power. If we can collect more than \\(p^2\\) samples, we should. But how exactly should the samples be collected, and how is the replicated design analyzed?\nThe design decision is context dependent,\nWe can use the same row and column levels.\nWe can keep column levels, but have different row levels.\nEquivalently, can keep rows, but different columns.\n\nWe can have different row and different column levels.\nNote that in each case, we use a different Latin square in each replicate.\n\n\n\nFigure 1: Design where all the row and column levels are reused.\n\n\n\n\n\n\nFigure 2: Design where only the column levels are reused. Pay attention to the difference in row labels from one group to the next.\n\n\n\n\n\n\nFigure 3: Design where neither the row nor column levels are reused.\n\n\n\nFortunately, the analysis is conceptually unified. We continue to have row, column, and treatment mean squares. Then, we have to add in replicate mean squares, to track variation from replicate-to-replicate. The code in each setting is the same,\nlm(Y ~ Replicate + W1 + W2 + X)\nGraceo-Latin Squares\nWe motivated Latin squares by asking how we can block 2 factors simultaneously. What if we have 3? We could go on forever…. That said, the transition from 2 to 3 is not hard\nIntroduce \\(p\\) greek letters to represent the third blocking factor. When \\(p=5\\), we would have \\(\\alpha, \\beta, \\gamma, \\delta, \\epsilon\\), for example.\nA Graeco-Latin square is like two Latin Squares overlaid on one another,\n\n\n\nFigure 4: An example graeco-latin square.\n\n\n\nwith one additional requirement: each Latin and each Greek letter must only appear together once. This last requirement is called orthogonality.\nA hypothesis test can be defined, by using the decomposition, \\[\nSS_{\\text{Total}} = SS_{\\text{Rows}} + SS_{\\text{Columns}} + SS_{\\text{Treatments}} + SS_{\\text{Greek}} + \nSS_{E}\n\\] and noting that,\n\\[\n\\frac{MS_{\\text{Treatment}}}{MS_{E}} = \\frac{\\frac{1}{p - 1}SS_{\\text{Treatment}}}{\\frac{1}{\\left(p - 1\\right)\\left(p - 3\\right)}SS_{E}} \\sim F\\left(p - 1, \\left(p - 1\\right)\\left(p - 3\\right)\\right).\n\\]\n\n\n\n",
    "preview": "posts/2021-08-06-week5-5/figure/ls-1.png",
    "last_modified": "2021-10-04T16:52:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week5-1/",
    "title": "Randomized Complete Block Design",
    "description": "Dealing with batch effects using a generalization of pairing.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-05",
    "categories": [],
    "contents": "\nReadings 4.1, Rmarkdown\nHow can we generalize the idea of pairing to the case that we have more than two treatments to compare? Suppose that the nuisance factor is known and controllable. For simplicity, also assume that the blocks have the same number \\(a\\) of samples as we have treatments (\\(a=2 \\to\\) pairing)\nExample [page 120]. A medical device manufacturer creates vascular grafts, and wants to see the effect of manufacturing pressure on defect rate. But the resin used to make the graft arrives in batches, and may itself contribute to defects.\n\nIdea: Randomly assign each of the \\(a\\) treatments to the units within each block. This is called a Randomized Complete Block Design (RCBD).\n\nThis is a restriction on randomization. Only random treatment assignments where each block has each treatment are allowed. In the medical example, we would test all the manufacturing pressures on each of the resin batches, rather than having batches that receive only subsets of pressures. The figure below shows how this relates to pairing. In the left-hand case, one of each pair of treatments (different colors) is given in each block (rectangles). The right hand case generalizes this logic to three treatments.\n\n\n\nModel Setup\nFor hypothesis testing, we need a probability model. We suppose \\[\ny_{ij} = \\mu + \\tau_i + \\beta_{j} + \\epsilon_{ij}\n\\] where \\(\\epsilon_{ij} \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\) independently and \\(\\sum_{i}\\tau_i=\\sum_{j} \\beta_j=0\\) is required to ensure identifiability.\n\n\n\nFigure 1: An example of a dataset generated by the model with block effects.\n\n\n\nAs before,\n\\(\\mu\\) is a global mean\n\\(\\tau_i\\) is the treatment effect for the \\(i^{th}\\) treatment level\n\\(\\epsilon_{ij}\\) is the error term\nBut now we also have * \\(\\beta_{j}\\), the effect of the \\(j^{th}\\) block\nNotice that in each block, we must have exactly \\(a\\) samples, one from each treatment.\n\n\n\nFigure 2: Annotation of the previous dataset using terms from the model.\n\n\n\nHypothesis Testing\nWe’re interested in whether any treatments have an effect, \\[\\begin{align*}\nH_0 &: \\tau_1 = \\dots = \\tau_a = 0 \\\\\nH_{1} &: \\tau_{i} \\neq 0 \\text{ for at least one } i\n\\end{align*}\\]\nThere’s a useful decomposition of the total variance that accounts for block-to-block variation, \\[\\begin{align*}\n\\sum_{i = 1}^{a}\\sum_{j = 1}^{b}\\left(y_{ij} - \\bar{y}_{\\cdot\\cdot}\\right)^2 = &b\\sum_{i = 1}^{a} \\left(\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot}\\right)^2 + \\\\\n& a \\sum_{j = 1}^{b}\\left(\\bar{y}_{\\cdot j} - \\bar{y}_{\\cdot\\cdot}\\right)^2 + \\\\\n&\\sum_{i = 1}^{a}\\sum_{j = 1}^{b} \\left(y_{ij} - \\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot j} + \\bar{y}_{\\cdot\\cdot}\\right)^2\n\\end{align*}\\] We’ll abbreviate this as \\(SS_{\\text{Total}} = SS_{\\text{Treatment}} + SS_{\\text{Block}} + SS_{E}\\).\nThe corresponding mean squares are,\n\\(MS_{\\text{Total}}= \\frac{1}{N - 1}SS_{\\text{Total}}\\), which is the sample variance of all \\(y_{ij}\\)\n\\(MS_{\\text{Treatment}}=\\frac{1}{a - 1}SS_{\\text{Treatment}}\\)\n\\(MS_{\\text{Block}}=\\frac{1}{b - 1}SS_{\\text{Block}}\\)\n\\(MSE=\\frac{1}{(a - 1)(b - 1)}SS_{E}\\), which a good estimator for \\(\\sigma^2\\)\nAs in usual ANOVA, if \\(MS_{\\text{Treatment}}\\) is much larger than MSE, then we have evidence against the null hypothesis. In fact, it turns out that, under the null,\n\\[\n\\frac{MS_{\\text{Treatment}}}{MS_{E}} \\sim F\\left(a - 1, \\left(a - 1\\right)\\left(b - 1\\right)\\right)\n\\] so we can reject the null hypothesis if this statistic is larger than the \\(1 - \\alpha\\) quantile of an \\(F\\) distribution with \\((a - 1, (a - 1)(b - 1))\\) d.f.\nCode Example\nIn this code example, we’ll study a vascular graft dataset. We want to know the effect of pressure on yield, controlling for possible batch effects. We first download the data and reshape it so that batches are not spread across columns. Judging from a plot, there appear to be batch effects. For example, batch 5 always has lower yield than batch 6, across all pressure levels. There nonetheless appears to be a possible pressure effect. For example, runs with pressure level 9100 (the purple points) always seem to have lower yield.\n\n\n\n\n\nlibrary(readr)\nlibrary(tidyr)\ngraft <- read_table2(\"https://uwmadison.box.com/shared/static/0ciblk4z2f3k6zizbj4plg3q33w1d0n6.txt\") %>%\n  pivot_longer(-Pressure, names_to = \"batch\", values_to = \"yield\")\ngraft$Pressure <- as.factor(graft$Pressure)\n\nggplot(graft) +\n  geom_point(aes(x = batch, y = yield, col = Pressure)) +\n  scale_color_brewer(palette = \"Set2\") # custom color scheme\n\n\n\n\nWe can use the lm function to fit the RCBD model. As before, the response goes on the left hand side, but now the right hand side includes both the variable of interest (Pressure) and an indicator of which batch each sample came from (batch). We can build an ANOVA table for the expanded table using the same aov table as before.\n\n\nfit <- lm(yield ~ Pressure + batch, graft)\nsummary(aov(fit))\n\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \nPressure     3  178.2   59.39   8.107 0.00192 **\nbatch        5  192.2   38.45   5.249 0.00553 **\nResiduals   15  109.9    7.33                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIt appears that there is a significant batch effect. Controlling for this effect, we are able to detect a significant relationship between pressure and yield. Note that the results are identical to those in Figure 4.2 of the textbook.\nIf we hadn’t controlled for the batch effect, we would have still seen this relationship, but we would have lower power (the \\(p\\)-value is not as low here as it had been before).\n\n\nsummary(aov(lm(yield ~ Pressure, graft)))\n\n\n            Df Sum Sq Mean Sq F value Pr(>F)  \nPressure     3  178.2   59.39   3.931 0.0234 *\nResiduals   20  302.1   15.11                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n",
    "preview": "posts/2021-08-03-week5-1/week5-1_files/figure-html5/unnamed-chunk-4-1.svg",
    "last_modified": "2021-10-04T16:52:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-06-week5-2/",
    "title": "RCBD Diagnostics",
    "description": "Multiple comparison and model checks for RCBDs",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-10-05",
    "categories": [],
    "contents": "\nReadings 4.1, Rmarkdown\nMultiple Comparisons\nAs before, we may want to use contrasts to decide on which treatment effects are different We can continue to use the same multiple comparisons procedures as before, but have to account for a few differences,\nThe number of samples per treatment \\(n\\) is replaced by the number of blocks \\(b\\).\nThe d.f. for \\(MS_{E}\\) is now \\((a - 1)(b - 1)\\), not \\(N - a\\).\n\n\n\n\nFor example, the cutoff in Fisher’s LSD becomes \\[\nt_{\\frac{\\alpha}{2}, \\left(a - 1\\right)\\left(b - 1\\right)}\\sqrt{\\frac{2 MS_{E}}{b}}\n\\]\nModel Diagnostics\nThere are two key assumptions when we use RCBDs,\n\\(\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\) are independently distributed. Note that this also implies homoskedasticity (the variances are not changing from sample to sample).\nAdditivity (i.e. “no interaction”). The treatment effects \\(\\tau_i\\) need to be the same across all blocks.\n\n\n\n\nFigure 1: An example where the additivity assumption fails.\n\n\n\nWe’ve seen the first assumption before, and can continue to use normal probability plots and residual analysis to check it.\nOne way to check additivity is to look at residuals, and see whether they are consistently lower / higher in some blocks.\nWhat can we do if we find an interaction effect?\nSometimes the interaction effects will disappear after transforming the response (e.g., using \\(\\log\\) or \\(\\sqrt{}\\))\nOtherwise, another design may be necessary. Factorial designs (to be discussed soon) allow for inference even in the presence of interactions.\n\nCode Example\nWe will continue the graft example from the previous notes. First, let’s refit the RCBD model.\n\n\n\n\n\nlibrary(readr)\nlibrary(tidyr)\ngraft <- read_table2(\"https://uwmadison.box.com/shared/static/0ciblk4z2f3k6zizbj4plg3q33w1d0n6.txt\") %>%\n  pivot_longer(-Pressure, names_to = \"batch\", values_to = \"yield\")\ngraft$Pressure <- as.factor(graft$Pressure)\nfit <- lm(yield ~ Pressure + batch, graft)\naov_table <- aov(fit)\n\n\n\nWe can fit contrasts and correct for multiple comparisons using the same type of code as for ANOVA without batch effects.\n\n\nlibrary(gmodels)\nlibrary(DescTools)\n\ncontrasts <- matrix(\n  c(1, 1, -1, -1,\n    1, 0, 0, -1),\n  nrow = 2, byrow=TRUE\n)\n\nfit.contrast(aov_table, \"Pressure\", contrasts)\n\n\n                         Estimate Std. Error  t value     Pr(>|t|)\nPressure c=( 1 1 -1 -1 ) 9.816667   2.209940 4.442052 0.0004751988\nPressure c=( 1 0 0 -1 )  7.050000   1.562663 4.511528 0.0004136854\nattr(,\"class\")\n[1] \"fit_contrast\"\n\nPostHocTest(aov_table, method = \"scheffe\", contrasts = contrasts)\n\n\n\n  Posthoc multiple comparisons of means: Scheffe Test \n    95% family-wise confidence level\n\n$Pressure\n                           diff     lwr.ci     upr.ci   pval    \n8500,8700,8900,9100-  184.50000  177.31746  191.68254 <2e-16 ***\n8500,8900-             88.91667   83.83785   93.99549 <2e-16 ***\n-8500,8900            -92.81667  -97.89549  -87.73785 <2e-16 ***\n-8500,8700,8900,9100 -174.68333 -181.86587 -167.50080 <2e-16 ***\n\n$batch\n                                               diff     lwr.ci\nBatch1,Batch2,Batch3,Batch4,Batch5,Batch6-  177.450  168.65322\nBatch1,Batch3,Batch5-                        91.000   84.77974\n-Batch1,Batch3,Batch5                       -85.325  -91.54526\n-Batch1,Batch2,Batch3,Batch4,Batch5,Batch6 -177.450 -186.24678\n                                               upr.ci   pval    \nBatch1,Batch2,Batch3,Batch4,Batch5,Batch6-  186.24678 <2e-16 ***\nBatch1,Batch3,Batch5-                        97.22026 <2e-16 ***\n-Batch1,Batch3,Batch5                       -79.10474 <2e-16 ***\n-Batch1,Batch2,Batch3,Batch4,Batch5,Batch6 -168.65322 <2e-16 ***\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(aov_table)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = fit)\n\n$Pressure\n               diff        lwr       upr     p adj\n8700-8500 -1.133333  -5.637161  3.370495 0.8854831\n8900-8500 -3.900000  -8.403828  0.603828 0.1013084\n9100-8500 -7.050000 -11.553828 -2.546172 0.0020883\n8900-8700 -2.766667  -7.270495  1.737161 0.3245644\n9100-8700 -5.916667 -10.420495 -1.412839 0.0086667\n9100-8900 -3.150000  -7.653828  1.353828 0.2257674\n\n$batch\n                diff         lwr        upr     p adj\nBatch2-Batch1  2.050  -4.1680828  8.2680828 0.8853016\nBatch3-Batch1  3.300  -2.9180828  9.5180828 0.5376297\nBatch4-Batch1  2.850  -3.3680828  9.0680828 0.6757699\nBatch5-Batch1 -2.375  -8.5930828  3.8430828 0.8105903\nBatch6-Batch1  6.750   0.5319172 12.9680828 0.0297368\nBatch3-Batch2  1.250  -4.9680828  7.4680828 0.9845521\nBatch4-Batch2  0.800  -5.4180828  7.0180828 0.9980198\nBatch5-Batch2 -4.425 -10.6430828  1.7930828 0.2483499\nBatch6-Batch2  4.700  -1.5180828 10.9180828 0.1986961\nBatch4-Batch3 -0.450  -6.6680828  5.7680828 0.9998784\nBatch5-Batch3 -5.675 -11.8930828  0.5430828 0.0837504\nBatch6-Batch3  3.450  -2.7680828  9.6680828 0.4925715\nBatch5-Batch4 -5.225 -11.4430828  0.9930828 0.1263042\nBatch6-Batch4  3.900  -2.3180828 10.1180828 0.3674672\nBatch6-Batch5  9.125   2.9069172 15.3430828 0.0027838\n\nTo check the normality and additivity assumptions, we can plot the residuals against the batch. Even though additivity doesn’t seem to be a problem (no group of residuals is consistently \\(> 0\\) or \\(< 0\\)), equality of variance across batches seems to be violated. The variance of Batch 6’s residuals is too small.\n\n\nlibrary(ggplot2)\ngraft$residual <- resid(fit)\nggplot(graft) +\n  geom_point(aes(x = batch, y = residual, col = Pressure)) +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n",
    "preview": "https://uwmadison.box.com/shared/static/dmob9sjh8my7l8hez390o7gnn3vu1oe6.png",
    "last_modified": "2021-10-04T16:55:53-05:00",
    "input_file": "week5-2.knit.md"
  },
  {
    "path": "posts/2021-08-03-week4-2/",
    "title": "Fitting Random Effects",
    "description": "Using the method of moments or maximum likelihood to estimate parameters",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-30",
    "categories": [],
    "contents": "\nReadings 3.9, Rmarkdown\nIn random effects models, there are three key parameters: \\(\\mu, \\sigma^2_{\\tau}, \\sigma^2\\). Two ways to fit them are (1) the method of moments and (2) maximum likelihood. The different approaches to estimation have different properties, so even though knowing the intricate details of the computation is not necessary, having a basic understanding of how the approaches work can help make sure we use appropriate software implementations.\nMethod of Moments\nFor estimating \\(\\mu\\), this method uses the overall mean \\(\\bar{y}\\).\nWhat about the \\(\\sigma^2\\)’s? The key identity is, \\[\\begin{align*}\n\\mathbf{E}\\left[MS_{\\text{treatments}}\\right] &= \\sigma^2 + n \\sigma_{\\tau}^2 \\\\\n\\mathbf{E}\\left[MS_{E}\\right] &= \\sigma^2\n\\end{align*}\\]\nWe can approximate the expected values using, \\[\\begin{align*}\nMS_{\\text{treatment}} \\approx \\sigma^2 + n \\sigma_{\\tau}^2 \\\\\nMS_{E} \\approx \\sigma^2\n\\end{align*}\\]\nIf we pretended these approximations were exact equalities, then we have two equations with two unknowns. The method of moments defines parameter estimates as the solutions to that system of equations. \\[\\begin{align}\n\\hat{\\sigma}^2 &= MS_{E} \\\\\n\\hat{\\sigma}^2_{\\tau} &= \\frac{1}{n}\\left[MS_{\\text{treatment}} - MS_{E}\\right]\n\\end{align}\\]\nHow can we get confidence intervals for these estimates?\nFor \\(\\hat{\\mu} = \\bar{y}\\), we can use \\(\\text{Var}\\left(y\\right) = \\frac{1}{N} \\text{Var}\\left(y_{ij}\\right)=\\frac{1}{N}\\left(\\sigma^2 + n\\sigma^2_{\\tau}\\right)\\)\nFor \\(\\hat{\\sigma}^2\\), we can use the fact that \\(\\frac{N - a}{\\sigma^2}MS_{E} \\sim \\chi^2_{N - a}\\).\nFor \\(\\hat{\\sigma}^2_{\\tau}\\), we’re out of luck, though some papers give approximate confidence intervals.\n\nMaximum Likelihood Estimation\nAn alternative approach is to use maximum likelihood. The first step is to stack all the \\(y_{ij}\\)’s into one long length \\(N\\) vector, and observe that the data are jointly normally distributed, \\[\n\\left(\\begin{array}{c}\ny_{11} \\\\\ny_{12} \\\\\n\\vdots \\\\\ny_{a(n-1)} \\\\\ny_{a n}\n\\end{array}\\right) \\sim \\mathcal{N}\\left(\\mu 1_{N},\\left(\\begin{array}{cccc}\n\\sigma^{2} I_{n}+\\sigma_{\\tau}^{2} 1_{n} 1_{n}^{T} & \\mathbf{0} & \\ldots & \\mathbf{0} \\\\\n\\mathbf{0} & \\sigma^{2} I_{n}+\\sigma_{\\tau}^{2} 1_{n} 1_{n}^{T} & \\ldots & \\mathbf{0} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n\\mathbf{0} & \\mathbf{0} & \\ldots & \\sigma^{2} I_{n}+\\sigma_{\\tau}^{2} 1_{n} 1_{n}^{T}\n\\end{array}\\right)\\right)\n\\] Here, \\(1_{n}\\) refers to a vector of \\(n\\) 1’s stacked in a column and \\(I_{n}\\) refers to the \\(n\\times n\\) identity matrix.\n\n\n\nThe specific form of the covariance isn’t important. What is important is that we can exactly evaluate the probability of \\(y_{11}, y_{12},\\dots, y_{an}\\) under any choice of the parameters \\(\\mu, \\sigma^2, \\sigma^2_{\\tau}\\).\nDefine \\(L(\\mu, \\sigma^2, \\sigma^2_{\\tau})\\) to be the probability of the dataset \\(y_{11}, y_{12}, \\dots y_{an}\\) viewed as a function of the normal distribution’s parameters. A good estimate for these parameters comes from finding the configuration that maximizes \\(L\\left(\\mu, \\sigma^2, \\sigma^2_{\\tau}\\right)\\). The maximizers can’t be found analytically, but algorithms exist to find the maximizers.\n\n\n\nSoftware also gives a confidence interval for the estimates. It works by studying the curvature of the likelihood function at the maximizer.\n\n\n\nFigure 1: A highly peaked likelihood surface will give a smaller confidence interval, because there is a smaller set parameters that gives a good probability to the observed data.\n\n\n\nCode Implementation\nWhich method should we use in practice? There are a few considerations,\nIf confidence intervals are necessary for more than just \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}^2\\), then maximum likelihood should be used.\nIt can turn out that the method of moments returns negative values for \\(\\hat{\\sigma}^2\\). It is solving an algebraic equation and has no prior knowledge that variances should always be positive. If a negative variance is likely to be problematic, then maximum likelihood should be used.\nMaximum likelihood is slow, because it requires an iterative optimization. The method of moments only requires the solution to a system of equations, though, and so is very fast. It is often the method of choice in large scale problems.\nThe lme4 package implements the maximum likelihood approach to random effects model estimation; this package is reviewed in the previous notes.\nTo estimate the method of moments, we can do all the calculations by hand – we just need the results from an ANOVA table.\n\n\nlibrary(readr)\nlibrary(broom)\nloom <- read_csv(\"https://uwmadison.box.com/shared/static/ezp3i2pflhi96si7u6rfn3dg3lb5cl3z.csv\")\nloom$loom <- as.factor(loom$loom)\nN <- nrow(loom)\na <- nlevels(loom$loom)\nn <- N / a\n\naov_table <- aov(lm(strength ~ loom, data = loom)) %>%\n  tidy()\n\n\n\nThe code below computes point estimates for \\(\\sigma^2\\) and \\(\\sigma_{\\tau}^2\\) based on the formulas above, using the MSE quantities available in the ANOVA table. It also computes the intraclass correlation coefficient (ICC), which is defined in the reading.\n\n\nsigma_sqs <- vector(length = 2)\nsigma_sqs[1] <- aov_table$meansq[2] # estimate for sigma^2\nsigma_sqs[2] <- (aov_table$meansq[1] - aov_table$meansq[2]) / n # estimate for sigma^2_tau\n\nsigma_sqs\n\n\n[1] 1.895833 6.958333\n\nsigma_sqs[2] / sum(sigma_sqs) # ICC\n\n\n[1] 0.7858824\n\nWe can also use the formulas above to compute confidence intervals for \\(\\hat{\\sigma}^2\\). We use a formula from the book to build an analogous interval for the ICC.\n\n\nint_bounds <- c(0.975, 0.025)\n(N - a) * sigma_sqs[1] / qchisq(int_bounds, N - a) # CI for sigma^2\n\n\n[1] 0.9748608 5.1660065\n\nratio_bounds <- 1 / n * (aov_table$statistic[1] / qf(int_bounds, a - 1, N - a) - 1)\nratio_bounds / (1 + ratio_bounds) # CI for ICC\n\n\n[1] 0.3850736 0.9824420\n\n\n\n\n",
    "preview": "posts/2021-08-03-week4-2/week4-2_files/figure-html5/unnamed-chunk-1-1.svg",
    "last_modified": "2021-10-04T16:31:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week4-3/",
    "title": "Nonparametric ANOVA",
    "description": "A model-free alternative to ANOVA.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-30",
    "categories": [],
    "contents": "\nReadings 3.11, Rmarkdown\nWhat can we do when the errors seem very far from normally distributed? Imagine all attempts to transform the data have failed. An alternative is to use nonparametric ANOVA. The figure below gives an example of highly skewed data, for which a standard ANOVA might not be appropriate.\n\n\n\nThe intuition is that, if all the groups have the same means, then no group should be consistently ranked higher than another. Consider hte figure below. In the left case, there seems to be a difference in the groups, just by looking at the ranks. In the right, the groups seem more or less comparable.\n\n\n\nTo implement this idea quantitatively,\nTransform the data to their ranks. The smallest of the \\(y_{ij}\\) becomes 1, the next smallest becomes 2, etc. Denote these ranks by \\(R_{ij}\\).\nCompute the test statistic \\[\n  \\frac{1}{S^2}\\left[\\sum_{i = 1}^{a} \\frac{R_{i\\cdot}^2}{n_{i}} - \\frac{N\\left(N + 1\\right)^2}{4}\\right]\n  \\] where we define \\(R_{i\\cdot}\\) to be the sum of the ranks in the \\(i^{th}\\) group, and \\[\n  S^2 = \\frac{1}{N - 1} \\left[\\sum_{i, j} R_{ij}^2 - \\frac{N\\left(N + 1\\right)^2}{4}\\right].\n  \\]\nCompare the test statistic to a \\(\\chi^2_{a -1}\\) reference distribution. If it seems too large to be plausible, reject the null hypothesis.\n\nWhere did this test statistic come from? It’s possible to show that the statistic is equivalent to \\[\n  \\frac{\\sum_{i} n_{i}\\left(\\bar{R}_{i} - \\bar{R}\\right)^2}{\\frac{1}{N - 1}\\sum_{i, j} \\left(R_{ij} - \\bar{R}\\right)^2}\n  \\] which compares the average rank in group \\(i\\) to the average rank overall, and standardizes by the overall variance of the ranks. The first formula is the one presented in the book, though, and it’s easier to calculate by hand.\nWhy not always use nonparametric ANOVA? If the data are actually normal, than this approach has less power than standard ANOVA. If you have doubts about validity, a safe approach is to try both. If the approaches approximately agree, default to standard ANOVA.\nCode Example\nThis test is implemented by the kruskal.test function. It expects input in the same form as lm in the earlier ANOVA examples. Below, we apply the test to the etch rate data. The \\(p\\)-value indicates that the groups have significantly different ranks, which is consistent with our previous findings.\n\n\nlibrary(readr)\netch_rate <- read_csv(\"https://uwmadison.box.com/shared/static/vw3ldbgvgn7rupt4tz3ditl1mpupw44h.csv\")\netch_rate$power <- as.factor(etch_rate$power) # want to think of power as distinct groups\nkruskal.test(rate ~ power, data = etch_rate)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  rate by power\nKruskal-Wallis chi-squared = 16.907, df = 3, p-value =\n0.0007386\n\n\n\n\n",
    "preview": "https://uwmadison.box.com/shared/static/a2evjeib97ay362iue2vhkxk18gbh7l4.png",
    "last_modified": "2021-10-04T16:31:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week4-1/",
    "title": "Random Effects",
    "description": "An introduction to random effects models",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-28",
    "categories": [],
    "contents": "\nReadings 3.9, Rmarkdown\nSometimes a factor has so many levels, that we can’t collect observations for all of them. Or, even if we could collect them, having one parameter for each would lead to a clumsy model.\nIn this case, we typically settle for saying the effect of the factor on average, rather than than trying to estimating the effects of every single level of that factor.\nExamples,\nIs there a middle school effect on high school graduation? (instead of effects for individual schools)\nIs there a loom effect on fiber strength? (don’t care about individual looms)\nIs there a microbiome effect on preterm births?\nIs there a county effect on election outcomes?\n\n\n\n\nFigure 1: In random effects, the groups (schools, looms, …) we observe are assumed sampled from a larger population.\n\n\n\nModel\nRandom effects models have the form, \\[\\begin{align}\ny_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\n\\end{align}\\] where \\(\\tau_i \\sim \\mathcal{N}\\left(0, \\sigma_\\tau^2\\right)\\) and \\(\\epsilon_{ij} \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\). The crucial difference is that \\(\\tau_i\\) is now thought of as random, not fixed.\n\n\n\nNotice that \\[\\begin{align}\n\\text{Var}\\left(y_{ij}\\right) &= \\text{Var}\\left(\\tau_j\\right) + \\text{Var}\\left(\\epsilon_{ij}\\right) \\\\\n&= \\sigma_{\\tau}^2 + \\sigma^2\n\\end{align}\\] More generally, the covariance matrix is block diagonal, with blocks of \\(\\sigma_{\\tau}^2\\) within groups.\n\n\n\nHypothesis Testing\nWe may want to test whether there is any variation in response across factor levels. Formally, \\[\\begin{align*}\nH_0:& \\sigma_\\tau^2 = 0 \\\\\nH_1:& \\sigma^2 > 0\n\\end{align*}\\]\n\n\n\nFigure 2: Under the null, there is no difference between any of the groups.\n\n\n\nFor our test statistic, we can use the same one as before, \\(\\frac{MS_{\\text{treatments}}}{MS_{E}}\\) and for the same reasons as in fixed-effect ANOVA, this is \\(F\\)-distributed with \\((a - 1, N - a)\\) d.f. – we reject the null when this quantity is large.\nCode Example\nWe will illustrate this method on a dataset about the strength of looms in a factory. The block below reads in the data.\n\n\nlibrary(readr)\nloom <- read_csv(\"https://uwmadison.box.com/shared/static/ezp3i2pflhi96si7u6rfn3dg3lb5cl3z.csv\")\nloom$loom <- as.factor(loom$loom)\nloom\n\n\n# A tibble: 16 × 3\n   loom  replicate strength\n   <fct> <chr>        <dbl>\n 1 1     Ob1             98\n 2 2     Ob1             91\n 3 3     Ob1             96\n 4 4     Ob1             95\n 5 1     Ob2             97\n 6 2     Ob2             90\n 7 3     Ob2             95\n 8 4     Ob2             96\n 9 1     Ob3             99\n10 2     Ob3             93\n11 3     Ob3             97\n12 4     Ob3             99\n13 1     Ob4             96\n14 2     Ob4             92\n15 3     Ob4             95\n16 4     Ob4             98\n\nTo fit a random effects model, we can use the lmer function in the lme4 package. The syntax (1 | variable) means that this variable should be treated as a random effect. Compare the result with Example 3.10 in the textbook.\n\n\nlibrary(lme4)\nfit <- lmer(strength ~ (1 | loom), data = loom)\nsummary(fit)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: strength ~ (1 | loom)\n   Data: loom\n\nREML criterion at convergence: 63.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.38018 -0.57260 -0.04342  0.82574  1.52491 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n loom     (Intercept) 6.958    2.638   \n Residual             1.896    1.377   \nNumber of obs: 16, groups:  loom, 4\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   95.437      1.363   70.01\n\nThe most important quantities in this computer output are entries of the Variance column. These are the \\(\\sigma^2_{\\tau}\\) and \\(\\sigma^2\\) quantities in the model above; they are sometimes called “variance components.” We would interpret this result as meaning that there is quite a large variation in strength from one loom to the next. Even though the average fabric strength is around 95.4 across all looms (the Intercept field), if you drew a new loom, its typical fabric strength might be several points higher or lower. Precisely, the distribution of loom mean strengths is approximately \\(\\mathcal{N}\\left(95, 7\\right)\\). Within any given loom, though, the strength is not too variable, with a variance of only \\(\\approx 2\\).\n\n\n\n",
    "preview": "posts/2021-08-03-week4-1/week4-1_files/figure-html5/unnamed-chunk-2-1.svg",
    "last_modified": "2021-10-04T16:31:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week3-3/",
    "title": "Contrasts",
    "description": "Making pointed comparisons between treatment levels in ANOVA",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-23",
    "categories": [],
    "contents": "\nReadings 3.5, Rmarkdown\nWhen we reject the null in ANOVA, we know at least one of the treatments deviates from the global average. But which one(s)? Contrasts address this question. A contrast is a linear combination of the means, \\[\n \\Gamma(c)=\\sum_{i=1}^{a} c_{i} \\mu_{i}\n \\] For any particular \\(c\\), we test \\[\n \\begin{aligned}\n &H_{0}: \\Gamma(c)=0 \\\\\n &H_{1}: \\Gamma(c) \\neq 0\n \\end{aligned}\n \\]\n\n\n\nFigure 1: Is there a difference between group 2 and the average of groups 1 and 3?\n\n\n\nTo motivate this, suppose we have 4 different means, \\(\\mu_1, \\mu_2, \\mu_3, \\mu_4\\),\nc = (1, 1, -1, -1): Are the first two means different from the last two, on average?\nc = (1, -1, 0, 0): Are the first two means equal to each other?\netc.\n\nTesting Contrasts\nRecall the hypothesis testing recipe. We need, (a) a test statistic and (b) a reference distribution for that statistic. Our best guess at \\(\\mu_i\\) is \\(\\bar{y}_i\\), so a reasonable statistic is, \\[\n  \\hat{\\Gamma}(c)=\\sum_{i=1}^{a} c_{i} \\bar{y}_{i}\n  \\]\nHow will we find its reference distribution? Under the null, this statistic is normally distributed with mean 0 and variance, \\[\n \\begin{aligned}\n \\operatorname{Var}(\\hat{\\Gamma}(c)) &=\\sum_{i=1}^{a} c_{i}^{2} \\operatorname{Var}\\left(\\bar{y}_{i}\\right) \\\\\n &=\\frac{\\sigma^{2}}{n} \\sum_{i=1}^{a} c_{i}^{2}\n \\end{aligned}\n \\] Standardizing our original statistic, we obtain,\n\\[\\begin{aligned}\n \\frac{\\hat{\\Gamma}(c)}{\\sqrt{\\operatorname{Var}(\\hat{\\Gamma}(c))}} &=\\frac{\\sum_{i=1}^{a} c_{i} \\bar{y}_{i}}{\\sqrt{\\frac{\\sigma^{2}}{n} \\sum_{i=1}^{a} c_{i}^{2}}} \\\\\n & \\approx \\frac{\\sum_{i=1}^{a} c_{i} \\bar{y}_{i}}{\\sqrt{\\frac{\\hat{\\sigma}^{2}}{n} \\sum_{i=1}^{a} c_{i}^{2}}}\n \\end{aligned}\\]\nTo estimate \\(\\sigma^2\\), we can use \\(\\hat{\\sigma}^2 := MS_E\\). This is a good choice, because it remains valid even when the null is untrue.\nSince we plugged-in the estimate \\(\\hat{\\sigma}^2\\), we have divided our normal distribution by the square root of a chi-square. Therefore, the reference distribution is a t-distribution with \\(N - a\\) df.\nConfidence Intervals for Contrasts\nIf we make the same computations as above, but without assuming that the null is true, we would find that, \\[\n \\mathbf{P}\\left(\\frac{\\sum_{i=1}^{a} c_{i} \\bar{y}_{i}-\\sum_{i=1}^{a} c_{i} \\mu_{i}}{\\sqrt{\\frac{\\hat{\\sigma}^{2}}{n} \\sum_{i=1}^{a} c_{i}^{2}}} \\in\\left[t_{\\text {left }}, t_{\\text {right }}\\right]\\right)=0.95\n \\] where we choose \\(t_{\\text{left}}\\) and \\(t_{\\text{right}}\\) to be the 0.025 and 0.975 quantiles of a \\(t\\)-distribution with \\(N - a\\) df.\nThe resulting confidence interval is, \\[\n \\left[\\sum_{i=1}^{a} c_{i} \\bar{y}_{i}-t_{\\text {right }} \\sqrt{\\frac{\\hat{\\sigma}^{2}}{n} \\sum_{i=1}^{a} c_{i}^{2}}, \\sum_{i=1}^{a} c_{i} \\bar{y}_{i}+t_{\\text {right }} \\sqrt{\\left.\\frac{\\hat{\\sigma}^{2}}{n} \\sum_{i=1}^{a} c_{i}^{2}\\right]}\\right.\n \\] This is an explicit formula that you can use in your computations, but don’t let the complexity of the symbols here confuse you. Returning to our original definitions, this is just, \\[\n \\left[\\hat{\\Gamma}(c)-t_{\\mathrm{left}} \\sqrt{\\widehat{\\operatorname{Var}}(\\hat{\\Gamma}(c))}, \\hat{\\Gamma}(c)+t_{\\mathrm{left}} \\sqrt{\\widehat{\\operatorname{Var}}(\\hat{\\Gamma}(c))}\\right]\n \\] which is our point estimate plus and minus \\(t_{\\text{left}}\\) standard deviations (we’re writing \\(\\hat{\\text{Var}}\\) instead of \\(\\text{Var}\\) because we don’t know the true variance and have plugged in the estimate \\(\\hat{\\sigma}^2)\\).\nCode Example\nWe will continue the etch rate example. Let’s refit the same model from before.\n\n\nlibrary(readr)\netch_rate <- read_csv(\"https://uwmadison.box.com/shared/static/vw3ldbgvgn7rupt4tz3ditl1mpupw44h.csv\")\netch_rate$power <- as.factor(etch_rate$power) # consider as discrete levels\nfit <- lm(rate ~ power, data = etch_rate)\n\n\n\nLet’s say we’re interested in the contrast between power levels of 160 and 180. We have to encode the contrast as a vector, and then pass it to the fit.contrast function. The choice c(1, -1, 0, 0) comes from the fact that we want the difference between the first two power levels (160 and 180) and are ignoring all the rest (the two 0’s at the end).\n\n\nlibrary(gmodels)\ncontrast <- c(1, -1, 0, 0)\nfit.contrast(fit, \"power\", contrast)\n\n\n                     Estimate Std. Error   t value    Pr(>|t|)\npower c=( 1 -1 0 0 )    -36.2   11.55335 -3.133289 0.006416224\nattr(,\"class\")\n[1] \"fit_contrast\"\n\nWe can get a confidence interval using the conf.int parameter.\n\n\nfit.contrast(fit, \"power\", contrast, conf.int = 0.95)\n\n\n                     Estimate Std. Error   t value    Pr(>|t|)\npower c=( 1 -1 0 0 )    -36.2   11.55335 -3.133289 0.006416224\n                      lower CI  upper CI\npower c=( 1 -1 0 0 ) -60.69202 -11.70798\nattr(,\"class\")\n[1] \"fit_contrast\"\n\n\n\n\n",
    "preview": "posts/2021-08-03-week3-3/week3-3_files/figure-html5/unnamed-chunk-1-1.svg",
    "last_modified": "2021-10-04T16:31:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week3-4/",
    "title": "Multiple Comparisons",
    "description": "The multiple comparisons problem and some solutions.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-23",
    "categories": [],
    "contents": "\nReadings 3.5, Rmarkdown\nIf we planned in advance which contrasts we want to use, we are fine. But what if we hadn’t planned any, and go in search for significant results using different contrasts?\nImagine testing 100 hypothesis at = 0.05. Suppose they are all null). We would see 5 rejected null hypotheses on average.\nTherefore, if we want to allow ourselves some flexibility in searching over contrasts, we need to adapt our methodology. We should control the experimentwise error rate, the probability that any test results in a false positive.\nScheffe’s Method\nLet’s say we’re interested in \\(m\\) contrasts, \\(c_1, \\cdots ,c_m\\). The idea is to widen our confidence intervals slightly, to make false positives rarer.\nHow much should the intervals be widened? It’s not obvious, but Scheffe found that we should multiply the endpoints of each of our \\(m\\) intervals by \\[\n\\sqrt{\\left(a - 1\\right)F_{0.025, a - 1, N - a}}\n\\] (this is for 95% confidence intervals).\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/tmez3gdyre3lth822zm2wimmes3nujyy.png\")\n\n\n\n\nTukey’s Method\nIf we only care about the differences between pairs of group means, we can use Tukey’s method. All the contrasts now have the form, \\[\n\\Gamma\\left(c\\right) = \\mu_i - \\mu_j\n\\]\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/lc34pprk90p7evabiycjiw5c22xsm666.png\")\n\n\n\n\nWe’re going to make confidence intervals for these, and it’s natural to center them around, \\[\\begin{align*}\n\\hat{\\Gamma}\\left(c\\right) &= \\bar{y}_i - \\bar{y}_j.\n\\end{align*}\\]\nHow wide should the intervals be? Tukey found a reference distribution for \\[\n\\frac{\\bar{y}_{\\max }-\\bar{y}_{\\min }}{\\frac{\\hat{\\sigma}}{\\sqrt{n}}}\n\\] where \\(\\bar{y}_{\\text{max}}\\) refers to the maximum group’s average across the \\(a\\) groups. From there, he tabulated the quantiles as \\(q_{\\alpha}(a, \\text{df})\\).\nIt turns out that the appropriate width of the confidence intervals can be derived from these quantiles, \\[\n\\left[\\left(\\bar{y}_{i}-\\bar{y}_{j}\\right)-q_{\\alpha}(a, \\text{df}) \\frac{\\hat{\\sigma}}{\\sqrt{n}},\\left(\\bar{y}_{i}-\\bar{y}_{j}\\right)+q_{\\alpha}(a, \\text{df}) \\frac{\\hat{\\sigma}}{\\sqrt{n}}\\right]\n\\]\n\n\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/alo5fgwmkwrniwejs32ow05carx9uwna.png\"))\n\n\n\n\nThis works because if the difference between the max and min group averages is contained within this interval, then all pairs \\(i, j\\) of differences are also contained in this interval.\nFisher’s Least Significant Difference\nFisher’s LSD is used to compare pairs of means. Unlike Tukey’s method, it doesn’t control the experimentwise error rate\nNotice that the variance of the differences is \\[\n\\begin{aligned}\n\\operatorname{Var}\\left(\\bar{y}_{i}-\\bar{y}_{j}\\right) &=\\operatorname{Var}\\left(\\bar{y}_{i}\\right)+\\operatorname{Var}\\left(\\bar{y}_{j}\\right) \\\\\n&=\\frac{\\sigma^{2}}{n_{i}}+\\frac{\\sigma^{2}}{n_{j}} \\\\\n& \\approx \\hat{\\sigma}^{2}\\left(\\frac{1}{n_{i}}+\\frac{1}{n_{j}}\\right)\n\\end{aligned}\n\\]\nFisher’s LSD compares each difference \\(\\left|y_- y_j\\right|\\) to the cutoff, \\[\nt_{\\text {right }} \\sqrt{\\hat{\\sigma}^{2}\\left(\\frac{1}{n_{i}}+\\frac{1}{n_{j}}\\right)}\n\\] and rejects the null that the pairs have equal means if the difference is larger.\n\n\n\n",
    "preview": "posts/2021-08-03-week3-4/week3-4_files/figure-html5/unnamed-chunk-3-1.svg",
    "last_modified": "2021-10-04T16:31:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week3-1/",
    "title": "ANOVA",
    "description": "The ANOVA model and sum-of-squares decomposition",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-21",
    "categories": [],
    "contents": "\n\n\n\nReadings 3.1 - 3.3, Rmarkdown\nANOVA is used when we want to compare the effect of different treatments on a continuous response. For example,\nHow does the etch rate of a tool depend on its power setting?\nHow do an opera company’s different donation strategies compare with one another?\nHow does the average rental time compare across cars?\n\nIt is an extension of two sample testing when there are more than two levels possible for a single factor.\n\n\n\nModel and Test Setup\nFormally, consider the model, \\[\n  y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\n  \\] where \\(i=1 \\dots a\\) and \\(j=1, \\dots, n\\) and the errors \\(\\epsilon_{ij} \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\) are independent.\n\\(i\\) indexes different groups\n\\(j\\) indexes the samples within groups\n\\(a\\) is the number of groups\n\\(n\\) is the number of samples in each group\n\\(N=na\\) is the total number of samples\n\n\n\n\nFigure 1: The underlying distributions in the ANOVA model. Under the null, all the distributions have the same vertical offset.\n\n\n\nOur null hypothesis is that none of the groups deviate from the global mean. The alternative is that at least one of the groups is different. Formally, \\[\n  H_0: \\tau_1 = \\dots, = \\tau_{a} = 0 \\\\\n  H_1: \\tau_{i} \\neq 0 \\text{ for at least one }i.\n  \\]\nImportant Identities\nThe word “analysis” in ANOVA is used in the classical sense of to break something into its parts1. ANOVA breaks the observed variation into distinct components, \\[\n \\sum_{ij} \\color{#ff8200}{\\left(y_{ij} - \\bar{y}\\right)}^2 = n\\sum_{i} \\color{#447583}{\\left(\\bar{y}_i - \\bar{y}\\right)}^2 + \\sum_{i,j} \\color{#b090c2}{\\left(y_{ij} - \\bar{y}_{i}\\right)}^2\n \\] which is abbreviated as\n\\[\n \\color{#ff8200}{SS_{\\text{total}}} = \\color{#447583}{SS_{\\text{treatments}}} + \\color{#b090c2}{SS_{\\text{error}}}.\n \\] This is usually called the “ANOVA identity.”\n\n\n\nFigure 2: Visual representation of the three terms in the ANOVA identity.\n\n\n\nIf any of the groups had a mean that was different from the global mean, then we’d expect the  term to be larger than it would otherwise be. How large is large enough to reject?\nSince the variance within each group is \\(\\sigma^2\\), the variance of each \\(\\bar{y}_i\\) is \\(\\frac{\\sigma^2}{n}\\). The blue term looks like how we would usually estimate the variances of the \\(y_i\\), i.e.,\n\\[\n\\frac{1}{a - 1}\\sum_{i}\\color{#447583}{\\left(\\bar{y}_i - \\bar{y}\\right)}^2 \\approx \\frac{\\sigma^2}{n}\n\\] 7. On the other hand, under the null, all the \\(y_{ij} \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\\), so we would also know, \\[\n      \\frac{1}{N-a} \\sum_{i, j}\\color{#b090c2}{\\left(y_{i, j}-\\bar{y}_{i}\\right)}^{2} \\approx \\sigma^{2},\n      \\] so under the null, \\[\n      \\frac{\\frac{\\color{#447583}{SS_{\\text {treatment }}}}{a-1}}{\\color{#b090c2}{\\frac{SS_{\\text {error }}}{N-a}}} \\approx 1.\n      \\] Note that under the alternative, it would be larger than 1.\nFrom our results in the probability review lectures, both the  and  are chi-squares, with \\(a - 1\\) and \\(N - a\\) d.f., respectively. It’s not obvious, but they’re also independent (this is called Cochran’s theorem). Therefore, the null reference distribution is an \\(F\\) distribution with \\((a - 1, N - a)\\) d.f.\n\n\n\nFigure 3: Under the null, the scaling between the treatment and error sums of squares is known.\n\n\n\nWe usually call the numerator and denominator above \\(\\color{#447583}{MS_{\\text{treatment}}}\\) and \\(\\color{#b090c2}{MS_{E}}\\).\nCode Example\nLet’s read in an example dataset. We are looking at the etch rate of a machine under three different power settings. We want to know whether there is any difference in the rates, as a function of the power.\n\n\nlibrary(readr)\n\netch_rate <- read_csv(\"https://uwmadison.box.com/shared/static/vw3ldbgvgn7rupt4tz3ditl1mpupw44h.csv\")\netch_rate$power <- as.factor(etch_rate$power) # consider as discrete levels\netch_rate\n\n\n# A tibble: 20 × 3\n   power replicate  rate\n   <fct> <chr>     <dbl>\n 1 160   Ob1         575\n 2 160   Ob2         542\n 3 160   Ob3         530\n 4 160   Ob4         539\n 5 160   Ob5         570\n 6 180   Ob1         565\n 7 180   Ob2         593\n 8 180   Ob3         590\n 9 180   Ob4         579\n10 180   Ob5         610\n11 200   Ob1         600\n12 200   Ob2         651\n13 200   Ob3         610\n14 200   Ob4         637\n15 200   Ob5         629\n16 220   Ob1         725\n17 220   Ob2         700\n18 220   Ob3         715\n19 220   Ob4         685\n20 220   Ob5         710\n\n\n\n\nFigure 4: Etch rate as a function of power.\n\n\n\nTo compute all the quantities above, we can use the lm and anova functions. The column statistic is the F statistic, and the \\(p\\)-value is derived from the reference distribution of that statistic under the null.\n\n\nfit <- lm(rate ~ power, data = etch_rate)\nanova(fit)\n\n\nAnalysis of Variance Table\n\nResponse: rate\n          Df Sum Sq Mean Sq F value    Pr(>F)    \npower      3  66871 22290.2  66.797 2.883e-09 ***\nResiduals 16   5339   333.7                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTo extract terms from this table, it is helpful to first convert it to a data.frame using tidy() in the broom package.\n\n\nlibrary(broom)\naov_table <- tidy(anova(fit))\naov_table$meansq\n\n\n[1] 22290.18   333.70\n\naov_table\n\n\n# A tibble: 2 × 6\n  term         df  sumsq meansq statistic  p.value\n  <chr>     <int>  <dbl>  <dbl>     <dbl>    <dbl>\n1 power         3 66871. 22290.      66.8  2.88e-9\n2 Residuals    16  5339.   334.      NA   NA      \n\nWhat if our data were arranged like the data.frame below? We can no longer use the lm function, because the outcome of interest isn’t isolated into a single column.\n\n\netch <- read_csv(\"https://uwmadison.box.com/shared/static/3ltmo89ea0xowsh1386x9fk58qc51ned.txt\")\netch$Power <- as.factor(etch$Power)\netch\n\n\n# A tibble: 4 × 6\n  Power   Ob1   Ob2   Ob3   Ob4   Ob5\n  <fct> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 160     575   542   530   539   570\n2 180     565   593   590   579   610\n3 200     600   651   610   637   629\n4 220     725   700   715   685   710\n\nTo reorganize the data into an acceptable form, we can use the pivot_longer function from the tidyr package.\n\n\nlibrary(tidyr)\npivot_longer(etch, -Power, names_to = \"replicate\", values_to = \"etch_rate\")\n\n\n# A tibble: 20 × 3\n   Power replicate etch_rate\n   <fct> <chr>         <dbl>\n 1 160   Ob1             575\n 2 160   Ob2             542\n 3 160   Ob3             530\n 4 160   Ob4             539\n 5 160   Ob5             570\n 6 180   Ob1             565\n 7 180   Ob2             593\n 8 180   Ob3             590\n 9 180   Ob4             579\n10 180   Ob5             610\n11 200   Ob1             600\n12 200   Ob2             651\n13 200   Ob3             610\n14 200   Ob4             637\n15 200   Ob5             629\n16 220   Ob1             725\n17 220   Ob2             700\n18 220   Ob3             715\n19 220   Ob4             685\n20 220   Ob5             710\n\n\ni.e. the opposite of “synthesis.”↩︎\n",
    "preview": "posts/2021-08-03-week3-1/week3-1_files/figure-html5/unnamed-chunk-2-1.svg",
    "last_modified": "2021-10-04T16:31:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week3-2/",
    "title": "Model Checking",
    "description": "How should we check the assumptions of the ANOVA model?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-21",
    "categories": [],
    "contents": "\nReadings 3.4, Rmarkdown\nRecall the ANOVA model, \\[\ny_{i j}=\\mu+\\tau_{i}+\\epsilon_{i j}\n\\] with independent errors \\(\\epsilon_{ij} \\sim \\mathcal{N}\\left(0,  \\sigma^2\\right)\\). There are a few ways that this model can fail,\nThere might be systematic variations besides the group deviations \\(\\tau_i\\).\nThe errors might not be normally distributed\nThe errors might not be independent\nThe variance might not be the same in each group\n\nTo see if the model is okay, it will be helpful to define residuals, \\[\\begin{align*}\ne_{i j} &=y_{i j}-\\hat{y}_{i j} \\\\\n&=y_{i j}-\\bar{y}_{i}.\n\\end{align*}\\] Residuals are our best guess of what the true random error \\(\\epsilon_{ij}\\) is like.\n\n\n\nFigure 1: The true error depends on the unknown means for each group; however, residuals can give a close approximation.\n\n\n\nWe can extract this using the resid function; the example below continues the etch rate example.\n\n\nlibrary(readr)\netch_rate <- read_csv(\"https://uwmadison.box.com/shared/static/vw3ldbgvgn7rupt4tz3ditl1mpupw44h.csv\")\netch_rate$power <- as.factor(etch_rate$power) # consider as discrete levels\nfit <- lm(rate ~ power, data = etch_rate)\nresid(fit)\n\n\n    1     2     3     4     5     6     7     8     9    10    11 \n 23.8  -9.2 -21.2 -12.2  18.8 -22.4   5.6   2.6  -8.4  22.6 -25.4 \n   12    13    14    15    16    17    18    19    20 \n 25.6 -15.4  11.6   3.6  18.0  -7.0   8.0 -22.0   3.0 \n\nNormal Probability Plots\nWe can’t check normality of \\(\\epsilon_{ij}\\) directly, but we can check normality of the residuals \\(e_{ij}\\) using normal probability plots.\n\n\nqqnorm(resid(fit))\nqqline(resid(fit))\n\n\n\n\nFigure 2: A qqplot for the etch rate data.\n\n\n\nOf the ways that the model can fail, normality of the residuals is not the most severe, because you can often count on the central limit theorem to make the reference \\(F\\) distribution still approximately correct.\nPlotting Residuals\nThe way to check for systematic variation beyond the \\(\\tau_i\\)’s, try plotting residuals against measured variables. If you see “patterns,” those may correspond to missing terms in the model.\n\n\netch_rate$residual <- resid(fit)\nggplot(etch_rate) +\n  geom_point(aes(power, residual))\n\n\n\n\nFigure 3: There doesn’t seem to be a relationship between the measured variable and the residuals, so there is no reason to suspect missing terms in the model. (The third power level has slightly higher variance, but it’s barely noticeable.)\n\n\n\nWe can use plots to check for independence. For example, if you plot the residuals over time and you see clear trends, then the errors are likely correlated over time.\nIt’s often useful to plot residuals against the fitted values. This can reveal nonconstant variance across the groups \\(i\\).\n\n\netch_rate$fitted <- predict(fit)\nggplot(etch_rate) +\n  geom_point(aes(fitted, residual))\n\n\n\n\nFigure 4: An example plotting residuals against the fitted values.\n\n\n\nTesting Equality of Variances\nThere are formal tests to test whether the equal variance assumption of the ANOVA is valid (it’s very meta). The most common are,\nBartlett’s test\nThe Modified Levene test\n\nThe main difference is that the Modified Levene test is still valid even when the errors are not normally distributed. You don’t need to memorize the test statistics, but know that they exist, and be able to interpret associated computer output.\nTransformations\nWhat can you do if you detect nonconstant variance across groups? The most common fix is to use a variance stabilizing transformation. That is, apply some function \\(f(x)\\) to each data point and then perform the ANOVA.\nThere are various rules of thumb1, though the process is still somewhat informal,\n\\(f(x) = \\sqrt{x}\\) or \\(f(x) = \\sqrt{1 + x}\\) if the data are counts\n\\(f(x) = \\log x\\) if the data seem lognormal\n\\(f(x) = \\arcsin\\left(\\sqrt{x}\\right)\\) if the data are binomial-derived fractions\n\n\n\nx <- rpois(4000, 5)\nx <- data.frame(x)\n\nggplot(x) +\n  geom_histogram(aes(x = x), binwidth = 0.5)\n\n\n\n\nFigure 5: An example of using a transformation to bring counts data closer to normality.\n\n\n\nggplot(x) +\n  geom_histogram(aes(x = sqrt(1 + x)), binwidth = 0.1)\n\n\n\n\nFigure 6: An example of using a transformation to bring counts data closer to normality.\n\n\n\n\nIt’s not at all obvious why any of these transformations are effective – they are typically derived in introductory mathematical statistics courses.↩︎\n",
    "preview": "posts/2021-08-03-week3-2/week3-2_files/figure-html5/unnamed-chunk-1-1.svg",
    "last_modified": "2021-10-04T16:31:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week2-4/",
    "title": "Diagnostics and Power",
    "description": "Tricks to make sure tests aren't applied blindly",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-17",
    "categories": [],
    "contents": "\nReadings 2.4, Rmarkdown\nThe reference distribution depends on three assumptions,\nSamples are independent. If they aren’t, then we’re pretending we have more samples than we actually do.\nThe standard deviations are equal.\nThe populations are normally distributed.\nWe can check the last two assumptions using something called a normal probability plot. This plots the sample quantiles against the theoretical normal distribution’s quantiles.\n\n\nlibrary(EBImage)\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/n1a3bdzspet06ibsd1yebc1r6w7kzf3o.png\"))\n\n\n\n\nPower Analysis\nPeople will often call you asking about what a good sample size is for their experiment. A good way to answer this is to compute the power curves as a function of different signal strengths.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/06qu4t1q6jemmwto01vgd95jtzd0if3e.png\")\n\n\n\n\nOf course, we can never know the signal strength in advance. But we can test a few different plausible ranges, based on past experience.\nImportant Variations\nWhat if the variances are not equal? Our test statistic used a pooled standard deviation. If the variances aren’t equal, we could standardize differently, \\[\n\\frac{\\bar{y}_1 - \\bar{y}_2}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}.\n\\]\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/uzh72egfwwp251xzac932cej6kna1woz.png\")\n\n\n\n\nThis is unfortunately not exactly \\(t\\)-distributed under the null. That said, the reference distribution can be well approximated by one, and almost any statistical package will let you compute corresponding \\(p\\)-values and confidence intervals.\nWhat if the variances are known? In this case, we can avoid using \\(S_1\\) and \\(S_2\\). Instead, we ought to standardize using the known standard deviations. Since there’s no additional randomness coming from estimation, the reference distribution is a standard normal, not a \\(t\\)-distribution.\n\n\n\n",
    "preview": "posts/2021-08-03-week2-4/week2-4_files/figure-html5/unnamed-chunk-1-1.svg",
    "last_modified": "2021-10-04T16:30:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week2-3/",
    "title": "Testing Differences in Means",
    "description": "The basic principles of hypothesis testing.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-16",
    "categories": [],
    "contents": "\nReadings 2.4, Rmarkdown\nStatistics is about making general conclusions based on specific evidence. One approach is based on hypothesis testing: we have a theory about the general (a null hypothesis), and we want to see whether our specific sample is consistent with that theory. This philosophy is made quantitative by following a standard recipe,\nPose a null hypothesis about the world\nDefine a test statistic that should detect departures from that null hypothesis\nDetermine a reference distribution for that test statistic\nCompute the test statistic on your data, and see if it’s plausibly a draw from your reference distribution\n\n\n\n\nProceeding in this way, there are a few types of error\n\nTested rejected\nTest didn’t reject\nNull is true\nFalse alarm\nCorrect\nNull is false\nCorrect\nMissed detection\n\\(p\\)-values. “Rejected” or “Not rejected” is only a very coarse description of how the data conforms to a theory. \\(p\\)-values give a measure of the degree of (im)plausibility of a test statistic under a given null hypothesis. The specific measure of plausibility will depend on the form of the test – we will see a specific example in the next set of notes.\nTwo Sample t-test\nMotivating example: You have two ways of making concrete mortar. Is one stronger than the other? By default, you think they are equally strong. We can denote this (the null hypothesis) by,\n\\[\nH_0: \\mu_1 = \\mu_2\n\\]\n\n\n\nThe alternative hypothesis is that the strengths are not equal,\n\\[\nH_1: \\mu_1 \\neq \\mu_2\n\\]\n\n\n\nOur test statistic for detecting departures from this null will be,\n\\[\nt_0 := \\frac{\\bar{y}_1 - \\bar{y}_2}{S_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\] where we define the pooled standard deviation by,\n\\[\nS^2_p = \\frac{\\left(n_1 - 1\\right)S_1^2 + \\left(n_2 - 1\\right)S_2^2}{n_1 + n_2 -2}\n\\] and \\(S_1\\) and \\(S_2\\) are the usual standard deviations for each group individually. (consider what happens when \\(n_1 = n_2 = n\\))\nUnder the null hypothesis, this is a ratio between a standard normal and chi-square, so \\(t_0\\) is \\(t\\)-distributed with \\(n_1 + n_2 - 2\\) d.f. This gives reference distribution under the null.\nConfidence intervals\nInstead of thinking we know the mean and trying to reject it, why don’t we try to directly estimate it (with an error bar)? A 95% confidence interval is an interval \\([L, U]\\) satisfying,\n\\[\n\\mathbf{P}\\left(\\theta \\in \\left[L, U\\right]\\right) = 0.95\n\\]\nThe randomness here is in \\(L\\) and \\(U\\). If we were being more formal, we would write those as functions of the (random) sample,\n\\[\n\\left[L\\left(y_1, \\dots, y_n\\right), U\\left(y_1, \\dots, y_n\\right)\\right]\n\\]\nTo construct one for the two sample test, recall that, based on the \\(t\\)-distribution characterization from the probability-review lecture,\n\\[\nP\\left(\\frac{\\left(\\bar{y}_1 - \\bar{y}_2\\right) - \\left(\\mu_1 - \\mu_2\\right)}{S_p\\sqrt{\\frac{2}{n}}} \\in \\left[t_{0.025, 2\\left(n - 1\\right)}, t_{0.975, 2\\left(n - 1\\right)}\\right]  \\right) = 0.95\n\\]\nTo simplify the algebra, let\n\\[\nT\\left(y\\right) := \\bar{y}_1 - \\bar{y}_2 \\\\\n\\theta := \\mu_1 - \\mu_2 \\\\\n\\hat{\\sigma} := S_p\\sqrt{\\frac{2}{n}} \\\\\nt_{0.025, 2\\left(n - 1\\right)} := t_{\\text{left}} \\\\\nt_{0.975, 2\\left(n - 1\\right)} := t_{\\text{right}}\n\\] so that the above expression reduces to,\n\\[\n\\mathbf{P}\\left(\\frac{T\\left(y\\right) - \\theta}{\\hat{\\sigma}} \\in \\left[t_{\\text{left}}, t_{\\text{right}}\\right]\\right) = 0.95\n\\]\nNow, if we rearrange terms, we find\n\\[\n\\mathbf{P}\\left(\\theta \\in \\left[T\\left(y\\right) - \\hat{\\sigma}t_{\\text{right}}, T\\left(y\\right) - \\hat{\\sigma}t_{\\text{left}}\\right]\\right) = 0.95\n\\] We can use the fact that \\(t_{\\text{left}} = -t_{\\text{right}}\\) to simplify the expression further to\n\\[\n\\mathbf{P}\\left(\\theta \\in \\left[T\\left(y\\right) - \\hat{\\sigma}t_{\\text{right}}, T\\left(y\\right) + \\hat{\\sigma}t_{\\text{right}}\\right]\\right) = 0.95\n\\] This is exactly the property that a confidence interval has to satisfy. Plugging in the original expressions gives the confidence interval for the difference in means, assuming shared variance.\n\n\n\n",
    "preview": "posts/2021-08-03-week2-3/week2-3_files/figure-html5/unnamed-chunk-3-1.svg",
    "last_modified": "2021-10-04T16:30:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week2-2/",
    "title": "Common Distributions",
    "description": "Distributions that appear across experimental design.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-15",
    "categories": [],
    "contents": "\nReadings 2.3, Rmarkdown\nDon’t try to memorize the formulas for all the probability distributions! Instead, it’s much more useful to learn,\nThe relationships between distributions\nThe basic shapes of the distributions (unimodal? nonnegative? …)\nHow their shapes change then their parameters are changed We’ll give a refresher of some common probability distributions in these notes.\nChi-Square Distribution. This distribution arises as the sum of squares of standard normals. That is, if \\(z_[k] \\sim \\mathcal{N}\\left(0, 1\\right)\\), then \\(\\sum_{k} z_{k}^2 \\sim \\chi^2_{K}\\), a chi-square distribution with \\(K\\)-degrees of freedom (d.f.).\nThis distributions claim to fame is that if \\(y_i \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\\) independently, then\n\\[\n\\frac{1}{\\sigma^2}\\sum_{i = 1}^{n} \\left(y_i- \\bar{y}\\right)^2 \\sim \\chi^2_{n -1}\n\\] which is a nontrivial but very useful fact, since the expression on the right is similar to the usual estimator for the sample standard deviation. We’ll make use of connection when we construct some common hypothesis tests.\n\\(t\\) distribution. A \\(t\\) distribution with \\(k\\) d.f. arises as a ratio between a normal and the square root of a chi-square with K d.f.,\n\\[\n\\frac{\\mathcal{N}\\left(0, 1\\right)}{\\sqrt{\\frac{\\chi^2_{K}}{K}}}\n\\]\nThis seems like an esoteric fact, but notice that the usual way of standardizing the mean (when the true variance is unknown) has this form,\n\\[\n\\frac{\\sqrt{n}\\left(\\bar{y} - \\mu\\right)}{S}\n\\]\n\\(F\\) Distribution. The \\(F\\) distribution occurs as the ratio of independent chi-squares (suitably rescaled),\n\\[\nF_{u, v} = \\frac{\\frac{1}{u}\\chi^2_u}{\\frac{1}{v}\\chi^2_v}\n\\]\nSince chi-squares arise whenever we have sums of squares, this distribution will come in handy whenever we need to compare two different sums of squares.\n\n\nlibrary(EBImage)\ndisplay(readImage(\"https://uwmadison.box.com/shared/static/dv5tvok0m9vkqqmkd3c0woam5is7gzse.png\"))\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-03-week2-2/week2-2_files/figure-html5/unnamed-chunk-1-1.svg",
    "last_modified": "2021-10-04T16:30:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week2-1/",
    "title": "Probability Review",
    "description": "Probability distributions, their properties, and relationships.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-14",
    "categories": [],
    "contents": "\nReadings 2.1 - 2.2, Rmarkdown\nThe most basic idea of statistics is that if you ran an experiment again, you would get different results i.e., there is randomness. Probability is the calculus of randomness.\nDefinitions\nIf \\(y\\) is a discrete random variable taking on values \\(y_{k}\\) with probability \\(p_{k}\\), then its mean is defined as \\(\\mathbf{E}\\left[y\\right] = \\sum_{k} p_{k}y_{k}\\). If it is a continuous variable with density \\(p\\left(y\\right)\\), the corresponding quantity is \\(\\mathbf{E}\\left[y\\right] = \\int_{\\mathbf{R}} y p\\left(y\\right) dy\\). Think of integral in the continuous case like the limit of a Riemann sum in calculus\n\n\n\n\n\n\nFigure 1: The expectation in a two-valued random variable is a weighted average between the values it can take on.\n\n\n\nTo build intuition about this formula, consider some special cases,\nIf there are just two values with equal probability, it’s just a midpoint\nIf one of the probability weights is larger, it’s closer to the larger weight\nIf you have many values, it’s closer to the ones with large weight\nThe variance of a random variable \\(Y\\) is defined as \\(\\text{Var}\\left[y\\right] = \\mathbf{E}\\left[y - \\mathbf{E}\\left[y\\right]\\right]^2\\). This measures the typical distance of \\(Y\\) around its mean.\n\n\n\nFigure 2: Variance measures the typical distance of an observation from the distribution’s mean.\n\n\n\nUseful properties\nFor calculations, it’s often easier to use properties of mean and variance to reduce to simpler expressions, rather than using the formulas above. For example, expectation is a linear function,\n\\[\n\\mathbf{E}\\left[c_{1}y_{1} + c_{2}y_{2}\\right] = c_{1}\\mathbf{E}\\left[y_{1}\\right] + c_{2}\\mathbf{E}\\left[y_{2}\\right].\n\\]\nVariance is not linear, but the variance of a linear combination of two random variables can be found simply enough,\n\\[\n\\text{Var}\\left[c_1 y_1 + c_2 y_2\\right] = c_1^2 \\text{Var}\\left[y_1\\right] + \n  c_2^2 \\text{Var}\\left[y_2\\right] +\n  c_1 c_2 \\text{Cov}\\left[y_1, y_2\\right]\n\\]\nwhere we define the covariance as, \\[\n\\text{Cov}\\left[y_1, y_2\\right] = \\mathbf{E}\\left[\\left(y_1 - \\mathbf{E}\\left[y_1\\right]\\right)\\left(\ny_2 - \\mathbf{E}\\left[y_2\\right]\\right)\\right]\n\\]\n\n\n\nFigure 3: If two variables have high covariance, then whether or not they are above their means is often synchronized.\n\n\n\nSampling and Estimators\nWhy is probability useful in statistics. From a high-level, statistics is concerned with drawing inferences from the specific to the general. Starting from a sample, we would like to say something true about the population. A typical strategy is to compute a statistic (a function of the sample) to say something about the probability distribution that it was drawn from (a property of the population).\nSuppose we have observed \\(n\\) samples \\(y_{1}, \\dots, y_{n}\\). Two very useful statistics are the sample mean,\n\\[\n\\bar{y} = \\frac{1}{n}\\sum_{i = 1}^{n}y_i\n\\] and the sample standard deviation \\[\nS = \\sqrt{\\frac{1}{n - 1}\\sum_{i = 1}^{n}\\left(y_i - \\bar{y}\\right)^2}\n\\]\n\n\n\n\n\n\nand the sample standard deviation,\nStatisticians have come up with a variety of properties that they would like their statistics to satisfy. Two common requirements are that the statistic be “unbiased” and “minimum variance.” Unbiased means it’s centered around the correct value, on average Minimum variance means it’s not too far from the correct value, on average.\nCentral limit theorem\nFor very many distributions, an appropriately rescaled version of the sample mean converges to a normal distribution. Specifically, if all the \\(y_i\\) are drawn i.i.d. from some distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then\n\\[\n\\frac{\\sqrt{n}\\left(\\bar{y} - \\mu\\right)}{\\sigma} \\to \\mathcal{N}\\left(0, 1\\right).\n\\]\nThis phenomenon is called the central limit theorem.\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-03-week2-1/week2-1_files/figure-html5/unnamed-chunk-7-1.svg",
    "last_modified": "2021-10-04T16:30:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-02-week1-1/",
    "title": "Principles and Vocabulary",
    "description": "An introduction to randomization, replication, and blocking.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-09",
    "categories": [],
    "contents": "\nReadings 1.1, 1.3, Rmarkdown\nWhat is an experiment?\n\nA test or series of runs in which purposeful changes are made to the input variables of a process or system so that we may observe and identify the reasons for changes that may be observed – Montgomery, pg. 1\n\nMore simply, in an experiment, our goal is to learn how inputs affect outputs. It’s not enough to passively watch – we need to see how turning certain “knobs” affects the system.\nTo illustrate, we can consider a planting example. There are a variety of factors that could influence how the plants grow (soil type, watering schedule, etc.). We could allocate different plots of land to trying different configurations of factors. At the end, we hope we can arrive at generalizable knowledge about which configurations we should use during future growing seasons.\n\n\n\nRandomization\nOne of three key principles of experimental design is randomization. The book says that randomization has been applied if the,\n\nAllocation of experimental material and the order in which the individual runs are performed are randomly determined – Montgomery, pg. 11\n\nMore simply – we should assign treatments using a coin toss (or random number generator). Why is this important? There are many factors besides treatment that can influence outcome. We don’t want these superfluous factors to bias our conclusions.\n[Treating the sickest patients] What could go wrong in the absence of randomization? Suppose we are at a hospital and are trying to see whether a new treatment is effective. If we don’t randomize, we might end up only treating sicker patients than usual. If the sickest patients have worse outcomes on average, then we might underestimate the effectiveness of our treatment.\nIf we randomize, the differences coming from these extraneous factors (like amount of sickness) will cancel out. However, if the treatment does have an effect, we will be able to detect it.\n\n\n\nReplication\nReplication is the second of the three main principles of experimental design. A replicate is,\n\nAn independent run of each factor combination – Montgomery, pg. 12\n\nReplication is important because it helps us understand run-to-run variation. If we had only grown the plant once, we’d have no idea about the range of variation we’d expect even when fixing the influential factors. Moreover, if we can get many replicates, our estimates of the mean will improve (orange -> red)\n\n\n\nThe book highlights a distinction between replicates and repeated measures. Repeated measures are several measurements on the same experimental unit. Replicates are distinct experimental units drawn under the same overarching conditions.\n[Computer Chips]. If we were trying to build better computer chips, then repeated measures would be several measurements on the same chip. Replicates would be completely independent chips.\n\n\nBlocking\nThe final major principle of experimental design is blocking, defined as,\n\nA design technique used to improve the precision with which comparisons among the factors of interest are made. Often used to reduce or eliminate variability from nuisance factors.\n\n[Shoe soles] Suppose that we want to test the difference between these purple and green shoe sole types (which wears down faster?).\n\n\n\nTwo designs seem natural,\nDesign 1 [No blocking]: Each person is randomly assigned a shoe sole type.\nDesign 2 Blocking: Each person gets one of each shoe sole, and randomly wears on on the left / right foot.\nUnder design 1, any true shoe sole effect would be drowned out by the amount of walking each person did In the blocked design, consistent effects within individuals become detectable. In this example, blocking helped remove nuisance variation resulting from some people walking more than others.\n\n\n\nFigure 1: Each nearby pair of dots is a person assigned two of the same sole type. Differences between sole types are washed out by differences in how much people walk.\n\n\n\n\n\n\nFigure 2: When each person is assigned one of each type, consistent improvements in the purple shoe sole become clearer.\n\n\n\n\n\n\n",
    "preview": "https://uwmadison.box.com/shared/static/rnuet4s27hhidnxde2ys5n3taw9ygjhy.png",
    "last_modified": "2021-10-04T16:30:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week1-2/",
    "title": "Motivating Examples",
    "description": "Why are experiments run in the first place?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-09",
    "categories": [],
    "contents": "\nReadings 1.1, 1.2, 1.4, Rmarkdown\n[Golf] We can imagine someone’s golf score as being a function of many factors,\ngolf score = f(driver type, type of ball, ...)\nIn theory, we could manipulate these factors to see how they influenced golf score. If we considered only two factors at a time, each with two possible levels, this would be called a \\(2^2\\) design.\nWe can visualize the 4 possible configurations as corners of a square The golf score is the height of the plane.\n\n\n\nMathematically,\n\\[\ny = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\epsilon\n\\]\nInteractions\nIt’s possible that the effect of one factor depends on the value of the other – this called an interaction between the two factors. If this happens, then the slopes along the edges are no longer parallel. The previous formula cannot capture this. Instead, we need,\n\\[\ny = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{12}x_{1}x_{2} + \\epsilon\n\\]\nbecause now the slopes can change depending on the value of the other factor.\n\n\n\nFor example, rearranging terms, we can see that the slope for \\(x_1\\) depends on the value of \\(x_2\\),\n\\[\ny = \\beta_{0} + \\left(\\beta_{1} + \\beta_{12}x_2\\right)x_{1} + \\beta_{2}x_{2} + \\epsilon.\n\\]\nCan you write an expression showing how the slope for \\(x_{2}\\) depends on \\(x_{1}\\)?\nFor each configuration of factors, it is better to play several rounds of golf. The more rounds we play, the better our estimates of the effects for each factor. This is a special case of what we discussed in the last notes; the more replicates, the better our estimates.\n\n\n\nMore than 2 factors\nSuppose we want to see how K different binary factors influence golf score. We can no longer visualize the effects as corners of a square, but we can still collect samples for each configuration of factors. This is called a \\(2^K\\) experiment.\nA challenge is that for large K, this means collecting lots of samples\nK = 3 means 8 configurations\nK = 4 means 16\netc.\n\n\n\nExperimental design is often used in characterizing a process; i.e., how do each of the knobs affect the outcome? Alternatively, we may ask a simpler question – are there knobs that have no effect on the outcome? This is called factor screening. An example is the soldering experiment.\n\n\n\nSometimes we care more about optimization. In this case, we don’t care so much about how each factor influences an outcome; we just want a combination of factors that maximizes it. We can visualize the outcome of hte process as a function of several continuous variables.\nIntuitively, our experimentation should proceed by first making a preliminary test and then proceeding in the direction of the max. This intuition is formalized in response surface methodology.\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-03-week1-2/week1-2_files/figure-html5/unnamed-chunk-3-1.svg",
    "last_modified": "2021-10-04T16:30:26-05:00",
    "input_file": {}
  }
]
