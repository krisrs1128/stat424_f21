[
  {
    "path": "posts/2021-08-03-week2-3/",
    "title": "Testing Differences in Means",
    "description": "The basic principles of hypothesis testing.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-16",
    "categories": [],
    "contents": "\nReadings 2.4, Rmarkdown\nStatistics is about making general conclusions based on specific evidence. One approach is based on hypothesis testing: we have a theory about the general (a null hypothesis), and we want to see whether our specific sample is consistent with that theory. This philosophy is made quantitative by following a standard recipe,\nPose a null hypothesis about the world\nDefine a test statistic that should detect departures from that null hypothesis\nDetermine a reference distribution for that test statistic\nCompute the test statistic on your data, and see if it’s plausibly a draw from your reference distribution\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/e2vep3vvfvnz8v4kiilim1pjxe2w2upr.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/66793m770ob31z53fpgc4xoci4y0s7fi.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/r9dzi8ar8l7ic7h2xarnrdiwoab6tojz.png\")\n\n\n\n\nProceeding in this way, there are a few types of error\n\nTested rejected\nTest didn’t reject\nNull is true\nFalse alarm\nCorrect\nNull is false\nCorrect\nMissed detection\n\\(p\\)-values. “Rejected” or “Not rejected” is only a very coarse description of how the data conforms to a theory. \\(p\\)-values give a measure of the degree of (im)plausibility of a test statistic under a given null hypothesis. The specific measure of plausibility will depend on the form of the test – we will see a specific example in the next set of notes.\nTwo Sample t-test\nMotivating example: You have two ways of making concrete mortar. Is one stronger than the other? By default, you think they are equally strong.\nOur test statistic for detecting departures from this null will be,\nwhere we define the pooled standard deviation by,\nand S1 and S2 are the usual standard deviations for each group individually. (consider what happens when n1 = n2 = n)\nUnder the null hypothesis, this is a ratio between a standard normal and chi-square, so t0 is t-distributed with n - 2 df. This gives reference distribution under the null. Confidence intervals Instead of thinking we know the mean and trying to reject it, why don’t we try to directly estimate it (with an error bar)?\nA 95% confidence interval is an interval [L, U] satisfying,\nThe randomness here is in L and U. If we were being more formal, we would write those as functions of the (random) sample,\nTo construct one for the two sample test, observe,\nTo simplify the algebra, let\nSo that the above expression reduces to\nThen if we rearrange terms, we find\nIf you are calculating by hand, you can use the fact that tleft=-tright, to simplify the expression to\nWhich is exactly the definition of a confidence interval. Plugging in the original expressions gives the confidence interval for the difference in means, assuming shared variance.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week2-1/",
    "title": "Probability Review",
    "description": "Probability distributions, their properties, and relationships.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-14",
    "categories": [],
    "contents": "\nReadings , Rmarkdown\nThe most basic idea of statistics is that if you ran an experiment again, you would get different results i.e., there is randomness Probability is the calculus of randomness\nDefinitions\nIf \\(y\\) is a discrete random variable taking on values \\(y_{k}\\) with probability \\(p_{k}\\), then its mean is defined as \\(\\matbf{E}\\left[y\\right] = \\sum_{k} p_{k}y_{k}\\). If it is a continuous variable with density \\(p\\left(y\\right)\\), the corresponding quantity is \\(\\mathbf{E}\\left[y\\right] = \\int_{\\mathbf{R}} y p\\left(y) dy\\). Think of integral in the continuous case like the limit of a Riemann sum in calculus\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/rrvufphhkjkvqjkwgvu4pj712hn9tigt.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/8truwe0fd247iuq7m5p56f2ylx4nmewn.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/plbcevhwx1sq8f9rzvn1vdwr7x70syk5.png\")\n\n\n\n\nTo build intuition about this formula, consider some special cases,\nIf there are just two values with equal probability, it’s just a midpoint\nIf one of the probability weights is larger, it’s closer to the larger weight\nIf you have many values, it’s closer to the ones with large weight\nThe variance of a random variable Y is defined as \\(\\mathbf{V}\\left[y\\right] = \\mathbf{E}\\left[y - \\mathbf{E}\\left[y\\right]\\right]^2\\). This measures the typical distance of \\(Y\\) around its mean.\nUseful properties\nFor calculations, it’s often easier to use properties of mean and variance to reduce to simpler expressions, rather than using the formulas above. For example, expectation is a linear function,\n\\[\n\\mathbf{E}\\left[c_{1}y_{1} + c_{2}y_{2}\\right] = c_{1}\\mathbf{E}\\left[y_{1}\\right] + c_{2}\\mathbf{E}\\left[y_{2}\\right].\n\\]\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/pvfua6rtht3vubmz50yi3xry02t2elo3.png\")\n\n\n\n\nVariance is not linear, but the variance of a linear combination of two random variables can be found simply enough,\n\\[\n\\mathbf{V}\\left[c_1 y_1 + c_2 y_2\\right] = c_1^2 \\mathbf{V}\\left[y_1\\right] + \n  c_2^2 \\mathbf{V}\\left[y_2\\right] +\n  c_1 c_2 \\textbf{Cov}\\left[y_1, y_2\\right]\n\\]\nwhere we define the covariance as,\n\\[\n\\textbf{Cov}\\left[y_1, y_2\\right] = \\mathbf{E}\\left[\\left(y_1 - \\mathbf{E}\\left[y_1\\right]\\right)\\left(\ny_2 - \\mathbf{E}\\left[y_2\\right]\\right)\\right]\n\\]\nSampling and Estimators\nWhy is probability useful in statistics. From a high-level, statistics is concerned with drawing inferences from the specific to the general. Starting from a sample, we would like to say something true about the population. A typical strategy is to compute a statistic (a function of the sample) to say something about the probability distribution that it was drawn from (a property of the population).\nSuppose we have observed \\(n\\) samples \\(y_{1}, \\dots, y_{n}\\). Two very useful statistics are the sample mean,\n\\[\n\\bar{y} = \\frac{1}{n}\\sum_{i = 1}^{n}y_i\n\\] and the sample standard deviation \\[\nS = \\sqrt{\\frac{1}{n - 1}\\sum_{i = 1}^{n}\\left(y_i - \\bar{y}\\right)^2}\n\\]\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/o3li54yw6986aurviiljfmgmrmuh5wms.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/310ij7e4ki9xnn4doe191hks8k6h3o9z.png\")\n\n\n\n\nand the sample standard deviation,\nStatisticians have come up with a variety of properties that they would like their statistics to satisfy. Two common requirements are that the statistic be “unbiased” and “minimum variance.” Unbiased means it’s centered around the correct value, on average Minimum variance means it’s not too far from the correct value, on average.\nCentral limit theorem\nFor very many distributions, an appropriately rescaled version of the sample mean converges to a normal distribution. Specifically, if all the \\(y_i\\) are drawn i.i.d. from some distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then\n\\[\n\\frac{\\sqrt{n}\\left(\\bar{y} - \\mu\\right)}{\\sigma} \\to \\mathcal{N}\\left(0, 1\\right).\n\\]\nThis phenomenon is called the central limit theorem.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/2nue43dk0rw0v25swmbd552qk1bz1jze.png\")\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week2-2/",
    "title": "Common Distributions",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-14",
    "categories": [],
    "contents": "\nhttps://docs.google.com/document/d/1fJxl3kNL0EdbMTHMQub3IiKqthXqKdW6HUoB4s8gw9I/edit#\nReadings 2.3, Rmarkdown\nDon’t try to memorize the formulas for all the probability distributions! Instead, it’s much more useful to learn,\nThe relationships between distributions\nThe basic shapes of the distributions (unimodal? nonnegative? …)\nHow their shapes change then their parameters are changed We’ll give a refresher of some common probability distributions in these notes.\nChi-Square Distribution. This distribution arises as the sum of squares of standard normals. That is, if \\(z_[k] \\sim \\mathcal{N}\\left(0, 1\\right)\\), then \\(\\sum_{k} z_{k}^2 \\sim \\chi^2_{K}\\), a chi-square distribution with \\(K\\)-degrees of freedom (d.f.).\nThis distributions claim to fame is that if \\(y_i \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\\) independently, then\n\\[\n\\frac{1}{\\sigma^2}\\sum_{i = 1}^{n} \\left(y_i- \\bar{y}\\right)^2 \\sim \\chi^2_{n -1}\n\\] which is a nontrivial but very useful fact, since the expression on the right is similar to the usual estimator for the sample standard deviation. We’ll make use of connection when we construct some common hypothesis tests.\n\\(t\\) distribution. A \\(t\\) distribution with \\(k\\) d.f. arises as a ratio between a normal and the square root of a chi-square with K d.f.,\n\\[\n\\frac{\\mathcal{N}\\left(0, 1\\right)}{\\sqrt{\\frac{\\chi^2_{K}}{K}}}\n\\]\nThis seems like an esoteric fact, but notice that the usual way of standardizing the mean (when the true variance is unknown) has this form,\n\\[\n\\frac{\\sqrt{n}\\left(\\bar{y} - \\mu\\right)}{S}\n\\]\n\\(F\\) Distribution. The \\(F\\) distribution occurs as the ratio of independent chi-squares (suitably rescaled),\n\\[\nF_{u, v} = \\frac{\\frac{1}{u}\\chi^2_u}{\\frac{1}{v}\\chi^2_v}\n\\]\nSince chi-squares arise whenever we have sums of squares, this distribution will come in handy whenever we need to compare two different sums of squares.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/dv5tvok0m9vkqqmkd3c0woam5is7gzse.png\")\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-02-week1-1/",
    "title": "Principles and Vocabulary",
    "description": "An introduction to randomization, replication, and blocking.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-09",
    "categories": [],
    "contents": "\nReadings 1.1, 1.3, Rmarkdown\nWhat is an experiment?\n\nA test or series of runs in which purposeful changes are made to the input variables of a process or system so that we may observe and identify the reasons for changes that may be observed – Montgomery, pg. 1\n\nMore simply, in an experiment, our goal is to learn how inputs affect outputs. It’s not enough to passively watch – we need to see how turning certain “knobs” affects the system.\nTo illustrate, we can consider a planting example. There are a variety of factors that could influence how the plants grow (soil type, watering schedule, etc.). We could allocate different plots of land to trying different configurations of factors. At the end, we hope we can arrive at generalizable knowledge about which configurations we should use during future growing seasons.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/hzv2nghaxm87u7s0awfqttmsh2k3963h.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/c9dof66gs3b2fwp3rhx353e6tpojhqdn.png\")\n\n\n\n\nRandomization\nOne of three key principles of experimental design is randomization. The book says that randomization has been applied if the,\n\nAllocation of experimental material and the order in which the individual runs are performed are randomly determined – Montgomery, pg. 11\n\nMore simply – we should assign treatments using a coin toss (or random number generator). Why is this important? There are many factors besides treatment that can influence outcome. We don’t want these superfluous factors to bias our conclusions.\n[Treating the sickest patients] What could go wrong in the absence of randomization? Suppose we are at a hospital and are trying to see whether a new treatment is effective. If we don’t randomize, we might end up only treating sicker patients than usual. If the sickest patients have worse outcomes on average, then we might underestimate the effectiveness of our treatment.\nIf we randomize, the differences coming from these extraneous factors (like amount of sickness) will cancel out. However, if the treatment does have an effect, we will be able to detect it.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/cmm759fivv4g0xsg85k25b1cq32zn7gp.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/y531b9lly0twl9i987978dqp8kwzfze5.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/t4btwq8hyhkt14kcn0p2o8rwzg2magqa.png\")\n\n\n\n\nReplication\nReplication is the second of the three main principles of experimental design. A replicate is,\n\nAn independent run of each factor combination – Montgomery, pg. 12\n\nReplication is important because it helps us understand run-to-run variation. If we had only grown the plant once, we’d have no idea about the range of variation we’d expect even when fixing the influential factors. Moreover, if we can get many replicates, our estimates of hte mean will improve (orange -> red)\n\n\ninclude_graphics(\"https://uwmadison.box.com/s/vwu2gz8cm01l1fa2u3vc5fha8rsvxycx\")\n\n\n\n\nThe book highlights a distinction between replicates and repeated measures. Repeated measures are several measurements on the same experimental unit. Replicates are distinct experimental units drawn under the same overarching conditions.\n[Computer Chips]. If we were trying to build better computer chips, then repeated measures would be several measurements on the same chip. Replicates would be completely independent chips.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/rqu00u66f4szo40wfevwmesslkniz2d6.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/inwjp3ntywaoim7vs88wehl6m73wu3pb.png\")\n\n\n\n\nBlocking\nThe final major principle of experimental design is blocking, defined as,\n\nA design technique used to improve the precision with which comparisons among the factors of interest are made. Often used to reduce or eliminate variability from nuisance factors.\n\n[Shoe soles] Suppose that we want to test the difference between these purple and green shoe sole types (which wears down faster?).\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/iimo8kf3idq1s9rjb8i853zbrlg989c2.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/8rmn6uh1enadfmfriq646bjvr3q5hx0w.png\")\n\n\n\n\nTwo designs seem natural,\nDesign 1 [No blocking]: Each person is randomly assigned a shoe sole type.\nDesign 2 Blocking: Each person gets one of each shoe sole, and randomly wears on on the left / right foot.\nUnder design 1, any true shoe sole effect would be drowned out by the amount of walking each person did In the blocked design, consistent effects within individuals become detectable. In this example, blocking helped remove nuiscance variation resulting from some people walking more than others.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/rnuet4s27hhidnxde2ys5n3taw9ygjhy.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/8rmn6uh1enadfmfriq646bjvr3q5hx0w.png\")\n\n\n\n\n\n\n\n",
    "preview": "https://uwmadison.box.com/shared/static/hzv2nghaxm87u7s0awfqttmsh2k3963h.png",
    "last_modified": "2021-08-10T15:10:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week1-2/",
    "title": "Motivating Examples",
    "description": "Why are experiments run in the first place?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-09-09",
    "categories": [],
    "contents": "\nReadings 1.1, 1.2, 1.4, Rmarkdown\n[Golf] We can imagine someone’s golf score as being a function of many factors,\ngolf score = f(driver type, type of ball, ...)\nIn theory, we could manipulate these factors to see how they influenced golf score. If we considered only two factors at a time, each with two possible levels, this would be called a \\(2^2\\) design.\nWe can visualize the 4 possible configurations as corners of a square The golf score is the height of the plane.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/nl6161tped9imsph7qc8c830mmh4ru02.png\")\n\n\n\n\nMathematically,\n\\[\ny = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\epsilon\n\\]\nInteractions\nIt’s possible that the effect of one factor depends on the value of the other – this called an interaction between the two factors. If this happens, then the slopes along the edges are no longer parallel. The previous formula cannot capture this. Instead, we need,\n\\[\ny = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{12}x_{1}x_{2} + \\epsilon\n\\]\nbecause now the slopes can change depending on the value of the other factor.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/hwusoeowryssz905tm5qsf6s9vbkxvd9.png\")\n\n\n\n\nFor example, rearranging terms, we can see that the slope for \\(x_1\\) depends on the value of \\(x_2\\),\n\\[\ny = \\beta_{0} + \\left(\\beta_{1} + \\beta_{12}x_2\\right)x_{1} + \\beta_{2}x_{2} + \\epsilon.\n\\]\nCan you write an expression showing how the slope for \\(x_{2}\\) depends on \\(x_{1}\\)?\nFor each configuration of factors, it is better to play several rounds of golf. The more rounds we play, the better our estimates of the effects for each factor. This is a special case of what we discussed in the last notes; the more replicates, the better our estimates.\nMore than 2 factors\nSuppose we want to see how K different binary factors influence golf score. We can no longer visualize the effects as corners of a square, but we can still collect samples for each configuration of factors. This is called a \\(2^K\\) experiment.\nA challenge is that for large K, this means collecting lots of samples\nK = 3 means 8 configurations\nK = 4 means 16\netc.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/hwusoeowryssz905tm5qsf6s9vbkxvd9.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/2w0x5hzg03zk8113e3rcto58tocc4y6w.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/yzxaz23mgvezuny1ybgydrvpxazn3pm2.png\")\n\n\n\n\nExperimental design is often used in characterizing a process; i.e., how do each of the knobs affect the outcome? Alternatively, we may ask a simpler question – are there knobs that have no effect on the outcome? This is called factor screening. An example is the soldering experiment.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/da48ixumvvvh51y2mi0b6vkaeczqnsmb.png\")\n\n\n\n\nSometimes we care more about optimization. In this case, we don’t care so much about how each factor influences an outcome; we just want a combination of factors that maximizes it. We can visualize the outcome of hte process as a function of several continuous variables.\nIntuitively, our experimentation should proceed by first making a preliminary test and then proceeding in the direction of the max. This intuition is formalized in response surface methodology.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/stgfkm41btsdbnzdxkxs7osnd37hahdj.png\")\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-06-week5-2/",
    "title": "RCBD Diagnostics",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-08-06",
    "categories": [],
    "contents": "\nReadings 4.1, Rmarkdown\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/qezu75a0hv1qhqyeoz8wb7sbb294vrsm.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/dmob9sjh8my7l8hez390o7gnn3vu1oe6.png\")\n\n\n\n\nMultiple Comparisons\nAs before, we may want to use contrasts to decide on which treatment effects are different We can continue to use the same multiple comparisons procedures as before Only differences: The number of samples per treatment n is replaced by the number of blocks b. The df for MSE is now (a - 1)(b - 1), not N - a. For example, the cutoff in Fisher’s LSD becomes\nModel Diagnostics\nThere are two key assumptions, ijN(0, 2) are independently distributed. Note that this also implies homoskedasticity (the variances are not changing from sample to sample). Additivity (i.e. “no interaction”). The treatment effect i needs to be the same across all blocks.\nWe’ve seen the first assumption before, and can continue to use normal probability plots and residual analysis to check it.\nOne way to check additivity is to look at residuals, and see whether they are consistently lower / higher in some blocks.\nWhat can we do if we find an interaction effect? Sometimes it’s enough to transform the response Otherwise, another design may be necessary. Factorial designs (to be discussed soon) allow for inference even in the presence of interactions.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-06-week5-3/",
    "title": "RCBD with Random Block Effects",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-08-06",
    "categories": [],
    "contents": "\nReadings 4.1, Rmarkdown\n\n\ninclude_graphics(\"https://uwmadison.box.com/s/gu58jurmhyraebsdyh4txlpyxegxn9xf\")\n\n\n\n\nAs in standard random effects, sometimes the blocks are from a larger population We care about a “resin effect,” but don’t care about each individual batch The model is setup as before,\nexcept now both jN(0, 2) and ijN(0, 2), all independently.\nObservations are correlated within each block are correlated\nTesting\nWe’re interested in whether any of the treatments have an effect,\nWe won’t show it, but it turns that\nso we should reject the null when MSTreatment is much larger than MSE. In fact, as in the fixed block case,\nso we can use the same F-distribution cutoff when testing whether any treatment effects are nonzero.\nEstimation\nAs in random effects for the completely randomized design, we can use either the method of moments or maximum likelihood The method of moments estimators are\nFinding confidence intervals continues to be a challenge for the method of moments approach. If you want that, you should use maximum likelihood, as shown in the computer example accompanying these notes.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-06-week5-4/",
    "title": "Latin Squares, part 1",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-08-06",
    "categories": [],
    "contents": "\nReadings 4.2, 4.3, Rmarkdown\nRCBD is nice when we have one nuisance factor What if we have two? We’re testing a manufacturing procedure, but raw materials come in batches and different operators have different skills We’re testing different diets on cows over a series of days, but there will both cow and day effects\nAssume two nuisance factors, each with p levels Assume that we care about p different treatments.\nDesign idea: Make sure to run each treatment on each nuisance factor. Each operator sees each treatment once Each batch of materials is used for each treatment\nSetup\nLatin Squares are pp arrays, filled with p letters, so that each letter appears exactly once in each row and each column.\nThis is basically sudoku for statisticians Why do we care? It tells us how we can implement the design idea above. Randomly choose a pp latin square The rows are levels of the first blocking factor The columns are levels of the second blocking factor The letters are the treatment levels The experiment consists of p2 runs, on each of the pairs of blocking levels, with treatment specified by the cell’s label\nModel Setting\nInstead of just one set of block effects, we’ll have two sets, i and k. This results in,\nwhere ijkN(0,2) independently. Here, each of the indices i, j and k range from 1, , p.\nTo make the model identifiable, we assume\nHypothesis Testing\nOur test hasn’t changed at all,\nBut now we need to account for block-to-block variation across both nuisance factors. The formula isn’t pretty, but it’s exactly analogous to the decompositions we’ve seen before,\nThis is abbreviated as,\nand we define MSRows=1p - 1SSRows MSTreatments=1p - 1SSTreatments MSColumns=1p - 1SSColumns MSE=1(p - 1)(p - 2)SSE\nIt turns out that\nwhich forms the basis for the test: reject the null if the ratio lies above the 1 - quantile of this F-distribution.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-06-week5-5/",
    "title": "Latin Squares, part 2",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-08-06",
    "categories": [],
    "contents": "\nReadings 4.2, 4.3, Rmarkdown\nWhen p is small, this test can have low power. If we can collect more than p2 samples, we should. But how exactly should the samples be collected, and how is the replicated design analyzed?\nThe design decision is context dependent, You can use the same row and column levels You can keep column levels, but have different row levels Equivalently, can keep rows, but different columns You can have different row and different column levels\nNote that in each case, you use a different latin square in each replicate.\nFortunately, the analysis is conceptually unified. We continue to have row, column, and treatment mean squares We have to add in replicate mean squares, to track variation from replicate-to-replicate The code in each case is the same, lm(Y ~ Replicate + W1 + W2 + X)\nGraceo-Latin Squares\nWe motivated Latin squares by asking how we can block 2 factors simultaneously. What if we have 3? We could go on forever…. That said, the transition from 2 to 3 is not hard\nIntroduce p greek letters to represent the 3rd blocking factor. When p=5, we would have , , ,, , for example.\nA Graeco-Latin square is like two Latin Squares overlaid on one another,\nwith one additional requirement: each latin and each greek letter must only appear together once. This last requirement is called orthogonality\nA hypothesis test can be defined, by using the decomposition,\nand noting that,\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week2-4/",
    "title": "Diagnostics and Power",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-08-03",
    "categories": [],
    "contents": "\nhttps://docs.google.com/document/d/1vM-26q7Go4HqaqeR18mZFMM8RBYJVGWkGHeTYmmigG4/edit\nReadings 2.4, Rmarkdown\nThe reference distribution depends on three assumptions, * Samples are independent. If they aren’t, then we’re pretending we have more samples than we do * The standard deviations are equal * The populations are normally distributed\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/n1a3bdzspet06ibsd1yebc1r6w7kzf3o.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/uzh72egfwwp251xzac932cej6kna1woz.png\")\n\n\n\n\nFor the last two, you can use something called a normal probability plot. This plots the sample quantiles against the theoretical normal distribution quantiles. Power Analysis People will often call you asking about what a good sample size is for their experiment. A good way to answer this is to compute the power curves as a function of different signal strengths.\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/06qu4t1q6jemmwto01vgd95jtzd0if3e.png\")\n\n\n\n\nOf course, you can never know the signal strength in advance. But you can test a few different plausible ranges, based on past experience. Important Variations What if the variances are not equal?\nOur test statistic used a pooled standard deviation. If the variances aren’t equal, we standardize differently,\nThis is unfortunately not exactly t-distributed under the null. That said, the reference distribution can be well approximated by one, and almost any statistical package will let you compute corresponding p-values and confidence intervals.\nWhat if the variances are known?\nIn this case, don’t use S1 and S2 – standardize using the known standard deviations. Since there’s no additional randomness coming from this estimation, the reference distribution is actually standard normal.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week3-1/",
    "title": "ANOVA",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-08-03",
    "categories": [],
    "contents": "\nReadings 3.1 - 3.3, Rmarkdown\nhttps://uwmadison.box.com/shared/static/h3bbv7cjo0our7syhlsc0tdn5c22xyme.png https://uwmadison.box.com/shared/static/a8jqduhcmjzj9re22a81236k3enbtzzn.png https://uwmadison.box.com/shared/static/znpaeugwi14nxhuvz2lkmoo2ovv2jtx1.png https://uwmadison.box.com/shared/static/n836t4q718m2o16hluvglplqi4lshgcm.png https://uwmadison.box.com/shared/static/7prsedegwnp6wdghcw6vfv2rs7zsr7nj.png https://uwmadison.box.com/shared/static/l40cisieegn7u37ite50eu1yeoed8v88.png\nProblem Setting ANOVA is used when you want to compare the effect of different treatments on a continuous response. For example, How does the etch rate of a tool depend on its power setting? How do an opera company’s different donation strategies compare with one another? How does the average rental time compare across cars?\nVisually, it can be used whenever the data can be arranged as a series of boxplots. Model and Test Setup Formally, consider the model\nwhere i=1, , a and j=1, , n and the errors ijN(0, 2) are independent.\ni indexes different groups j indexes the samples within groups. a is the number of groups n is the number of samples in each group N=na is the total number of samples\nOur null hypothesis is that none of the groups deviate from the global mean. The alternative is that at least one of the groups is different. Formally,\nImportant Identities The word “analysis” in ANOVA is used in the classical sense of to break something into its parts. ANOVA breaks the observed variation into distinct components,\nwhich is abbreviated as\nSStotal = SStreatments + SSerror.\nIf any of the groups had a mean that was different from the global mean, then you’d expect the treatment term to be larger than it would otherwise be. How large is large enough to reject?\nSince the variance within each group is 2, the variance of each yi is 2n. The blue term looks like how we would usually estimate the variances of the yi, i.e.,\nOn the other hand, under the null, all the yijN(,2), so we would also know,\nSo under the null,\nbut otherwise it would be larger.\nNote that the numerator and denominator are chi-squares, with a - 1 and N - a df, respectively. It’s not obvious, but they’re also independent (this is called Cochran’s theorem). Therefore, the null reference is an F distribution with (a - 1, N - a) df. The ANOVA Table If we introduce MStreatments and MSE for the numerator and denominator in the ratio above, we can organize this information as,\nThis is also how most computer output will be displayed, so it’s worth being familiar with.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week3-2/",
    "title": "Model Checking",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-08-03",
    "categories": [],
    "contents": "\nReadings 3.4, Rmarkdown\nhttps://docs.google.com/document/d/1Ds7H3hZyqLNYhF8wQ2OZO7viJuHd74EzYNSXjrTH6eU/edit\nhttps://uwmadison.box.com/shared/static/ka2t5b3awtqt0mqdqtm8o5zbzs8ur7xp.png\nAssumptions\nRecall the ANOVA model,\nwith independent errors ijN(0, 2). There are a few ways that this model can fail, There might be systematic variations besides the group deviations i. The errors might not be normally distributed The errors might not be independent The variance might not be the same in each group\nTo see if the model is okay, it will be helpful to define residuals,\n. Residuals are our best guess of what the true random error ij is like. Normal Probability Plots\nWe can’t check normality of ijdirectly, but we can check normality of the residuals eij using normal probability plots.\nIn the plot to the right, it looks like the data are somewhat more tightly clustered near the middle, though the most extreme points are more extreme than expected. Do you see why?\nOf the ways that the model can fail, normality of the residuals is not the most severe, because you can often count on the central limit theorem to make the reference F distribution still approximately correct. Plotting Residuals The way to check for systematic variation beyond the i’s, try plotting residuals against measured variables. If you see “patterns”, those represent missing terms in your model. Use plots to check for independence. For example, if you plot the residuals over time and you see clear trends, then the errors are likely correlated over time. Plotting residuals against the fitted values. This can reveal nonconstant variance across the groups. Testing Equality of Variances There are formal tests to test whether the equal variance assumption of the ANOVA is valid (it’s sort of meta). The most common are, Bartlett’s test The Modified Levene test The main difference is that the Modified Levene test is still valid even when the errors are not normally distributed. You don’t need to memorize the test statistics, but know that they exist, and be able to interpret associated computer output. Transformations What can you do if you detect nonconstant variance across groups? The most common fix is to use a variance stabilizing transformation. That is, apply some function f(x) to each data point and then perform the ANOVA.\nThere are various rules of thumb, though the process is still somewhat informal, f(x) =x or f(x) =1 + x If your data seem Poisson f(x) =(x): If your data seem lognormal f(x) =(x ): If your data are binomial in fraction form\nThese are special cases. More generally, if you notice , then setting f(x) = x1 - will stabilize the variance.\n(It’s not at all obvious why any of these transformations are effective – they are typically derived in introductory mathematical statistics courses).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week3-3/",
    "title": "Contrasts",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-08-03",
    "categories": [],
    "contents": "\nhttps://docs.google.com/document/d/12jrrcuA4oGYoVqSR-eqNhWYf0pW09ISaMgibEugwf8w/edit\n\n\ninclude_graphics(\"https://www.airmeet.com/event/3124e6e0-8b3d-11eb-adfc-b1c12ad96800?code=ffbe7ca8-0134-473b-9f02-83856e75c8f4\")\ninclude_graphics(\"https://www.airmeet.com/event/3124e6e0-8b3d-11eb-adfc-b1c12ad96800?code=ffbe7ca8-0134-473b-9f02-83856e75c8f4\")\n\n\n\n\nReadings 3.5, Rmarkdown\nWhat is a contrast? When we reject the null in ANOVA, we know at least one of the treatments deviates from the global average. But which one(s)?\nContrasts help. A contrast is a linear combination of the means,\nFor any particular c, we test\nTo see that this is actually something worth doing, consider the case that we have 4 different means, 1, 2, 3, 4, c = (1, 1, -1, -1): Are the first two means different from the last two, on average? c = (1, -1, 0, 0): Are the first two means equal to each other? etc. Testing Contrasts Remember the hypothesis testing recipe. We need, A test statistic A reference distribution\nOur best guess at i is yi, so a reasonable statistic is,\nHow will we find its reference distribution? Under the null, this statistic is normally distributed with mean 0 and variance,\nSo, a standardized statistic is,\nTo estimate 2, we can use MSE. This is a good choice, because it remains valid even when the null is untrue.\nSince we plugged-in the estimate 2, we have divided our normal distribution by the square root of a chi-square. The result is therefore t-distributed, with N - a df. Confidence Intervals for Contrasts If we make the same computations as above, but without assuming that the null is true, we would find that,\nwhen we choose tleft and tright to be the 0.025 and 0.975 quantiles of a t distribution with N - a df.\nThe resulting confidence interval,\nThis is an explicit formula that you can use in your computations, but don’t let the density of the symbols here confuse you. Returning to our original definitions, this is just,\nwhere we’re writing Var instead of Var because we’re plugging in the estimate 2.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week3-4/",
    "title": "Multiple Comparisons",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-08-03",
    "categories": [],
    "contents": "\nReadings 3.5, Rmarkdown\nhttps://docs.google.com/document/d/1D44LrqM0uUv2MlC2s50A0yzMqZxyOOnqWxp8VKP-5HY/edit#\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/tmez3gdyre3lth822zm2wimmes3nujyy.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/lc34pprk90p7evabiycjiw5c22xsm666.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/alo5fgwmkwrniwejs32ow05carx9uwna.png\")\n\n\n\n\nIf we planned in advance which contrasts we want to use, we are fine. But what if we hadn’t planned any, and go in search for significant results using different contrasts?\nImagine testing 100 hypothesis at = 0.05. Suppose they are all null). We would see 5 rejected null hypotheses on average. If we want to allow ourselves some flexibility in searching over contrasts, we need to adapt our methodology. We should control the experimentwise error rate, the probability that any test results in a false positive. Scheffe’s Method Let’s say we’re interested in m contrasts, c1, ,cm. The idea is to widen our confidence intervals slightly, to make false positives rarer.\nHow much should the intervals be widened? It’s not obvious, but Scheffe found that we should multiply the endpoints of each of our m intervals by\n(this is for 95% confidence intervals). Tukey’s Method If we only care about the differences between pairs of group means, we can use Tukey’s method.\nAll the contrasts now have the form,\nWe’re going to make confidence intervals for these, and it’s natural to center them around,\nHow wide should the intervals be?\nTukey found a reference distribution for\nwhere ymax refers to the maximum groups average across the a groups. From there, he tabulated the quantiles as q(a, df).\nIt turns out that the appropriate width of the confidence intervals can be derived from these quantiles,\nThis works because, if the difference between the max and min group averages is contained within this interval, then all pairs i, j of differences are also contained in this interval. Fisher’s Least Significant Difference Fisher’s LSD is used to compare pairs of means Unlike Tukey’s method, it doesn’t control the experimentwise error rate\nNotice that the variance of the differences is\nFisher’s LSD compares each difference |yi- yj| to the cutoff,\nand rejects the null that the pairs have equal means if the difference is larger.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week4-1/",
    "title": "Random Effects",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-08-03",
    "categories": [],
    "contents": "\nhttps://docs.google.com/document/d/1dHiCnwlLtSe2SX4ZE8WldiUKpkl1Kr4uIR5N0Oa916E/edit#heading=h.ga02xa5s37bi\nReadings 3.9, Rmarkdown\nSometimes a factor has so many levels, that we can’t collect observations for all of them. Or, even if you can collect them, having one parameter for each would lead to a clumsy model We instead settle for saying the effect of the factor on average, rather than any specific level\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/ig50uxftq5mz19t15w74kryh9ko0jm6e.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/f2e3b39odm4ejkyqinvpii24s9tegnqb.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/ujmby14i61prsex43i6drsgpgsu9lvjq.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/l9nclru13n0s5e52oiu1aasz66vbwr6m.png\")\n\n\n\n\nExamples, Is there a high school effect on college GPA? (instead of effects for individual schools) Is there a loom effect on fiber strength? (don’t care about individual looms)\nModel\nThe model has the form,\nwhere iN(0, 2) and ijN(0, 2). The crucial difference is that i is now thought of as random, not fixed.\nNotice that\nMore generally, the covariance matrix is block diagonal, with blocks of 2 within groups.\nHypothesis Testing\nWe may want to test whether there is any variation in response across factor levels. Formally,\nFor our test statistic, we can use the same one as before,\nand for the same reasons as in fixed-effect ANOVA, this is F-distributed with (a - 1, N - a) df, and we can reject when this is large.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week4-2/",
    "title": "Fitting Random Effects",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-08-03",
    "categories": [],
    "contents": "\nReadings 3.9, Rmarkdown https://docs.google.com/document/d/1OjgpajLhWmrbjB2PG52__QUYstWYvMFuIaU2aTiBh4c/edit\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/f2e3b39odm4ejkyqinvpii24s9tegnqb.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/ujmby14i61prsex43i6drsgpgsu9lvjq.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/0au3iwj17u49ueqor5djqg58rensvmqo.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/kqgsua08wh8n2h6zhssbmxf4byldcq2o.png\")\n\n\n\n\nThere are three key parameters: , 2,2. Two ways to fit them are (1) the method of moments and (2) maximum likelihood.\nMethod of Moments\nFor estimating , this method uses the overall mean y.\nWhat about the 2’s? The key identity is,\nWe can approximate the expected values through,\nIf we pretended these approximations were exact equalities, then we have two equations with two unknowns. The method of moments defines parameter estimates as the solutions to that system of equations.\nHow can we get confidence intervals for these estimates?\nFor =y, we can use Var(y)=1NVar(yij)=2+n2N For 2, we can use the fact that (N - a)MSE22N - a For 2, we’re out of luck, though some papers give approximate confidence intervals.\nMaximum Likelihood Estimation\nAn alternative approach is to use maximum likelihood.\nStack all the yij’s into one long length N vector, and observe that the data are jointly normally distributed\nThe specific form of the covariance isn’t important. What is important is that we can exactly evaluate the probability of y11, y12,yan under any choice of the parameters , 2, 2.\nDefine L(, 2, 2) the probability of the dataset y11, y12,yanviewed as a function of the normal distribution’s parameters A good estimate for these parameters comes from finding the configuration that maximizes L(, 2, 2).\nThe maximizers can’t be found analytically, but algorithms exist to find the maximizers Software also gives a confidence interval for the estimates. It works by studying the curvature of the likelihood function at the maximizer, though we don’t need to worry about the details.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week4-3/",
    "title": "Nonparametric Anova",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-08-03",
    "categories": [],
    "contents": "\nReadings 3.11, Rmarkdown https://docs.google.com/document/d/1P7RRGBBthrBIAK7esOSl2TBFtXfPZY9xZytZG3T691s/edit\nWhat can we do when the errors seem very far from normally distributed? Imagine all attempts to transform the data have failed. We can always use nonparametric ANOVA\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/kdv2ayi2heooodf0a0j15swxld4ofll7.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/nn8v4t3ntvcsowvuy91b5lvoajju3ekf.png\")\n\n\n\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/a2evjeib97ay362iue2vhkxk18gbh7l4.png\")\n\n\n\n\nIdea: If the groups have the same means, then no group should be consistently ranked higher than another.\nRecipe\nTransform the data to their ranks. The smallest of the yij becomes 1, the next smallest becomes 2, etc. Denote these ranks by Rij. Compute the test statistic\nwhere we define Ri. to be the sum of the ranks in the ith group, and\nCompare the test statistic to a 2a -1 reference distribution. If it seems too large to be plausible, reject the null hypothesis.\nWhere did this test statistic come from?\nIt’s possible to show that the statistic is equivalent to\nwhich compares the average rank in group i to the average rank overall, and standardizes by the overall variance of the ranks. The first formula is the one presented in the book, though, and it’s easier to calculate by hand.\nWhy not always use nonparametric ANOVA?\nIf the data are actually normal, than this approach has less power than standard ANOVA If you have doubts about validity, a safe approach is to try both. If the approaches approximately agree, default to standard ANOVA.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-week5-1/",
    "title": "Randomized Complete Block Design",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2021-08-03",
    "categories": [],
    "contents": "\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/pq0v9wj2z63szngnxqm0mjfiq323q28k.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/35mmpml4az9ztv2wd5zp45mxiun11gja.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/rp2oe5pq19cxxvgps7hie04c3tni14r2.png\")\n\n\n\ninclude_graphics(\"https://uwmadison.box.com/shared/static/fgtpgaimnwwlm5s63bmj9f8ahe1nq4qp.png\")\n\n\n\n\nReadings 4.1, Rmarkdown https://docs.google.com/document/d/17cdyQSI9V7XQ68F4ZYC14mr8P7tIHjRonaf3XPxpakw/edit#\nHow can we generalize the idea of pairing to the case that we have more than two treatments to compare? Suppose that the nuisance factor is known and controllable Also assume the blocks have the same number a of samples as we have treatments (a=2 → pairing)\nExample [page 120]: Medical device manufacturer creates vascular grafts, and wants to see effect of manufacturing pressure on defect rate. But the resin used to make the graft arrives in batches, and may itself contribute to defects.\nIdea: Randomly assign each of the a treatments to the units within each block. This is called the RCBD.\nThis is a restriction on randomization. Only random treatment assignments where each block has each treatment are allowed.\nModel Setup\nFor hypothesis testing, we need a probability model. We suppose\nwhere ijN(0, 2) independently and ii=jj=0 is required to ensure identifiability.\nAs before, is a global mean i is the treatment effect for the ith treatment level ij is the error term\nBut now we also have j, the effect of the jthblock\nNotice that in each block, we must have exactly a samples, one from each treatment.\nHypothesis Testing\nWe’re interested in whether any treatments have an effect,\nThere’s a useful decomposition of the total variance that accounts for block-to-block variation,\nWe’ll abbreviate this as SST= SSTreatment+ SSBlock+SSE.\nThe corresponding mean squares are,\nMST=1N - 1SST, which is the sample variance of all yij MSTreatment=1a - 1SSTreatment MSBlock=1b - 1SSBlock MSE=1(a - 1)(b - 1)SSE, which a good estimator for 2\nAs in usual ANOVA, if MSTreatment is much larger than MSE, then we have evidence against the null hypothesis. In fact, it turns out that, under the null,\nSo, we can reject the null hypothesis if this statistic is larger than the1 - quantile of an F distribution with (a - 1, (a - 1)(b - 1)) df.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T15:10:54-05:00",
    "input_file": {}
  }
]
