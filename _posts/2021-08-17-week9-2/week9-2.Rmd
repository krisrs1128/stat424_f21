---
title: "Examples of $2^K$ Designs"
description: |
  Three case studies in using $2^K$ designs.
author:
  - name: Kris Sankaran
    url: {}
date: 11-04-2021
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE, echo = TRUE, fig.align = "center", out.width = "80%")
library(ggplot2)
theme424 <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16),
    legend.position = "bottom"
  )
theme_set(theme424)
```


_Readings [6.6](https://www.wiley.com/en-us/Design+and+Analysis+of+Experiments%2C+10th+Edition-p-9781119492443#content-section), [Rmarkdown](https://github.com/krisrs1128/stat424_f21/blob/main/_posts/2021-08-17-week9-2/week9-2.Rmd)_

1. Like the corresponding section in the book, these notes introduce no new
technical material. Instead, they illustrate end-to-end analysis workflows for
$2^K$ designs and highlight the types of judgments that need to be exercised in
practice.

### Example 6.3

2. An experiment was done to see how the "advance rate of a drill" varied as a
function of four factors^[The factors are drill load, flow rate, rotational
speed, and drilling mud.] which we will call A, B, C, and D. To start, we code
the variables `A` to `D` so that they are $\pm 1$'s instead of the characters
`+` and `-`.

```{r}
library(dplyr)
library(readr)
code <- function(x) ifelse(x == '-', -1, 1)
drill <- read_csv("https://uwmadison.box.com/shared/static/7l8bpcu36a12a8c0chlh4id0qezdnnoe.csv") %>%
  mutate_at(vars(A:D), code)
```

3. It always helps to make a preliminary plot of the data. The two columns
encode the value of $A$ and the two rows encode $D$. Informally, it seems that
adding B and C both increase the response. The slope in the bottom right panel
is also a bit steeper than the others, so perhaps there is an interaction
between B and A. With four factors though, it's hard to judge everything from
just a figure, so the effect estimates should be a useful guide.

```{r}
ggplot(drill) +
  geom_point(aes(B, rate, col = as.factor(C))) +
  scale_color_brewer(palette = "Set2") +
  facet_grid(A ~ D)
```

4. As a first pass at the analysis, we fit a full $2^4$ model. The Daniel plot
is below. We use the same helper function as the last notes, and again we use
the fact that the effects are twice the regression coefficients.

```{r, fig.margin = FALSE, fig.hold = TRUE}
daniel_plot <- function(effects, probs = c(0.3, 0.7)) { 
  qq <- qqnorm(effects, datax = TRUE)
  qqline(effects, col = "red", probs = probs, datax = TRUE)
  text(qq$x, qq$y, names(effects), pos=1)
}

fit <- lm(rate ~ A * B * C * D, drill)
effects <- 2 * coef(fit)[-1]
daniel_plot(effects, c(0.35, 0.65))
```

5. The absence of A among the large effects suggests dropping factor $A$ in the
fit. However, when we study the residuals, we notice they are heteroskedastic,
with larger residuals associated with higher predicted values.

```{r, fig.margin = FALSE}
fit <- lm(rate ~ B * C * D, drill) # removed A from the model
drill_resid <- drill %>%
  mutate(residual = resid(fit), y_hat = predict(fit))
ggplot(drill_resid) +
  geom_point(aes(y_hat, residual))
```

6. Since the data are rates, we take a log-transform. We refit the full model,
which suggests a much simpler set of factors (B, C, and D), with no
interactions. The residuals of the associated submodel also look much better
now.

_Lessons_: 
    * Examining residuals can motivate useful transformations of the data.
    * It’s a good thing replicates were made.


```{r, fig.margin = FALSE}
fit <- lm(log(rate) ~ A * B * C * D, data = drill)
daniel_plot(2 * coef(fit)[-1])
fit <- lm(log(rate) ~ B + C + D, data = drill)

drill_resid <- drill %>%
  mutate(residual = resid(fit), y_hat = predict(fit))
ggplot(drill_resid) +
  geom_point(aes(y_hat, residual))
```

### Example 6.4

7. An experiment was done to see how defect rate in airplane windows varied
according to four factors: temperature (A), clamp time (B), resin flow (C), and
press closing time (D). Our first step is exactly the same as before: download
the data, code the variables, and plot the raw data. It seems that adding A
increases defects and adding C removes defects, but this is a just a heuristic
description.

```{r}
windows <- read_csv("https://uwmadison.box.com/shared/static/62phufkeprheu9gu35mu1e75x6rc2shv.csv") %>%
  mutate_at(vars(A:D), code)

ggplot(windows) +
  geom_point(aes(A, defects, col = as.factor(C))) +
  scale_color_brewer(palette = "Set2") +
  facet_grid(B ~ D)
```

8. Below, we estimate the effects and make a Daniel plot. The story seems simple,
    * Temperature (A) has a strong positive effect
    * Resin (C) flow has a slight negative effect.
  
```{r}
fit <- lm(defects ~ A * B * C * D, data = windows)
daniel_plot(2 * coef(fit)[-1])
```
  
9. Next, we refit the model with just these two main effects and then examine
residuals. This reveals a kind of heteroskedasticity,

```{r}
fit <- lm(defects ~ A + C, data = windows)
windows$residual <- resid(fit)
ggplot(windows) +
  geom_point(aes(x = B, y = residual))
```

10. At this point, we don’t do any transforms, but instead recommend low
temperature, high resin flow, and low clamp time (because lower clamp time ->
lower variability)

_Lessons_:
  * In practice, it’s often useful to take variability into account, rather than
  just average response
  * A residual plot can be directly actionable
  
#### Dispersion estimates (Optional)

11. The heuristic in the previous example can be formalized.
    * Let $S^2\left(j^{+}\right)$ be an estimated standard deviation of responses when contrast $j$ is active.
    * Theory predicts that $\log\left(\frac{S^{2}\left(j^{+}\right)}{S^{2}\left(j^{-}\right)}\right)$ will be approximately normal.
  	* We call these the _dispersions_
	
12. Since the dispersions are approximately normal, it makes sense to create a
QQ plot from them. This has the potential to highlight whether any factors have
high discrepancies in spread, as a function of level. The code below loops over
every factor (columns of the model matrix `M`), computes these dispersions, and
makes a Daniel plot from them.

```{r, size = "tiny"}
M <- model.matrix(defects ~ A * B * C * D, data = windows)[, -1] # remove intercept
S <- list()

for (k in seq_len(ncol(M))) {
  S[[k]] <- data.frame(
    "effect" = colnames(M)[k],
    "sd_plus" = sd(windows$residual[M[, k] == 1]),
    "sd_minus" = sd(windows$residual[M[, k] == -1])
  )
}

S <- do.call(rbind, S)
s_ratio <- setNames(log(S$sd_plus / S$sd_minus), S$effect)
daniel_plot(s_ratio)
```

### Example 6.5

13. A $2^{4}$ experiment is setup to improve semiconductor manufacturing. The
question of interest is: How do temperature (A), time (B), pressure (C), and gas
flow (D) affect oxide thickness of the wafers? The code below downloads the data and reshapes it so that the oxide thickness variable is all in one column.

```{r}
library(tidyr)
oxide <- read_csv("https://uwmadison.box.com/shared/static/vyk6uoe3zbnonv4n6jcusbrocmt4cvru.csv") %>%
  pivot_longer(starts_with("wafer"), names_to = "variable")

ggplot(oxide) +
  geom_point(aes(A, value, col = as.factor(B))) +
  scale_color_brewer(palette = "Set2") +
  facet_grid(C ~ D)
```

14. In the design, four wafers are put in the furnace at a time. Note that these
are repeated measures, not replicates! Therefore, we take the average of the
wafers, and treat this as an unreplicated design. The block below computes the
mean and standard deviation of the response across each configuration of factors
A - D.

```{r, size = "scriptsize"}
oxide_collapse <- oxide %>%
  group_by(A, B, C, D) %>% # isolate independent configurations
  summarise(mean = mean(value), var = var(value)) # take average and var. across groups
oxide_collapse
```

15. Next, we perform an analysis of the variation in average thickness across
factor configurations. A look at the effects suggest that A, B, C, and the
interactions AB and AC are likely nonnull.

```{r, size = "scriptsize"}
fit <- lm(mean ~ A * B * C * D, data = oxide_collapse)
daniel_plot(2 * coef(fit)[-1])
```

16. To fit the all the main effects together when the interactions AB, AC, we
can use the formula `~ A * (B + C)`. This formula gets expanded into `A * B + A * C`, and each of the products expands into main effects and interactions (e.g.,
`A * B = A + B + A:B`). This captures all the effects we mention in the previous
point.

```{r, size = "scriptsize"}
fit <- lm(mean ~ A * (B + C), data = oxide_collapse)
summary(fit) # compare with Table 6.20
```

17. We can plot the full response surface from the fitted model. The `image`
method in the `rsm` ("response surface methodology") package directly outputs
this. For example, in the first plot, we see that the response is highest at A
and B both equal to 1. The curvature in this surface also suggests that there is
an interaction between these two terms.

```{r}
library(rsm)
image(fit,  ~ A + B + C)
```

18. In addition to modeling the average across wafers, we can model the standard
deviation. This is potentially useful if we want to find configurations with
more consistency in oxide thickness. The estimated effects for this model are
shown below.

```{r}
fit <- lm(var ~ A * B * C * D, data = oxide_collapse)
daniel_plot(2 * coef(fit)[-1])
fit <- lm(var ~ A + B * D, data = oxide_collapse)
coef(fit)
```

19. We can now use the two response surfaces jointly to determine factor
combinations that will have a target oxide thickness, and low variability around
that.

```{r}
library(rsm)
image(fit, ~ A + B + D)
```

20. Warning: What would have happened if we treated the repeated measures as
true replicates? We would incorrectly include that many factors are relevant
when they aren’t — this happens because our estimate of $\sigma^2$ is too small.
This can lead to lots of wasted effort.

_Lessons_:
  * Don't treat repeated measures as replicates, or we risk many false positive
  effects
  * It can be useful to model the variance of the response, rather than simply the
  mean

```{r, size = "scriptsize"}
fit <- lm(value ~ A * B * C * D, data = oxide)
summary(fit)
```