---
title: "Testing Differences in Means"
description: |
  The basic principles of hypothesis testing.
author:
  - name: Kris Sankaran
    url: {}
date: 09-16-2021
output:
  distill::distill_article:
    self_contained: false
---


```{r setup, include=FALSE}
library("knitr")
opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE, echo = TRUE)
```

_Readings [2.4](), [Rmarkdown]()_

1. Statistics is about making general conclusions based on specific evidence.
One approach is based on hypothesis testing: we have a theory about the general
(a null hypothesis), and we want to see whether our specific sample is
consistent with that theory. This philosophy is made quantitative by following
a standard recipe,

* Pose a null hypothesis about the world
* Define a test statistic that should detect departures from that null
hypothesis
* Determine a reference distribution for that test statistic
* Compute the test statistic on your data, and see if it’s plausibly a draw from
your reference distribution

```{r}
include_graphics("https://uwmadison.box.com/shared/static/e2vep3vvfvnz8v4kiilim1pjxe2w2upr.png")
```

```{r}
include_graphics("https://uwmadison.box.com/shared/static/66793m770ob31z53fpgc4xoci4y0s7fi.png")
```

```{r}
include_graphics("https://uwmadison.box.com/shared/static/r9dzi8ar8l7ic7h2xarnrdiwoab6tojz.png")
```

2. Proceeding in this way, there are a few types of error

|  | Tested rejected | Test didn't reject |
|---|---|---|
| Null is true | False alarm | Correct |
| Null is false | Correct | Missed detection |

3. **$p$-values**. “Rejected” or “Not rejected” is only a very coarse
description of how the data conforms to a theory. $p$-values give a measure of
the degree of (im)plausibility of a test statistic under a given null
hypothesis. The specific measure of plausibility will depend on the form of the
test -- we will see a specific example in the next set of notes.

### Two Sample t-test

4. Motivating example: You have two ways of making concrete mortar. Is one
stronger than the other? By default, you think they are equally strong.

Our test statistic for detecting departures from this null will be,


where we define the pooled standard deviation by,


and S1 and S2 are the usual standard deviations for each group individually. (consider what happens when n1 = n2 = n)

Under the null hypothesis, this is a ratio between a standard normal and chi-square, so t0 is t-distributed with n - 2 df.
This gives reference distribution under the null.
Confidence intervals
Instead of thinking we know the mean and trying to reject it, why don’t we try to directly estimate it (with an error bar)?

A 95% confidence interval is an interval [L, U] satisfying,



The randomness here is in L and U. If we were being more formal, we would write those as functions of the (random) sample,



To construct one for the two sample test, observe,


To simplify the algebra, let


So that the above expression reduces to



Then if we rearrange terms, we find



If you are calculating by hand, you can use the fact that tleft=-tright, to simplify the expression to



Which is exactly the definition of a confidence interval.  Plugging in the original expressions gives the confidence interval for the difference in means, assuming shared variance.

