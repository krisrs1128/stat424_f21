---
title: "Interpreting effects in $2 ^ 3$ Designs"
description: |
  Testing, uncertainty, and visualization in $2^3$ designs.
author:
  - name: Kris Sankaran
    url: {}
date: 10-28-2021
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE, echo = TRUE)
```

_Readings [6.3 - 6.4](https://www.wiley.com/en-us/Design+and+Analysis+of+Experiments%2C+10th+Edition-p-9781119492443#content-section), [Rmarkdown](https://github.com/krisrs1128/stat424_f21/blob/main/_posts/2021-08-16-week8-4/week8-4.Rmd)_

```{r, echo = FALSE}
library(ggplot2)
theme424 <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16),
    legend.position = "bottom"
  )
theme_set(theme424)
```

1. As before, the $SS$ terms can be obtained by squaring the contrasts and
dividing by the number of data points. This lets us build the associated ANOVA table,

```{r}
library(readr)
library(dplyr)
code <- function(x) ifelse(x == '-', -1, 1)
plasma <- read.table("https://uwmadison.box.com/shared/static/f3sggiltyl5ycw1gu1vq7uv7omp4pjdg.txt", header = TRUE) %>%
  mutate_at(vars(A:C), code)
```

```{r}
fit <- lm(Rate ~ A * B * C, data = plasma)
summary(aov(fit))
```

2. Consider the regression view of this situation. The full model would be
$$y_{i} = \beta_0 + \sum_{k = 1}^{3} \beta_k x_{ik} + \sum_{\text{pairs } k, k^\prime} \beta_{k k^\prime} x_{ik}x_{ik^{\prime}} + \epsilon_{i}$$
though we will often be interested in whether a submodel (which discards some of
the main or interaction effects) can do as well.
	
3. To compare a full model with a submodel, we can use the relative sums of squares,

$$R^2 = \frac{SS_{\text{Model}}}{SS_{\text{Total}}} = 1 - \frac{SS_{E}}{SS_{\text{Total}}}$$

```{r, echo = FALSE, fig.margin = TRUE}
library("EBImage")
display(readImage("https://uwmadison.box.com/shared/static/j2ya0idl19qtdwb991oq14hak97f8qla.png"))
display(readImage("https://uwmadison.box.com/shared/static/ckwb70swk9c0nr1h5nkt44wh8emzw07k.png"))
```

4. Instead of trying to understand the entire model’s importance, we might want
to understand the importance of specific terms. For this, it’s useful to have an
uncertainty estimate. Here is an example calculation,

__Variance estimate for effect of A__

\begin{align*}
\text{Var}\left(\text{Effect }A\right) &= \text{Var}\left(\frac{1}{2^{K - 1} n}\left(a - b - c + ab + ...\right)\right) \\
&= \left(\frac{1}{2^{K - 1} n}\right)^2\text{Var}\left(a - b - ac + ab + ...\right)
\end{align*}

But remember that $a$ refers to the sum of all samples at corner $a$, and likewise for $b$, $ac$, etc.,
\begin{align*}
\text{Var}\left(a - b - ac + ab + ...\right) &= \text{Var}\left(\sum_{\text{corner } a}y_{i} - \sum_{\text{corner }b}y_{i} - \sum_{\text{corner }ac}y_{i} + ...\right) \\
&= \sum_{\text{corner } a}\text{Var}\left(y_i\right) + \sum_{\text{corner }b}\text{Var}\left(y_i\right) + ...  \\
&= 2^K n \sigma^2
\end{align*}

so at the end of the day, we get
\begin{align*}
\text{Var}\left(\text{Effect }A\right) &= \frac{\sigma^2}{2^{K - 2}n}
\end{align*}

and we can estimate $\sigma^2$ by the error sum of squares $S^2$. From these
variance estimates, we can build confidence intervals that summarize all the
effects.

```{r}
summary(fit)
```

### Generalization: $2^K$ designs

5. Everything we’ve spoken about can be generalized to the case of arbitrary
numbers of factors. For example, the table notation can be used to get effect
estimate for interaction ABCD listed before equation 6.22 in the book, and the
sum of squares remain just the normalized square of the contrasts.

6. The key observation is that the regression representation continues to be
true even for large $K$. In particular, the effect estimates and their standard
errors will always be twice the coefficients of the regression onto coded
factors. For example, the code below manually estimates A's effect and compares
it with the coefficient in the regression,

```{r}
mean(plasma$Rate[plasma$A == 1] - plasma$Rate[plasma$A == -1])
2 * coef(fit)["A"]
```

This means we can use general regression machinery, without having to
manually substitute into formulas for different effects. This connection will be
explored in more depth in the next few weeks.