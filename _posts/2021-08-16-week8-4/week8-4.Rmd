---
title: "Interpreting effects in $2 ^ 3$ Designs"
description: |
  A short description of the post.
author:
  - name: Kris Sankaran
    url: {}
date: 08-16-2021
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE, echo = TRUE)
```


```{r, echo = FALSE}
library(dplyr)
library(ggplot2)
library(readr)
theme_set(theme_bw())
```

1. As before, the $SS$ terms can be obtained by squaring the contrasts and
dividing by the number of data points. This lets us build the associated ANOVA table,

```{r, echo = FALSE}
plasma <- read.table("https://uwmadison.box.com/shared/static/f3sggiltyl5ycw1gu1vq7uv7omp4pjdg.txt", header = TRUE) %>%
  mutate(
    A = as.factor(A),
    B = as.factor(B),
    C = as.factor(C)
  )
```

```{r, echo = FALSE, margin_note=TRUE}
fit <- lm(Rate ~ A * B * C, data = plasma)
anova(fit)
```

2. Consider the regression view of this situation. The full model would be
$$y_{i} = \beta_0 + \sum_{k = 1}^{3} \beta_k x_{ik} + \sum_{\text{pairs } k, k^\prime} \beta_{k k^\prime} x_{ik}x_{ik^{\prime}} + \epsilon_{i}$$
though we will often be interested in whether a submodel (which discards some of
the main or interaction effects) can do as well.
	
3. To compare a full model with a submodel, we can use the relative sums of squares,

$$R^2 = \frac{SS_{\text{Model}}}{SS_{\text{Total}}} = 1 - \frac{SS_{E}}{SS_{\text{Total}}}$$

```{r, echo = FALSE, fig.margin = TRUE}
library("EBImage")
display(readImage("https://uwmadison.box.com/shared/static/j2ya0idl19qtdwb991oq14hak97f8qla.png"))
display(readImage("https://uwmadison.box.com/shared/static/ckwb70swk9c0nr1h5nkt44wh8emzw07k.png"))
```

4. Instead of trying to understand the entire model’s importance, we might want
to understand the importance of specific terms. For this, it’s useful to have an
uncertainty estimate. Here is an example calculation,

__Variance estimate for effect of A__

\begin{align*}
\text{Var}\left(\text{Effect }A\right) &= \text{Var}\left(\frac{1}{2^{K - 1} n}\left(a - b - c + ab + ...\right)\right) \\
&= \left(\frac{1}{2^{K - 1} n}\right)^2\text{Var}\left(a - b - ac + ab + ...\right)
\end{align*}

But remember that $a$ refers to the sum of all samples at corner $a$, and likewise for $b$, $ac$, etc.,
\begin{align*}
\text{Var}\left(a - b - ac + ab + ...\right) &= \text{Var}\left(\sum_{\text{corner } a}y_{i} - \sum_{\text{corner }b}y_{i} - \sum_{\text{corner }ac}y_{i} + ...\right) \\
&= \sum_{\text{corner } a}\text{Var}\left(y_i\right) + \sum_{\text{corner }b}\text{Var}\left(y_i\right) + ...  \\
&= 2^K n \sigma^2
\end{align*}

so at the end of the day, we get
\begin{align*}
\text{Var}\left(\text{Effect }A\right) &= \frac{\sigma^2}{2^{K - 2}n}
\end{align*}

and we can estimate $\sigma^2$ by the error sum fo squares $S^2$. From these
variance estimates, we can build confidence intervals that summarize all the
effects.

```{r, echo = FALSE}
summary(fit)
```

### Generalization: $2^K$ designs

5. Everything we’ve spoken about can be generalized to the case of arbitrary
numbers of factors. For example, the table notation can be used to get effect
estimate for interaction ABCD listed before equation 6.22 in the book, and the
sum of squares remain just the normalized square of the contrasts.
