---
title: "Interpreting Effects in $2^2$ Designs"
description: |
  Drawing conclusions from parameter estimates.
author:
  - name: Kris Sankaran
    url: {}
date: 10-26-2021
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE, echo = TRUE, fig.align = "center")
```

_Readings [6.2](https://www.wiley.com/en-us/Design+and+Analysis+of+Experiments%2C+10th+Edition-p-9781119492443#content-section), [Rmarkdown](https://github.com/krisrs1128/stat424_f21/blob/main/_posts/2021-08-16-week8-2/week8-2.Rmd)_

```{r, echo = FALSE}
library(ggplot2)
library(EBImage)
theme424 <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16),
    legend.position = "bottom"
  )
theme_set(theme424)
```

1. We will often want to know whether estimated main or interaction effects are
significant. We can use ANOVA, though we have to be cautious when $n$ is small.
    * The numerators in the effect estimate expressions will be called _contrasts_
    for the estimated effect
    * For example, the contrast for the effect of $A$ is $ab + a - b -
    \left(1\right)$.
    
2. The associated sum of squares is
    $$
    \frac{1}{2^2 n}\left(\text{Contrast}\right)^2
    $$
    for example,
    $$
    SS_A = \frac{1}{2^2 n}\left[ab + a - b - (1)\right]^2
    $$
    
3. The associated ANOVA decomposition is
    $$
    SS_{\text{Total}} = SS_A + SS_B + SS_{AB} + SS_E
    $$
    and since the factors all have two levels, the df’s for the main and interaction
    terms are all 1. The df of $SS_T$ is $n 2^2 - 1$ (number of samples minus
    one). Taking the ratio between main and interaction $SS$ terms and $MS_{E} (= \frac{1}{4\left(n - 1\right)}SS_{E})$ gives the basis for $F$-statistics
    in the ANOVA table.

### Regression View

4. Another way of summarizing the $2^2$ model is to write a regression,
$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon
$$
where the $x_k$’s take on one of two values, depending on whether or not factor $k$ is active.
    * We’ve only included main effects. An interaction would be added via $\beta_{12} x_{1}x_{2}$
    * If the factors are binary (on vs. off), we can use a binary encoding. 

```{r, echo = FALSE, out.width = "50%"}
display(readImage("https://uwmadison.box.com/shared/static/giacopxkfj5jhuucepards9fv0cqvbmc.png"))
```


5. What if our factors are actually continuous?
	* We could _code_ the variables, converting low and high levels to ${-1, 1}$.
	* The model will still apply to all values in interval $[-1, 1]$.
	* An added benefit is that this coding (a) makes scales comparable and (b)
	induces orthogonality (roughly, it makes variables less correlated)
	
6. We will illustrate these ideas on a yield dataset. There are 12 samples
total, three replicates at each corner of the square.
	
```{r}
library(readr)
yield <- read_table2("https://uwmadison.box.com/shared/static/bfwd6us8xsii4uelzftg1azu2f7z77mk.txt")
yield
```

7. The same `lm + aov` approach used in general factorial designs applies to
$2^{K}$ designs. The only difficulty is that the data were originally coded as
`+` and `-`, and we need -1 and 1's. For this, we've prepared a small function
called `coded` and used it to create new columns, `cA` and `cB` which convert
those symbols into their numeric equivalents. From the ANOVA table below, we can
see that both factors have strong effects, though A's is stronger than B's. We
can also see that there is no detectable interaction effect, which is consistent
with the plot from the previous notes.

```{r}
coded <- function(x) ifelse(x == '-', -1, 1)
yield <- yield %>%
  mutate(cA = coded(A), cB = coded(B))
fit <- lm(Yield ~ cA * cB, data = yield)
summary(aov(fit))
```

8. The ANOVA table only describes whether a factor has any relationship with the
response -- it doesn't describe in what way the response changes when the
factors are turned on or off. For this, we need to look at the full model
summary. In the output below, the `Intercept` estimate (27.5) gives the response
when all the factors are turned off. The `cA` and `cB` estimates (4.17 and -2.5)
describe how the response changes when those factors are turned on. The interaction
effect measures how the effect differs from the additive effect when both
factors are active (it is 0.833 larger than what would be expected if we just
added `cA` and `cB`).

```{r}
summary(fit)
```
	
9. We can use this fit to build a response surface as well. This is a plot of
the yield from the top down -- darker colors correspond to higher yields. If the
interaction estimate was 0, the lines would be exactly parallel to one another.

```{r, asp = 1}
library(rsm)
image(fit, ~ cA + cB)
```
 